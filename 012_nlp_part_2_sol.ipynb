{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More NLP\n",
    "\n",
    "## Truncated Singular Value Decomposition and Dimensionality Reduction\n",
    "\n",
    "When processing text we end up with feature sets that are large! There is up to one feature per different word in our text sample, as well as more for multi-word combinations if there are larger ngrams allowed, far larger than a typical feature set that we're used to. One thing we can do when vectorizing is just to cap the number of features we end up with, but that doesn't seem to be the most sophisticated or smartest approach. \n",
    "\n",
    "TSVD is one thing that we can do to chop down the feature set - or reduce the dimensions - with a little more thought. \n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a common technique in machine learning, it does its name - reduces the dimensions in our feature data. We often want to do this for several reasons: \n",
    "<ul>\n",
    "<li> To reduce the amount of time it takes to train a model.\n",
    "<li> To reduce the amount of memory required to store the data.\n",
    "<li> To reduce the amount of noise in the data.\n",
    "<li> To make the data more interpretable.\n",
    "<li> To make the data more amenable to visualization.\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset from Last Time\n",
    "\n",
    "We'll load the spam dataset and vectorize it with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 89635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 easter</th>\n",
       "      <th>00 easter prize</th>\n",
       "      <th>00 easter prize draw</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 sub 16 remove</th>\n",
       "      <th>00 sub 16 unsub</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom cine actually</th>\n",
       "      <th>zoom cine actually tonight</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zouk nichols</th>\n",
       "      <th>zouk nichols paris</th>\n",
       "      <th>zouk nichols paris free</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>zyada kisi ko</th>\n",
       "      <th>zyada kisi ko kuch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 easter  00 easter prize  00 easter prize draw  00 sub  \\\n",
       "1226  0.0        0.0              0.0                   0.0     0.0   \n",
       "3898  0.0        0.0              0.0                   0.0     0.0   \n",
       "3540  0.0        0.0              0.0                   0.0     0.0   \n",
       "2122  0.0        0.0              0.0                   0.0     0.0   \n",
       "2053  0.0        0.0              0.0                   0.0     0.0   \n",
       "\n",
       "      00 sub 16  00 sub 16 remove  00 sub 16 unsub  00 subs  00 subs 16  ...  \\\n",
       "1226        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3898        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3540        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "2122        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "2053        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "\n",
       "      zoom cine actually  zoom cine actually tonight  zouk  zouk nichols  \\\n",
       "1226                 0.0                         0.0   0.0           0.0   \n",
       "3898                 0.0                         0.0   0.0           0.0   \n",
       "3540                 0.0                         0.0   0.0           0.0   \n",
       "2122                 0.0                         0.0   0.0           0.0   \n",
       "2053                 0.0                         0.0   0.0           0.0   \n",
       "\n",
       "      zouk nichols paris  zouk nichols paris free  zyada  zyada kisi  \\\n",
       "1226                 0.0                      0.0    0.0         0.0   \n",
       "3898                 0.0                      0.0    0.0         0.0   \n",
       "3540                 0.0                      0.0    0.0         0.0   \n",
       "2122                 0.0                      0.0    0.0         0.0   \n",
       "2053                 0.0                      0.0    0.0         0.0   \n",
       "\n",
       "      zyada kisi ko  zyada kisi ko kuch  \n",
       "1226            0.0                 0.0  \n",
       "3898            0.0                 0.0  \n",
       "3540            0.0                 0.0  \n",
       "2122            0.0                 0.0  \n",
       "2053            0.0                 0.0  \n",
       "\n",
       "[5 rows x 89635 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "tok_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA - Latent Semantic Analysis\n",
    "\n",
    "The TSVD performs somehting called latent semantic analysis. The process of LSA and the math behind it are not something we need to explore in detail. (LSA is often called LSI - Latent Semantic Indexing). The idea of LSA is that it can generate \"concepts\" in the text. These concepts are found by looking at which terms occur in which documents - documents that have the same terms repeated are likely related to the same concept; other documents that share other words with those documents are likely on the same concept as well.  \n",
    "\n",
    "An important part is the word \"Latent\" - i.e. the patterns detected are hidden, not explicit in the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SVD to Trim Dataset\n",
    "\n",
    "We are starting with LOTS of feature inputs. Below we can loop through several models of different number of remaining components to see the accuracy depending on the number of features we keep in the feature set. The truncated part of truncated SVD trims the featureset down to the most significant features. \n",
    "\n",
    "We started with a lot of features - we can make predictions that are close to as accurate with far fewer, hopefully!\n",
    "\n",
    "<b>Note:</b> this might take a long time to run, depending on your computer. Change the \"for i in range()\" part to cut down on the number of iterations to make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,15):\n",
    "\t\tn = i*10\n",
    "\t\tsteps = [('svd', TruncatedSVD(n_components=n)), ('m', LinearSVC(max_iter=100, tol=.01))]\n",
    "\t\tmodels[str(n)] = Pipeline(steps=steps)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t#Splits cut for speed\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=4, n_repeats=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, tok_df[0:1000], y[0:1000])\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "# plot model performance for comparison\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp_vec = tf_idf.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2 = tf_idf.get_feature_names()\n",
    "tmp_df = pd.DataFrame(tmp_vec.toarray(), columns=tok_cols2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88             I'm really not up to it still tonight babe\n",
      "2023    Is there any movie theatre i can go to and wat...\n",
      "1322                   Oh just getting even with u.... u?\n",
      "1498    Time n Smile r the two crucial things in our l...\n",
      "524     URGENT!: Your Mobile No. was awarded a å£2,000...\n",
      "164     BangBabes Ur order is on the way. U SHOULD rec...\n",
      "2530                   So the sun is anti sleep medicine.\n",
      "4030    Sorry vikky, i'm Watching olave mandara movie ...\n",
      "2363    Fantasy Football is back on your TV. Go to Sky...\n",
      "1749                               DO NOT B LATE LOVE MUM\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(tmp_df, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy\n",
    "\n",
    "Since we are planning on dropping a bunch of data, we can try a model first to see what the baseline accuracy is. I'm also going to limit the number of features here, since using the entire dataset will take ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9633883704235463"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_base = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\", max_features=2000)\n",
    "tmp_vec_base = tf_idf_base.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2_base = tf_idf_base.get_feature_names()\n",
    "tmp_df_base = pd.DataFrame(tmp_vec_base.toarray(), columns=tok_cols2_base)\n",
    "X_tr_base, X_te_base, y_tr_base, y_te_base = train_test_split(tmp_df_base, y)\n",
    "\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr_base, y_tr_base)\n",
    "pipe_test.score(X_te_base, y_te_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement LSA and Model\n",
    "\n",
    "We can use the truncated SVD to reduce the number of features in our dataset, in much the way we'd use any other data preparation step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9117013639626705"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tmp = TruncatedSVD(n_components=80)\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"svd\", svd_tmp), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr, y_tr)\n",
    "pipe_test.score(X_te, y_te)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Results\n",
    "\n",
    "In the second model, our feature set is far smaller, but we're still getting a very high accuracy. If the original dataset that we started with was very large, this impact would be magnified greatly. In general, NLP models use a lot of data, so this dimesionality reduction can help reduce training datasets that are massive and may even be impractical to process. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topics\n",
    "\n",
    "One of the things that LSA can do is to find \"topics\" in the text. We can use the components of the SVD to find the most important words in each topic. A \"topic\" is something that is not explicitly stated in the text, but is implied by the words that are used - if we have several documents that tend to use the same words, they are likely to be about the same topic. The LSA process is able to look for these cooccuring words and the documents that contain them, and group them as being about the same topic. The mechanics of this are some matrix math that is beyond what we neeed to know, but we can picture it like this.\n",
    "\n",
    "![LSA Math](images/lsa_math.webp \"LSA Math\")\n",
    "\n",
    "The topic extraction is also an example of unsupervised learning - something we'll look at more soon with clustering. We don't provide the topics to the mode in advance like we woud with a normal classification - we just give the LSA process the data, and it figures it out on its own.\n",
    "\n",
    "The model doesn't \"understand\" what each topic is, but it is able to pick up on trends of tokens that tend to occur together in documents. Text that contains \"ball\", \"game\", \"football\", \"play\", \"quarterback\" is likely to be about football - the model won't know it is football, but it will know that those words tend to occur together, and documetns that contain those words are likely to be about the same topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['amp imf', 'amp imf loan', 'bank amp', 'bank amp imf', 'bank directors', 'bank directors says']\n",
      "Topic:1  ['aaniye', 'aaniye pudunga', 'aaniye pudunga venaam', 'athletic', 'athletic lt', 'athletic lt gt']\n",
      "Topic:2  ['2gthr', '2gthr drinking', '2gthr drinking boost', 'aproach', 'aproach gal', 'aproach gal dt']\n",
      "Topic:3  ['love start', 'thank god', 'attraction', 'attraction feel', 'attraction feel need', 'beautiful lady']\n",
      "Topic:4  ['afternoons', 'afternoons evenings', 'afternoons evenings nights', 'amp happy birthday', 'approaching', 'approaching wish']\n",
      "Topic:5  ['bad problem', 'bad problem time', 'bed pillows', 'bed pillows floor', 'bed throw', 'bed throw laundry']\n",
      "Topic:6  ['bajarangabali', 'bajarangabali maruti', 'bajarangabali maruti pavanaputra', 'dodda', 'dodda problum', 'dodda problum nalli']\n",
      "Topic:7  ['4wrd', '4wrd dear', '4wrd dear loving', 'abt events', 'abt events espe', 'abt functions']\n",
      "Topic:8  ['afternoon wife', 'afternoon wife called', 'arrested murderer', 'arrested murderer immediately', 'called police', 'called police police']\n",
      "Topic:9  ['like tomorrow', 'lovers', 'avoids', 'avoids problems', 'avoids problems sent', 'becz']\n",
      "Topic:10  ['academic', 'department', 'academic department', 'academic department tell', 'academic secretary', 'academic secretary current']\n",
      "Topic:11  ['shahjahan', 'killed', 'mumtaz', '4th wife', '4th wife wifes', 'arises']\n",
      "Topic:12  ['dark', '1stone', '1stone sun', '1stone sun rose', 'amp sack', 'amp sack ful']\n",
      "Topic:13  ['tihs', 'tihs msg', 'yuo', 'amp nice sleep', 'ayn', 'ayn mitsake']\n",
      "Topic:14  ['able pay', 'able pay charge', 'ahead month', 'ahead month end', 'askin ahead', 'askin ahead month']\n",
      "Topic:15  ['amp concern', 'amp concern prior', 'amp fuelled', 'amp fuelled love', 'amp pain', 'amp pain pls']\n",
      "Topic:16  ['didn mean', 'angry left', 'angry left hit', 'bored wouldn', 'bored wouldn time', 'car wasn']\n",
      "Topic:17  ['beg just', 'beg just listen', 'clear things', 'clear things easy', 'creativity', 'creativity stifled']\n",
      "Topic:18  ['big files', 'big files download', 'come want', 'come want send', 'connection sucks', 'connection sucks remember']\n",
      "Topic:19  ['waves', 'crab', 'footprints', 'asked frnd', 'asked frnd clearing', 'beautiful footprints']\n",
      "Topic:20  ['1st 5wkg', '1st 5wkg days', '22 65', '22 65 61', '382', '382 ubi']\n",
      "Topic:21  ['4get person', 'dat girl', 'dat girl margaret', 'girl margaret', 'girl margaret hello', 'girl yes']\n",
      "Topic:22  ['chores', 'chores time', 'chores time joy', 'comes flowing', 'comes flowing dream', 'dirt']\n",
      "Topic:23  ['aid', 'aid usmle', 'aid usmle work', 'bam', 'bam aid', 'bam aid usmle']\n",
      "Topic:24  ['best choice yes', 'buy say', 'buy say happened', 'choice yes', 'choice yes wanted', 'choose room']\n",
      "Topic:25  ['medical insurance', '2waxsto', '2waxsto wat', '2waxsto wat want', 'able deliver', 'able deliver basic']\n",
      "Topic:26  ['aeronautics', 'aeronautics professors', 'aeronautics professors wer', 'aeroplane', 'aeroplane aftr', 'aeroplane aftr sat']\n",
      "Topic:27  ['children handle', 'children handle malaria', 'completely stop', 'days completely', 'days completely stop', 'gastroenteritis']\n",
      "Topic:28  ['argument week', 'argument week lt', 'bend', 'bend rule', 'bend rule way', 'doesnt inlude']\n",
      "Topic:29  ['know want', 'ptbo', 'babe sorry', 'babe sorry didn', 'bucks don', 'bucks don know']\n",
      "Topic:30  ['lies', 'amazing rearrange', 'amazing rearrange letters', 'astronomer', 'astronomer moon', 'astronomer moon starer']\n",
      "Topic:31  ['arrange shipping', 'arrange shipping cut', 'bids online', 'bids online arrange', 'care rest', 'care rest wud']\n",
      "Topic:32  ['sim card', 'amp asks', 'amp asks type', 'asks type', 'asks type lt', 'attempt terrorist']\n",
      "Topic:33  ['amplikater', 'amplikater fidalfication', 'amplikater fidalfication champlaxigating', 'atrocious', 'atrocious wotz', 'atrocious wotz ur']\n",
      "Topic:34  ['1apple', '1apple day', '1apple day doctor', '1cup', '1cup milk', '1cup milk day']\n",
      "Topic:35  ['warner', 'kiosk', 'warner village', '12 kiosk', '12 kiosk reply', '83118']\n",
      "Topic:36  ['1apple', '1apple day', '1apple day doctor', '1cup', '1cup milk', '1cup milk day']\n",
      "Topic:37  ['ec2a', 'laid', 'locations', 'sent direct', 'dogging network', 'largest']\n",
      "Topic:38  ['ask especially', 'ask especially girls', 'choosing', 'choosing phone', 'choosing phone plan', 'citizen smart']\n",
      "Topic:39  ['1st ur lovely', '2nd ur', '2nd ur sms', '3rd ur', '3rd ur nature', '4th ur']\n",
      "Topic:40  ['charity', '08701417012', '08701417012 profit', '08701417012 profit charity', '8007 nokias', '8007 nokias poly']\n",
      "Topic:41  ['1st ur lovely', '2nd ur', '2nd ur sms', '3rd ur', '3rd ur nature', '4th ur']\n",
      "Topic:42  ['18 30pp', '18 30pp txt', '1st 5free', '1st 5free 50', '30pp', '30pp txt']\n",
      "Topic:43  ['remembered', 'tayseer', 'tissco', 'aig', 'aig joined', 'aig joined tissco']\n",
      "Topic:44  ['attitude', 'attitude romantic', 'attitude romantic shy', 'attractive', 'attractive funny', 'attractive funny lt']\n",
      "Topic:45  ['ur lover', '07123456789', '07123456789 87077', '07123456789 87077 yahoo', '87077 yahoo', '87077 yahoo pobox36504w45wq']\n",
      "Topic:46  ['send 8883 cm', '118p', '118p msg', '118p msg rcvd', '16 118p', '16 118p msg']\n",
      "Topic:47  ['08712317606', '2rcv', 'cam moby', 'cam moby wanna', 'hlp', 'hlp 08712317606']\n",
      "Topic:48  ['113', '113 bray', '113 bray wicklow', 'address envelope', 'address envelope drinks', 'box 113']\n",
      "Topic:49  ['0870 chatlines', '0870 chatlines inclu', '250 want', '250 want 800', '3mobile', '3mobile 0870']\n",
      "Topic:50  ['000pes', '000pes 48', '000pes 48 tb', '12 000pes', '12 000pes 48', '48 tb']\n",
      "Topic:51  ['2stop', 'fastest', 'fastest growing', 'growing', '08714342399', '08714342399 2stop']\n",
      "Topic:52  ['20 poboxox36504w45wq', '20 poboxox36504w45wq 16', '41', '41 20', '41 20 poboxox36504w45wq', '4txt i1']\n",
      "Topic:53  ['luv vth', 'vth', 'alwa', 'alwa gud', 'alwa gud eveb', 'bed fals']\n",
      "Topic:54  ['affection', 'affection care', 'affection care luv', 'angry childish', 'angry childish true', 'angry wid']\n",
      "Topic:55  ['drop tear', 'drop tear dsn', 'dsn hav prayrs', 'dsn hav words', 'dsn lik', 'dsn lik stay']\n",
      "Topic:56  ['08701752560', '08701752560 450p', '08701752560 450p days', '100 filthy', '100 filthy films', '450p']\n",
      "Topic:57  ['16 remove txtx', '250 sms', '250 sms messages', '50p wk', '50p wk box139', '84025 use']\n",
      "Topic:58  ['offers mobile', '00 sub 16', 'sub 16', '00 sub', '150 worth', '150 worth discount']\n",
      "Topic:59  ['09050280520', '09050280520 subscribe', '09050280520 subscribe 25p', '121 chat rooms', '25p pm', '25p pm dps']\n",
      "Topic:60  ['bevies', 'bevies waz', 'bevies waz gona', 'bin watchng', 'bin watchng planet', 'comfey']\n",
      "Topic:61  ['2u rply', '2u rply poly', '8007 poly', '8007 poly breathe1', 'breathe1', 'breathe1 titles']\n",
      "Topic:62  ['acc wen', 'acc wen lt', 'credited lt', 'credited lt gt', 'draw acc', 'draw acc wen']\n",
      "Topic:63  ['affection', 'affection care', 'affection care luv', 'angry childish', 'angry childish true', 'angry wid']\n",
      "Topic:64  ['drop tear', 'drop tear dsn', 'dsn hav prayrs', 'dsn hav words', 'dsn lik', 'dsn lik stay']\n",
      "Topic:65  ['cruel', 'cruel face', 'cruel face romantic', 'decent', 'decent face', 'decent face lt']\n",
      "Topic:66  ['sae', 'await collection', 'await', 'collection', 'acc wen', 'acc wen lt']\n",
      "Topic:67  ['calculation', 'school ll', '4years', '4years dental', '4years dental school', 'accent']\n",
      "Topic:68  ['babe celebration', 'babe celebration rents', 'celebration rents', 'couch', 'couch link', 'couch link sent']\n",
      "Topic:69  ['think send', 'afternoon glorious', 'afternoon glorious anniversary', 'anniversary day', 'anniversary day sweet', 'coaxing']\n",
      "Topic:70  ['meeting today', 'salam', 'alaikkum', 'alaikkum pride', 'alaikkum pride pleasure', 'contact number']\n",
      "Topic:71  ['tank', 've got money', 'actually better', 'actually better yor', 'better yor', 'better yor ve']\n",
      "Topic:72  ['uk games', 'arcade game', 'arcade game console', 'arcade std', 'arcade std wap', 'buy space']\n",
      "Topic:73  ['answer den', 'answer den manage', 'asked girl', 'asked girl tell', 'boy asked', 'boy asked girl']\n",
      "Topic:74  ['sends', 'april les', 'april les got', 'bak college work', 'bristol st', 'bristol st week']\n",
      "Topic:75  ['sends', 'april les', 'april les got', 'bak college work', 'bristol st', 'bristol st week']\n",
      "Topic:76  ['sends', 'april les', 'april les got', 'bak college work', 'bristol st', 'bristol st week']\n",
      "Topic:77  ['2u 3wks', '2u 3wks pls', '3wks', '3wks pls', '3wks pls pls', 'bought test']\n",
      "Topic:78  ['dough', 'camry', 'camry like', 'camry like mr', 'clean need', 'clean need know']\n",
      "Topic:79  ['4u rply', '4u rply tone', '8007 titles', '8007 titles ghost', '8007 tone', '8007 tone dracula']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Truncated SVD\n",
    "\n",
    "Try to use the same text for predictions from the newsgroups last time. Try to use the TSVD with a limited number of components and see if the accuracy can stay similar to what we got last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (x,y): (857, 226373)   Test (x,y): (570, 226373)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and prep datasets\n",
    "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "X_train = news_tf.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "X_test = news_tf.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "print(\"Train (x,y):\", X_train.shape, \"  Test (x,y):\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m news_steps \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mscale\u001b[39m\u001b[39m\"\u001b[39m, StandardScaler()), (\u001b[39m'\u001b[39m\u001b[39msvd\u001b[39m\u001b[39m'\u001b[39m, tsvd), (\u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m, RandomForestClassifier())]\n\u001b[0;32m      4\u001b[0m news_model \u001b[39m=\u001b[39m Pipeline(steps\u001b[39m=\u001b[39mnews_steps)\n\u001b[1;32m----> 5\u001b[0m news_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      6\u001b[0m news_model\u001b[39m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    389\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 390\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    347\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    349\u001b[0m     cloned_transformer,\n\u001b[0;32m    350\u001b[0m     X,\n\u001b[0;32m    351\u001b[0m     y,\n\u001b[0;32m    352\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    353\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    355\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    357\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\base.py:855\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 806\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:870\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mif\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(X):\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_mean:\n\u001b[1;32m--> 870\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    871\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot center sparse matrices: pass `with_mean=False` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead. See docstring for motivation and alternatives.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    873\u001b[0m         )\n\u001b[0;32m    874\u001b[0m     sparse_constructor \u001b[39m=\u001b[39m (\n\u001b[0;32m    875\u001b[0m         sparse\u001b[39m.\u001b[39mcsr_matrix \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m sparse\u001b[39m.\u001b[39mcsc_matrix\n\u001b[0;32m    876\u001b[0m     )\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_std:\n\u001b[0;32m    879\u001b[0m         \u001b[39m# First pass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives."
     ]
    }
   ],
   "source": [
    "# Create Models\n",
    "tsvd = TruncatedSVD(n_components=50)\n",
    "news_steps = [(\"scale\", StandardScaler()), ('svd', tsvd), ('m', RandomForestClassifier())]\n",
    "news_model = Pipeline(steps=news_steps)\n",
    "news_model.fit(X_train, y_train)\n",
    "news_model.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at the Topics\n",
    "\n",
    "We can also take a look at what the topics identified in the data are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TruncatedSVD' object has no attribute 'components_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m terms \u001b[39m=\u001b[39m news_tf\u001b[39m.\u001b[39mget_feature_names()\n\u001b[0;32m      2\u001b[0m topics \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m index, component \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tsvd\u001b[39m.\u001b[39;49mcomponents_):\n\u001b[0;32m      4\u001b[0m     zipped \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(terms, component)\n\u001b[0;32m      5\u001b[0m     top_terms_key\u001b[39m=\u001b[39m\u001b[39msorted\u001b[39m(zipped, key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m t: t[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:\u001b[39m5\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TruncatedSVD' object has no attribute 'components_'"
     ]
    }
   ],
   "source": [
    "terms = news_tf.get_feature_names()\n",
    "topics = []\n",
    "for index, component in enumerate(tsvd.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Results\n",
    "\n",
    "Using LSA is a good way to condense our feature set that is often extremely large and extremely sparse, especially when we are dealing a large dataset, as is common with NLP. \n",
    "\n",
    "Simple applications in which this technique is used are documented clustering in text analysis, recommender systems, and information retrieval. More detailed use-cases of topic modeling are:\n",
    "<ul>\n",
    "<li> <b>Resume Summarization:</b> It can help recruiters to evaluate resumes by a quick glance. They can reduce effort in filtering pile of resume.\n",
    "<li> <b>Search Engine Optimization:</b> online articles, blogs, and documents can be tag easily by identifying the topics and associated keywords, which can improve optimize search results.\n",
    "<li> <b>Recommender System Optimization:</b> recommender systems act as an information filter and advisor according to the user profile and previous history. It can help us to discover unvisited relevant content based on past visits.\n",
    "<li> <b>Improving Customer Support:</b> Discovering relevant topics and associated keywords in customer complaints and feedback for examples product and service specifications, department, and branch details. Such information help company to directly rotated the complaint in respective department.\n",
    "<li> <b>Healthcare Industry:</b> topic modeling can help us to extract useful and valuable information from unstructured medical reports. This information can be used for patients treatment and medical science research purpose.\n",
    "</ul>\n",
    "\n",
    "In general, non-neural network approaches to NLP tend to be present in areas where we need to be able to process text quickly, without lots of processing. Spam filters are the classic example - we need to say yes or no, without spending ages to do so or burdening an email service with lots of processing. The examples above are similar - we are trying to draw a simple-ish conclusion. This is also somewhere that our old friend Bayes and his classifiers are most commonly seen - they are very fast at generating predictions once trained, so for something like emails, that's likely to be a good choice.\n",
    "\n",
    "An important concept from this example is the idea of condesing multiple features down into a smaller feature set while attempting to maintain the information in the original, that is something we'll revisit with Principal Component Analysis (PCA), a similar technique that is more generally applicable, later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Classification\n",
    "\n",
    "In addition to calculating things solely directly from our data, we can also use some external tools that can help create embeddings that are a little better (hopefully). This is also a neural network running behind the scenes to help us out. Word2Vec is an algorithm made by Google that can help process text and produce embeddings. Word2Vec looks for associations of words that occur with each other. This is an excellent illustrated description of Word2Vec: https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "### Word2Vec in Process\n",
    "\n",
    "Word2Vec generates its embeddings by looking at words in a sentence, and the surrounding words in that same sentence. This differs quite a bit from the data that we've generated with the vectorization, as this model is better able to capture the strutucre of a sentence, beyond only looking at the individual words. We will use word2vec for a couple of different things:\n",
    "<ul>\n",
    "<li> Primarily, we'll use word2vec in a \"two model\" sequence to set us up to do classifications. The word2vec model will replace the count/tf-idf scores that we previously used for our feature set with embeddings that it calculates as the w2v model trains. The w2v model is \"learning\" how to represent words with numbers, in this case dimensions in a multidimensional space.\n",
    "    <ul>\n",
    "    <li> The w2v training is what creates the N-dimension measurements of each token, those then feed into our feature set for our modelling. \n",
    "    </ul>\n",
    "<li> After the word2vec model is created, we can do things like check the similarity of words. \n",
    "</ul>\n",
    "\n",
    "#### Gensim\n",
    "\n",
    "Gensim is a package that we can install that has an implementation of Word2Vec that we can use pretty easily. This part just downloads some of the stuff we'll need, like stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "for package in ['stopwords','punkt','wordnet', 'omw-1.4']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words    = set(stopwords.words('english')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Since we are not using the vecorizer from sklearn, we need to provide our own tokenization. We can use the nltk based one from last time. We can also do any other types of processing here that we may want - stemming, customized stop words, etc... For this one I chopped out any 1 character tokens and added a regex filter to get rid of punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) #Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 2:\n",
    "                    filtered_tok.append(tmp)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Clan Text - Tokenize and Lemmatize\n",
    "\n",
    "Prep some data. The \"second half\" of the dataframe is what we can use with the Word2Vec prediction models - we have cleaned up lists of tokens as well as translating the targets to 1 and 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, wkly, comp, win, FA, Cup, final,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, early, hor, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, nt, think, go, usf, life, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  target2  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, g...        0  \n",
       "1                        [Ok, lar, Joking, wif, oni]        0  \n",
       "2  [Free, entry, wkly, comp, win, FA, Cup, final,...        1  \n",
       "3               [dun, say, early, hor, already, say]        0  \n",
       "4    [Nah, nt, think, go, usf, life, around, though]        0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = lemmaTokenizer(stop_words)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: tok(x))\n",
    "df[\"target2\"] = pd.get_dummies(df[\"target\"], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word2Vec Ebmeddings\n",
    "\n",
    "Now comes the word2vec model - instead of taking our clean data and counting it to extract features, we can train our Word2Vec model with our cleaned up data and the output of that model is our set of features. This will have Word2Vec do its magic behind the scenes and perform the training. W2V works in one of two ways, which are roughly opposites of each other, when doing this training:\n",
    "<ul>\n",
    "<li> Continuous Bag of Words: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at words surrounding target to try to predict it. \n",
    "</ul>\n",
    "\n",
    "We'll revisit the details of some of this stuff later on when we look at neural networks, since W2V is a neural network algorithm, it will make more sense in context. \n",
    "\n",
    "<b>Note:</b> this training is not making a model that we are using to make predictions. This is training inside the W2V algorithm to generate representations of our tokens. \n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "The embeddings that we are generating are vectors that represent the words in our text. We can look at the embeddings for a word to see what they look like, but they aren't comprehensible to humans. Our count vectors or the td-idf calculations we made previously are also embeddings, those are just far more simple. Word2Vec will generate embeddings that attempt to group words that are similar together in multidimensional space. We can look at a simple example in 2D:\n",
    "\n",
    "![Similarity](images/similarity.png \"Similarity\")\n",
    "\n",
    "The values here aren't calculated, they are chosen arbitrarily, but each word is represented here in two dimensions - x and y. Words that are similar in meaning should be close to each other in the vector representation, such as \"King\" and \"Queen\". Words that are not similar should be far apart, such as \"King\" and \"Rutabaga\". The embeddings that word2vec will generate from our data as a result of the training below will aim to represent each word in 200 dimension space. We feed the word2vec model our tokens, and it will generate a N-dimension vector for each token. We can use comparisons in this N dimensional space to determine how similar two words are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import inspect\n",
    "  \n",
    "# use signature()\n",
    "print(inspect.signature(Word2Vec))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model Architecture\n",
    "\n",
    "We've mentioned that the word2vec model we are making is a neural network. Neural networks, as we'll see later, have an architecture, or basically a size and design. The details don't matter too much to us yet, but one thing that we can change when determing our model in word2vec is that architecture - we can choose between CBOW and Skip-Gram. These two options are roughly opposites of each other. The details of how they work and how they differ are neural network details, so we'll set those details aside for now.\n",
    "\n",
    "<b>Continuous Bag of Words</b></br>\n",
    "![CBOW](images/cbow.webp \"CBOW\")\n",
    "\n",
    "<b>Skip-gram</b><br>\n",
    "![Skip-Gram](images/skip_gram.webp \"Skip-Gram\")\n",
    "\n",
    "These two models look like mirror images of each other, but what do they mean? Each does the same thing, though in a slightly different way. \n",
    "<ul>\n",
    "<li> CBOW: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at a word and tries to predict the surrounding words.\n",
    "</ul>\n",
    "\n",
    "For us, we can ignore the details of the differnece and think of the two options similarly to other options like regularization or entropy/gini. The way the internal neural network learns is different in the different architectures. \n",
    "\n",
    "#### Which to Use?\n",
    "\n",
    "For the most part, the real answer is our favorite one - test and choose the best. In general:\n",
    "<ul>\n",
    "<li> Skip Gram tends to work well with small amount of data and is found to represent rare words well.\n",
    "<li> CBOW is normally faster and has better representations for more frequent words.\n",
    "</ul>\n",
    "\n",
    "Parameters other than the ones we have listed here can be tweaked, but we'll somewhat ignore them for now, we're ok with the defaults. The \"sg\" parameter is the one that controls the architecture - 1 is skip-gram, 0 is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model = Word2Vec(df['clean_text'],min_count=3, vector_size=200, sg=1)\n",
    "#min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#combination of word and its vector\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model\n",
    "\n",
    "Each word in the vocabulary now has a vector representing it - of size 200. We can make a dataframe and see each token in our text and its vector representation. This vector is the internal representation of each token that is generated by Word2Vec. This is how the algorithm calculates things like similarity...\n",
    "\n",
    "The word2vec result that we are printing out here is each word in our vocabulary and its vector representation - or all of its dimensions in the 200D space we created while the model was trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>nt</th>\n",
       "      <th>get</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>ur</th>\n",
       "      <th>You</th>\n",
       "      <th>go</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>...</th>\n",
       "      <th>community</th>\n",
       "      <th>88039</th>\n",
       "      <th>SkilGme</th>\n",
       "      <th>meetin</th>\n",
       "      <th>ti</th>\n",
       "      <th>anywhere</th>\n",
       "      <th>opening</th>\n",
       "      <th>diff</th>\n",
       "      <th>living</th>\n",
       "      <th>boye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.148444</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.067295</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>-0.022945</td>\n",
       "      <td>0.114367</td>\n",
       "      <td>0.085709</td>\n",
       "      <td>0.064109</td>\n",
       "      <td>0.063963</td>\n",
       "      <td>0.054185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040629</td>\n",
       "      <td>0.041977</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.048504</td>\n",
       "      <td>0.024622</td>\n",
       "      <td>0.029028</td>\n",
       "      <td>0.045932</td>\n",
       "      <td>0.037331</td>\n",
       "      <td>0.044939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038233</td>\n",
       "      <td>-0.004675</td>\n",
       "      <td>-0.027695</td>\n",
       "      <td>-0.091960</td>\n",
       "      <td>-0.052563</td>\n",
       "      <td>-0.062537</td>\n",
       "      <td>-0.041445</td>\n",
       "      <td>0.008905</td>\n",
       "      <td>-0.032066</td>\n",
       "      <td>-0.029308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020359</td>\n",
       "      <td>-0.019174</td>\n",
       "      <td>-0.018625</td>\n",
       "      <td>-0.018485</td>\n",
       "      <td>-0.018153</td>\n",
       "      <td>-0.009105</td>\n",
       "      <td>-0.013288</td>\n",
       "      <td>-0.015207</td>\n",
       "      <td>-0.015266</td>\n",
       "      <td>-0.018489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091548</td>\n",
       "      <td>0.017172</td>\n",
       "      <td>0.053989</td>\n",
       "      <td>0.255615</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.097066</td>\n",
       "      <td>0.066605</td>\n",
       "      <td>0.028013</td>\n",
       "      <td>0.042960</td>\n",
       "      <td>0.073416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.048582</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>0.048829</td>\n",
       "      <td>0.037020</td>\n",
       "      <td>0.024018</td>\n",
       "      <td>0.024956</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.039022</td>\n",
       "      <td>0.043074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.161302</td>\n",
       "      <td>0.074408</td>\n",
       "      <td>0.107433</td>\n",
       "      <td>0.365333</td>\n",
       "      <td>0.379281</td>\n",
       "      <td>0.037254</td>\n",
       "      <td>0.053775</td>\n",
       "      <td>0.046690</td>\n",
       "      <td>0.063166</td>\n",
       "      <td>0.104828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>0.036919</td>\n",
       "      <td>0.030299</td>\n",
       "      <td>0.043386</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.033105</td>\n",
       "      <td>0.027924</td>\n",
       "      <td>0.034405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.184827</td>\n",
       "      <td>0.132882</td>\n",
       "      <td>0.165953</td>\n",
       "      <td>0.323456</td>\n",
       "      <td>0.349884</td>\n",
       "      <td>0.183901</td>\n",
       "      <td>0.156966</td>\n",
       "      <td>0.135485</td>\n",
       "      <td>0.144360</td>\n",
       "      <td>0.166430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079263</td>\n",
       "      <td>0.097863</td>\n",
       "      <td>0.070170</td>\n",
       "      <td>0.102670</td>\n",
       "      <td>0.088945</td>\n",
       "      <td>0.051913</td>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.079552</td>\n",
       "      <td>0.081643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.104215</td>\n",
       "      <td>0.063988</td>\n",
       "      <td>0.070005</td>\n",
       "      <td>-0.079753</td>\n",
       "      <td>-0.066267</td>\n",
       "      <td>0.085028</td>\n",
       "      <td>0.071899</td>\n",
       "      <td>0.122864</td>\n",
       "      <td>0.071898</td>\n",
       "      <td>0.053523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030069</td>\n",
       "      <td>0.038823</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.046448</td>\n",
       "      <td>0.038981</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>0.028290</td>\n",
       "      <td>0.044103</td>\n",
       "      <td>0.033981</td>\n",
       "      <td>0.038333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.042627</td>\n",
       "      <td>0.084556</td>\n",
       "      <td>0.052244</td>\n",
       "      <td>-0.127450</td>\n",
       "      <td>-0.139348</td>\n",
       "      <td>0.067103</td>\n",
       "      <td>0.081465</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.068133</td>\n",
       "      <td>0.037919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026394</td>\n",
       "      <td>0.034827</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>0.032232</td>\n",
       "      <td>0.022935</td>\n",
       "      <td>0.026195</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.033431</td>\n",
       "      <td>0.026697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.336169</td>\n",
       "      <td>-0.099395</td>\n",
       "      <td>-0.166457</td>\n",
       "      <td>-0.276381</td>\n",
       "      <td>-0.241740</td>\n",
       "      <td>-0.240602</td>\n",
       "      <td>-0.201684</td>\n",
       "      <td>-0.077809</td>\n",
       "      <td>-0.132518</td>\n",
       "      <td>-0.115541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087906</td>\n",
       "      <td>-0.114379</td>\n",
       "      <td>-0.087989</td>\n",
       "      <td>-0.112439</td>\n",
       "      <td>-0.106283</td>\n",
       "      <td>-0.065522</td>\n",
       "      <td>-0.073919</td>\n",
       "      <td>-0.108034</td>\n",
       "      <td>-0.094705</td>\n",
       "      <td>-0.103644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.172427</td>\n",
       "      <td>-0.054020</td>\n",
       "      <td>-0.033489</td>\n",
       "      <td>0.218018</td>\n",
       "      <td>0.189746</td>\n",
       "      <td>-0.105011</td>\n",
       "      <td>-0.104539</td>\n",
       "      <td>-0.047957</td>\n",
       "      <td>-0.056027</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036832</td>\n",
       "      <td>-0.044260</td>\n",
       "      <td>-0.028509</td>\n",
       "      <td>-0.044625</td>\n",
       "      <td>-0.046171</td>\n",
       "      <td>-0.025418</td>\n",
       "      <td>-0.025121</td>\n",
       "      <td>-0.041894</td>\n",
       "      <td>-0.037110</td>\n",
       "      <td>-0.039461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.071436</td>\n",
       "      <td>-0.095333</td>\n",
       "      <td>-0.041260</td>\n",
       "      <td>-0.086006</td>\n",
       "      <td>-0.053365</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>-0.005500</td>\n",
       "      <td>-0.061997</td>\n",
       "      <td>-0.065806</td>\n",
       "      <td>-0.056961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>-0.006205</td>\n",
       "      <td>-0.009349</td>\n",
       "      <td>-0.007575</td>\n",
       "      <td>-0.001635</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>-0.003940</td>\n",
       "      <td>-0.009226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         call        nt       get        gt        lt        ur       You  \\\n",
       "0    0.148444  0.063375  0.067295  0.016624 -0.022945  0.114367  0.085709   \n",
       "1   -0.038233 -0.004675 -0.027695 -0.091960 -0.052563 -0.062537 -0.041445   \n",
       "2    0.091548  0.017172  0.053989  0.255615  0.270400  0.097066  0.066605   \n",
       "3    0.161302  0.074408  0.107433  0.365333  0.379281  0.037254  0.053775   \n",
       "4    0.184827  0.132882  0.165953  0.323456  0.349884  0.183901  0.156966   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.104215  0.063988  0.070005 -0.079753 -0.066267  0.085028  0.071899   \n",
       "196  0.042627  0.084556  0.052244 -0.127450 -0.139348  0.067103  0.081465   \n",
       "197 -0.336169 -0.099395 -0.166457 -0.276381 -0.241740 -0.240602 -0.201684   \n",
       "198 -0.172427 -0.054020 -0.033489  0.218018  0.189746 -0.105011 -0.104539   \n",
       "199  0.071436 -0.095333 -0.041260 -0.086006 -0.053365  0.003383 -0.005500   \n",
       "\n",
       "           go      know      like  ...  community     88039   SkilGme  \\\n",
       "0    0.064109  0.063963  0.054185  ...   0.040629  0.041977  0.037886   \n",
       "1    0.008905 -0.032066 -0.029308  ...  -0.020359 -0.019174 -0.018625   \n",
       "2    0.028013  0.042960  0.073416  ...   0.032520  0.048582  0.038207   \n",
       "3    0.046690  0.063166  0.104828  ...   0.028579  0.036919  0.030299   \n",
       "4    0.135485  0.144360  0.166430  ...   0.079263  0.097863  0.070170   \n",
       "..        ...       ...       ...  ...        ...       ...       ...   \n",
       "195  0.122864  0.071898  0.053523  ...   0.030069  0.038823  0.031893   \n",
       "196  0.088400  0.068133  0.037919  ...   0.026394  0.034827  0.022689   \n",
       "197 -0.077809 -0.132518 -0.115541  ...  -0.087906 -0.114379 -0.087989   \n",
       "198 -0.047957 -0.056027 -0.005327  ...  -0.036832 -0.044260 -0.028509   \n",
       "199 -0.061997 -0.065806 -0.056961  ...   0.000587 -0.001575  0.001124   \n",
       "\n",
       "       meetin        ti  anywhere   opening      diff    living      boye  \n",
       "0    0.050819  0.048504  0.024622  0.029028  0.045932  0.037331  0.044939  \n",
       "1   -0.018485 -0.018153 -0.009105 -0.013288 -0.015207 -0.015266 -0.018489  \n",
       "2    0.048829  0.037020  0.024018  0.024956  0.041867  0.039022  0.043074  \n",
       "3    0.043386  0.032707  0.017042  0.027508  0.033105  0.027924  0.034405  \n",
       "4    0.102670  0.088945  0.051913  0.063504  0.087912  0.079552  0.081643  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.046448  0.038981  0.027305  0.028290  0.044103  0.033981  0.038333  \n",
       "196  0.031278  0.032232  0.022935  0.026195  0.033230  0.033431  0.026697  \n",
       "197 -0.112439 -0.106283 -0.065522 -0.073919 -0.108034 -0.094705 -0.103644  \n",
       "198 -0.044625 -0.046171 -0.025418 -0.025121 -0.041894 -0.037110 -0.039461  \n",
       "199 -0.006205 -0.009349 -0.007575 -0.001635 -0.002524 -0.003940 -0.009226  \n",
       "\n",
       "[200 rows x 3150 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(w2v)\n",
    "vectors = model.wv\n",
    "tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Similarity\n",
    "\n",
    "One of the things that Word2Vec allows us to do is to look at the similarity of words. This similarity is calculated via the cosine distance of the vectors. Cosine similarity is a technique to calculate the distance between two vectors - smaller distance, more similar. \n",
    "\n",
    "![Cosine Similarity](images/cosine_sim.png \"Cosine Similarity\" )\n",
    "\n",
    "Once the vectors are derived by in the training process, these similarity calculations are pretty easy and quick. \n",
    "\n",
    "<b>Note:</b> the similarites here are calculated by the values derived from our trained model. So they are based on the relationships in our text. Word2Vec and other NLP packages also commonly have pretrained models that can be downloaded that are based on large amounts of text. Words may be represented very differently in those vs whatever we train here - the more data we have, the more consistent they'll be; the more \"unique\" our text is, the more different it will be. If we were, for example, working in a specific domain such as patent law, we could use a large amount of patent law text to train a model that would be more consistent with our domain. Or, perhaps more likely, we could use a pretrained model that has been created with massive amounts of training data. \n",
    "\n",
    "### Types of Similarity\n",
    "\n",
    "When looking at the similarity of different words, we can measure that similarity in a couple of ways - lexical and semantic, that we mentioned before. Here, the model is looking at semantic similarity, the \"meaning\" of each word, in the context of our text, is being compared and the most similar words are returned. Note that we can only calculate similarity here for words that we have in our vocabulary. This is one place where large language models like chat GPT have a massive advantage, their vocabulary is huge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('want', 0.9961519837379456),\n",
       " ('ca', 0.9947837591171265),\n",
       " ('thing', 0.9945001602172852)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the most similar word to anything in our vocabulary \n",
    "vectors.most_similar(\"know\")[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do ok\n",
      "0.9721367\n"
     ]
    }
   ],
   "source": [
    "# We can also see how similar different words are. \n",
    "# I will grab two arbitrary words from the vocabulary and see how similar they are.\n",
    "# you could use anything in the vocabulary here, try some other ones!\n",
    "\n",
    "word_a = tmp.columns[30]\n",
    "word_b = tmp.columns[40]\n",
    "\n",
    "print(word_a, word_b)\n",
    "print(vectors.similarity(word_a, word_b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "We can take our actual data now and transform it through the Word2Vec model that we've made. This will generate our smaller feature set that we can build our models from, one of the things that the MeanEmbeddingVectorizer does is to collapse the data down to those 200 dimensions in the vector. Our dataset is spit out the other end, each row of text is now represented by a single vector of those 200 dimensions of our embedding values from the word2vec model (the columns).\n",
    "\n",
    "<b>Note:</b> if this is confusing, please ignore it, this is a bit of a tangent. The meanembeddingvectorizer thing is needed to \"flatten\" our data down from 200D to 1D for each token. This is because our models can only dal with data that is in that format (instances x features). We can't have a 200D vector for each token, we need to collapse it down to a single value. Later, when we look at neural networks, we'll see models with differnet architectures that can accomadate data that is multidimensional like this. That's one of the reasons that neural networks are so powerful, they can accomadate data that is multidimensional, so something like an image can be treated like an image, not just a bunch of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 200) (1393, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067330</td>\n",
       "      <td>-0.031478</td>\n",
       "      <td>0.070907</td>\n",
       "      <td>0.078295</td>\n",
       "      <td>0.158155</td>\n",
       "      <td>-0.123093</td>\n",
       "      <td>-0.087629</td>\n",
       "      <td>0.210541</td>\n",
       "      <td>-0.034585</td>\n",
       "      <td>0.125569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097020</td>\n",
       "      <td>-0.078663</td>\n",
       "      <td>-0.034641</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>0.135933</td>\n",
       "      <td>0.066504</td>\n",
       "      <td>0.046844</td>\n",
       "      <td>-0.170139</td>\n",
       "      <td>-0.054585</td>\n",
       "      <td>-0.018949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072015</td>\n",
       "      <td>-0.020641</td>\n",
       "      <td>0.058071</td>\n",
       "      <td>0.075310</td>\n",
       "      <td>0.146229</td>\n",
       "      <td>-0.117535</td>\n",
       "      <td>-0.086293</td>\n",
       "      <td>0.220376</td>\n",
       "      <td>-0.025747</td>\n",
       "      <td>0.114096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103871</td>\n",
       "      <td>-0.075003</td>\n",
       "      <td>-0.047092</td>\n",
       "      <td>-0.045035</td>\n",
       "      <td>0.123453</td>\n",
       "      <td>0.080726</td>\n",
       "      <td>0.061001</td>\n",
       "      <td>-0.152627</td>\n",
       "      <td>-0.070114</td>\n",
       "      <td>-0.023008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065776</td>\n",
       "      <td>-0.019488</td>\n",
       "      <td>0.055875</td>\n",
       "      <td>0.084187</td>\n",
       "      <td>0.151737</td>\n",
       "      <td>-0.101624</td>\n",
       "      <td>-0.081254</td>\n",
       "      <td>0.245679</td>\n",
       "      <td>-0.047375</td>\n",
       "      <td>0.118113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>-0.058960</td>\n",
       "      <td>-0.059484</td>\n",
       "      <td>-0.056558</td>\n",
       "      <td>0.123451</td>\n",
       "      <td>0.075468</td>\n",
       "      <td>0.065698</td>\n",
       "      <td>-0.138904</td>\n",
       "      <td>-0.055643</td>\n",
       "      <td>-0.050339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.073679</td>\n",
       "      <td>-0.031574</td>\n",
       "      <td>0.068011</td>\n",
       "      <td>0.062848</td>\n",
       "      <td>0.154091</td>\n",
       "      <td>-0.127947</td>\n",
       "      <td>-0.098683</td>\n",
       "      <td>0.212830</td>\n",
       "      <td>-0.026109</td>\n",
       "      <td>0.131231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109067</td>\n",
       "      <td>-0.088681</td>\n",
       "      <td>-0.032604</td>\n",
       "      <td>-0.037347</td>\n",
       "      <td>0.135932</td>\n",
       "      <td>0.076831</td>\n",
       "      <td>0.056990</td>\n",
       "      <td>-0.178684</td>\n",
       "      <td>-0.071122</td>\n",
       "      <td>-0.014774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075539</td>\n",
       "      <td>-0.028012</td>\n",
       "      <td>0.061126</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>-0.120229</td>\n",
       "      <td>-0.097460</td>\n",
       "      <td>0.228929</td>\n",
       "      <td>-0.030222</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108183</td>\n",
       "      <td>-0.075713</td>\n",
       "      <td>-0.040561</td>\n",
       "      <td>-0.053916</td>\n",
       "      <td>0.129965</td>\n",
       "      <td>0.080973</td>\n",
       "      <td>0.066813</td>\n",
       "      <td>-0.163958</td>\n",
       "      <td>-0.069962</td>\n",
       "      <td>-0.027575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.078612</td>\n",
       "      <td>-0.029912</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>0.064049</td>\n",
       "      <td>0.158875</td>\n",
       "      <td>-0.131994</td>\n",
       "      <td>-0.095947</td>\n",
       "      <td>0.219392</td>\n",
       "      <td>-0.025709</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110893</td>\n",
       "      <td>-0.087051</td>\n",
       "      <td>-0.033683</td>\n",
       "      <td>-0.036254</td>\n",
       "      <td>0.135953</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.060425</td>\n",
       "      <td>-0.177465</td>\n",
       "      <td>-0.070211</td>\n",
       "      <td>-0.018766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.061854</td>\n",
       "      <td>-0.057025</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.178205</td>\n",
       "      <td>-0.154648</td>\n",
       "      <td>-0.101647</td>\n",
       "      <td>0.206305</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>0.140963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122335</td>\n",
       "      <td>-0.096665</td>\n",
       "      <td>-0.033226</td>\n",
       "      <td>-0.027986</td>\n",
       "      <td>0.124033</td>\n",
       "      <td>0.049873</td>\n",
       "      <td>0.045605</td>\n",
       "      <td>-0.184484</td>\n",
       "      <td>-0.086276</td>\n",
       "      <td>0.008379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.081061</td>\n",
       "      <td>-0.040857</td>\n",
       "      <td>0.075242</td>\n",
       "      <td>0.060781</td>\n",
       "      <td>0.164685</td>\n",
       "      <td>-0.136626</td>\n",
       "      <td>-0.105341</td>\n",
       "      <td>0.215421</td>\n",
       "      <td>-0.025633</td>\n",
       "      <td>0.136803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113703</td>\n",
       "      <td>-0.093922</td>\n",
       "      <td>-0.027959</td>\n",
       "      <td>-0.027211</td>\n",
       "      <td>0.146967</td>\n",
       "      <td>0.075703</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>-0.191305</td>\n",
       "      <td>-0.074009</td>\n",
       "      <td>-0.010794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>0.051889</td>\n",
       "      <td>-0.039262</td>\n",
       "      <td>0.109287</td>\n",
       "      <td>0.140094</td>\n",
       "      <td>0.194180</td>\n",
       "      <td>-0.113960</td>\n",
       "      <td>-0.048785</td>\n",
       "      <td>0.235141</td>\n",
       "      <td>-0.077077</td>\n",
       "      <td>0.138319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073464</td>\n",
       "      <td>-0.066842</td>\n",
       "      <td>-0.052576</td>\n",
       "      <td>0.055743</td>\n",
       "      <td>0.164489</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>0.014546</td>\n",
       "      <td>-0.182704</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>-0.038318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>0.103340</td>\n",
       "      <td>-0.055275</td>\n",
       "      <td>0.065998</td>\n",
       "      <td>0.068483</td>\n",
       "      <td>0.175227</td>\n",
       "      <td>-0.173268</td>\n",
       "      <td>-0.119547</td>\n",
       "      <td>0.233281</td>\n",
       "      <td>-0.018184</td>\n",
       "      <td>0.165296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133199</td>\n",
       "      <td>-0.120234</td>\n",
       "      <td>-0.038438</td>\n",
       "      <td>-0.048985</td>\n",
       "      <td>0.166338</td>\n",
       "      <td>0.089687</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>-0.265692</td>\n",
       "      <td>-0.083953</td>\n",
       "      <td>-0.004604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.067330 -0.031478  0.070907  0.078295  0.158155 -0.123093 -0.087629   \n",
       "1     0.072015 -0.020641  0.058071  0.075310  0.146229 -0.117535 -0.086293   \n",
       "2     0.065776 -0.019488  0.055875  0.084187  0.151737 -0.101624 -0.081254   \n",
       "3     0.073679 -0.031574  0.068011  0.062848  0.154091 -0.127947 -0.098683   \n",
       "4     0.075539 -0.028012  0.061126  0.063572  0.151786 -0.120229 -0.097460   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174  0.078612 -0.029912  0.068667  0.064049  0.158875 -0.131994 -0.095947   \n",
       "4175  0.061854 -0.057025  0.072266  0.038116  0.178205 -0.154648 -0.101647   \n",
       "4176  0.081061 -0.040857  0.075242  0.060781  0.164685 -0.136626 -0.105341   \n",
       "4177  0.051889 -0.039262  0.109287  0.140094  0.194180 -0.113960 -0.048785   \n",
       "4178  0.103340 -0.055275  0.065998  0.068483  0.175227 -0.173268 -0.119547   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.210541 -0.034585  0.125569  ...  0.097020 -0.078663 -0.034641   \n",
       "1     0.220376 -0.025747  0.114096  ...  0.103871 -0.075003 -0.047092   \n",
       "2     0.245679 -0.047375  0.118113  ...  0.098110 -0.058960 -0.059484   \n",
       "3     0.212830 -0.026109  0.131231  ...  0.109067 -0.088681 -0.032604   \n",
       "4     0.228929 -0.030222  0.122837  ...  0.108183 -0.075713 -0.040561   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4174  0.219392 -0.025709  0.128633  ...  0.110893 -0.087051 -0.033683   \n",
       "4175  0.206305 -0.019283  0.140963  ...  0.122335 -0.096665 -0.033226   \n",
       "4176  0.215421 -0.025633  0.136803  ...  0.113703 -0.093922 -0.027959   \n",
       "4177  0.235141 -0.077077  0.138319  ...  0.073464 -0.066842 -0.052576   \n",
       "4178  0.233281 -0.018184  0.165296  ...  0.133199 -0.120234 -0.038438   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0    -0.016192  0.135933  0.066504  0.046844 -0.170139 -0.054585 -0.018949  \n",
       "1    -0.045035  0.123453  0.080726  0.061001 -0.152627 -0.070114 -0.023008  \n",
       "2    -0.056558  0.123451  0.075468  0.065698 -0.138904 -0.055643 -0.050339  \n",
       "3    -0.037347  0.135932  0.076831  0.056990 -0.178684 -0.071122 -0.014774  \n",
       "4    -0.053916  0.129965  0.080973  0.066813 -0.163958 -0.069962 -0.027575  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4174 -0.036254  0.135953  0.073171  0.060425 -0.177465 -0.070211 -0.018766  \n",
       "4175 -0.027986  0.124033  0.049873  0.045605 -0.184484 -0.086276  0.008379  \n",
       "4176 -0.027211  0.146967  0.075703  0.056903 -0.191305 -0.074009 -0.010794  \n",
       "4177  0.055743  0.164489  0.035839  0.014546 -0.182704 -0.000544 -0.038318  \n",
       "4178 -0.048985  0.166338  0.089687  0.055262 -0.265692 -0.083953 -0.004604  \n",
       "\n",
       "[4179 rows x 200 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"],df[\"target2\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)\n",
    "print(X_train_vectors_w2v.shape, X_test_vectors_w2v.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model\n",
    "\n",
    "We now have a pretty normal dataset and can use the new data to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9839124432884692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1194\n",
      "           1       0.93      0.86      0.90       199\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.95      0.93      0.94      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_w2v = RandomForestClassifier()\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Word2Vec\n",
    "\n",
    "Use the newsgroup data and Word2Vec to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datsets and Tokenize\n",
    "tok = lemmaTokenizer(stop_words)\n",
    "X_w2v_news_train = [tok(x) for x in data_train.data]\n",
    "X_w2v_news_test = [tok(x) for x in data_test.data]\n",
    "\n",
    "y_train_news = data_train.target\n",
    "y_test_news = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benedikt',\n",
       " 'Rosenau',\n",
       " 'writes',\n",
       " 'great',\n",
       " 'authority',\n",
       " 'Contradictory',\n",
       " 'property',\n",
       " 'language',\n",
       " 'If',\n",
       " 'correct',\n",
       " 'THINGS',\n",
       " 'DEFINED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 'object',\n",
       " 'definition',\n",
       " 'reality',\n",
       " 'If',\n",
       " 'amend',\n",
       " 'THINGS',\n",
       " 'DESCRIBED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 've',\n",
       " 'come',\n",
       " 'something',\n",
       " 'plainly',\n",
       " 'false',\n",
       " 'Failures',\n",
       " 'description',\n",
       " 'merely',\n",
       " 'failure',\n",
       " 'description',\n",
       " 'objectivist',\n",
       " 'remember']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview\n",
    "X_w2v_news_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model_news = Word2Vec(X_w2v_news_train, min_count=1, vector_size=200)\n",
    "w2v_news = dict(zip(model_news.wv.index_to_key, model_news.wv.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_news_w = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_news = model_news_w.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_news = model_news_w.transform(X_w2v_news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n",
      "Confusion Matrix:\n",
      " [[300  19]\n",
      " [236  15]]\n",
      "AUC: 0.5253343990807928\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf = SVC(probability=True)\n",
    "news_clf.fit(X_train_vectors_w2v_news, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news = news_clf.predict(X_val_vectors_w2v_news)\n",
    "y_prob_news = news_clf.predict_proba(X_val_vectors_w2v_news)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test_news, y_predict_news))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_news, y_prob_news)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec Models\n",
    "\n",
    "When we do the intial training of the word2vec model (not when we are making final predictions), we are using our corpus to generate the space and the embeddigns for all of our tokens. We can also download a pretrained model, that already has the N-dimensional space defined (when it was trained on some different data), and use that to generate our embeddings. This is a common practice, and can be a good way to get started. Gensim has several models that have been trained on varying amounts of data, they are listed here: https://github.com/RaRe-Technologies/gensim-data along with several other datasets that we could use to train a model. \n",
    "\n",
    "The differences with using this pretrained model (or an existing corpus below) are:\n",
    "<ul>\n",
    "<li> Above, when training word2vec with our data, we used our corpus to generate the space in which the tokens are placed, then calculate those embeddings for each token. \n",
    "<li> With a pretrained model, we are using the space that was generated by the model that was trained on some other data, then placing our tokens in that space. \n",
    "</ul>\n",
    "\n",
    "So if we are using some text from wikipedia (like the second example), the space in which embeddings are made is defined by the text in wikipedia. So the \"closeness\" in meaning of words is based on what is in that corpus. We then take our tokens and calculate their embeddings in that space. The big advantage to this is someone else can train a model on lots of data, which hopefully generates a better understanding of the relationships between words, and we can then just score our words on those scales. This approach is common in large models, like text processing or image recognition, where the training load can be too large for \"regular folk\". We can also take these trained models and \"customize\" them to our data, we'll look at that with image recognition at the end of the semester. \n",
    "\n",
    "#### Use a Twitter Trained Model\n",
    "\n",
    "We can try using a pretrained model that was trained on Twitter data. This model has been pretrained, so it already knows how to represent words, we will then feed it all of our tokens, and it will generate the embeddings for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 0.9448828101158142),\n",
       " ('baby', 0.9425430297851562),\n",
       " ('dream', 0.9267040491104126),\n",
       " ('miss', 0.9246909022331238),\n",
       " ('much', 0.9215252995491028),\n",
       " ('see', 0.919786810874939),\n",
       " ('happy', 0.9176183938980103),\n",
       " ('beautiful', 0.9173233509063721),\n",
       " ('smile', 0.9138967394828796),\n",
       " ('loves', 0.9123677611351013)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downlaod the model and do a little test\n",
    "\n",
    "import gensim.downloader as api\n",
    "model_twit = api.load(\"glove-twitter-25\")\n",
    "model_twit.most_similar(\"love\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings\n",
    "\n",
    "The model exists, so we will use it to transform our tokens into numerical representations. Then we can go use those to make classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.58      0.71       517\n",
      "           1       0.13      0.60      0.21        53\n",
      "\n",
      "    accuracy                           0.58       570\n",
      "   macro avg       0.53      0.59      0.46       570\n",
      "weighted avg       0.86      0.58      0.67       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_twit = dict(zip(model_twit.index_to_key, model_twit.vectors))\n",
    "model_twit_emb = MeanEmbeddingVectorizer(w2v_twit)\n",
    "\n",
    "X_train_twit = model_twit_emb.transform(X_w2v_news_train)\n",
    "X_test_twit = model_twit_emb.transform(X_w2v_news_test)\n",
    "\n",
    "# Make predictions\n",
    "twit_clf = SVC(probability=True)\n",
    "twit_clf.fit(X_train_twit, y_train_news)  #model\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict_twit = twit_clf.predict(X_test_twit)\n",
    "\n",
    "print(classification_report(y_predict_twit, y_test_news))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premade Corpus\n",
    "\n",
    "We can also train a model directly from a preexisting corpus, then generate our embeddings from that model. \n",
    "\n",
    "The \"text8\" corpus is a small corpus of text that is included with gensim. It is a small subset of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')\n",
    "model_corp = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 200)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corp = dict(zip(model_corp.wv.index_to_key, model_corp.wv.vectors)) \n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_corp_emb = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_test)\n",
    "X_train_vectors_w2v_corp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf_corp = SVC(probability=True)\n",
    "news_clf_corp.fit(X_train_vectors_w2v_corp, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news_corp = news_clf.predict(X_val_vectors_w2v_corp)\n",
    "y_prob_news_corp = news_clf.predict_proba(X_val_vectors_w2v_corp)[:,1]\n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news_corp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Me!\n",
    "\n",
    "As we see with things like chatGPT and the assortment of voice assistants, NLP is currently exploding in both capability and prevelence. Those other models are based on these concepts, but there are a few key differences that help those tools be more powerful:\n",
    "<ul>\n",
    "<li> They are trained on much larger datasets. Very, very, very large datasets. In NLP specifically, this helps because it can help address the problem with us having so many words, many of which aren't used super often - i.e. the fact that there are a lot of words that don't occur together in the same sentence. If the training data is massive (e.g. \"the internet\"), we massively reduce the impact of this problem, as we see each word many times. \n",
    "<li> The use of neural networks, in particular recurrant neural networks (RNNs) that are able to deal with data as a sequence, and \"remember\" other parts of a sequence of words. This helps these models understand the context of a sentence, and the relationships between words.\n",
    "    <ul>\n",
    "    <li> Of note with neural networks, especially those using massive training data sets, is that the first layers of the model can perform equivalent data prep work that we've done here. So the model is more able to deal with data in its raw form, and doesn't need to be preprocessed as much separately, in advance. \n",
    "    </ul>\n",
    "<li> Manual intervention is used, humans provide examples of convesation, define labels, and evaluate the quality of the model's work. You may have heard news of Kenyans being paid low wages to label data for these models.\n",
    "<li> Other model types are used to help, such as reinforcement learning. Responses that are good are rewarded, and those that are bad are punished. This helps the model learn what is good and what is bad. This is particularly useful for generative models, such as chatGPT.\n",
    "</ul>\n",
    "\n",
    "As noted, this stuff is actively being developed right now, and the more advanced the tool, the more likely we are to see innovation or specific interventions to correct issues. The foundations we have looked at here are the building blocks of that work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a67d3d74f81673499695f5753f7ab28afe8f7c82c0a4946d9a7d056c03b92cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
