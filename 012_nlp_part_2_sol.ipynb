{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More NLP\n",
    "\n",
    "## Truncated Singular Value Decomposition and Dimensionality Reduction\n",
    "\n",
    "When processing text we end up with feature sets that are large! There is up to one feature per different word in our text sample, as well as more for multi-word combinations if there are larger ngrams allowed, far larger than a typical feature set that we're used to. One thing we can do when vectorizing is just to cap the number of features we end up with, but that doesn't seem to be the most sophisticated or smartest approach. \n",
    "\n",
    "TSVD is one thing that we can do to chop down the feature set - or reduce the dimensions - with a little more thought. \n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is a common technique in machine learning, it does its name - reduces the dimensions in our feature data. We often want to do this for several reasons: \n",
    "<ul>\n",
    "<li> To reduce the amount of time it takes to train a model.\n",
    "<li> To reduce the amount of memory required to store the data.\n",
    "<li> To reduce the amount of noise in the data.\n",
    "<li> To make the data more interpretable.\n",
    "<li> To make the data more amenable to visualization.\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset from Last Time\n",
    "\n",
    "We'll load the spam dataset and vectorize it with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (5572,)\n",
      "vectorized: (5572, 89635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 easter</th>\n",
       "      <th>00 easter prize</th>\n",
       "      <th>00 easter prize draw</th>\n",
       "      <th>00 sub</th>\n",
       "      <th>00 sub 16</th>\n",
       "      <th>00 sub 16 remove</th>\n",
       "      <th>00 sub 16 unsub</th>\n",
       "      <th>00 subs</th>\n",
       "      <th>00 subs 16</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom cine actually</th>\n",
       "      <th>zoom cine actually tonight</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zouk nichols</th>\n",
       "      <th>zouk nichols paris</th>\n",
       "      <th>zouk nichols paris free</th>\n",
       "      <th>zyada</th>\n",
       "      <th>zyada kisi</th>\n",
       "      <th>zyada kisi ko</th>\n",
       "      <th>zyada kisi ko kuch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5475</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4854</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 easter  00 easter prize  00 easter prize draw  00 sub  \\\n",
       "5475  0.0        0.0              0.0                   0.0     0.0   \n",
       "3876  0.0        0.0              0.0                   0.0     0.0   \n",
       "3786  0.0        0.0              0.0                   0.0     0.0   \n",
       "902   0.0        0.0              0.0                   0.0     0.0   \n",
       "4854  0.0        0.0              0.0                   0.0     0.0   \n",
       "\n",
       "      00 sub 16  00 sub 16 remove  00 sub 16 unsub  00 subs  00 subs 16  ...  \\\n",
       "5475        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3876        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "3786        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "902         0.0               0.0              0.0      0.0         0.0  ...   \n",
       "4854        0.0               0.0              0.0      0.0         0.0  ...   \n",
       "\n",
       "      zoom cine actually  zoom cine actually tonight  zouk  zouk nichols  \\\n",
       "5475                 0.0                         0.0   0.0           0.0   \n",
       "3876                 0.0                         0.0   0.0           0.0   \n",
       "3786                 0.0                         0.0   0.0           0.0   \n",
       "902                  0.0                         0.0   0.0           0.0   \n",
       "4854                 0.0                         0.0   0.0           0.0   \n",
       "\n",
       "      zouk nichols paris  zouk nichols paris free  zyada  zyada kisi  \\\n",
       "5475                 0.0                      0.0    0.0         0.0   \n",
       "3876                 0.0                      0.0    0.0         0.0   \n",
       "3786                 0.0                      0.0    0.0         0.0   \n",
       "902                  0.0                      0.0    0.0         0.0   \n",
       "4854                 0.0                      0.0    0.0         0.0   \n",
       "\n",
       "      zyada kisi ko  zyada kisi ko kuch  \n",
       "5475            0.0                 0.0  \n",
       "3876            0.0                 0.0  \n",
       "3786            0.0                 0.0  \n",
       "902             0.0                 0.0  \n",
       "4854            0.0                 0.0  \n",
       "\n",
       "[5 rows x 89635 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Data\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding=\"ISO-8859-1\")\n",
    "df.drop(columns={\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"}, inplace=True)\n",
    "df.rename(columns={\"v1\":\"target\", \"v2\":\"text\"}, inplace=True)\n",
    "#TF-IDF\n",
    "vec_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,4), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp = vec_tf.fit_transform(df[\"text\"])\n",
    "tok_cols = vec_tf.get_feature_names()\n",
    "tok_df = pd.DataFrame(tmp.toarray(), columns=tok_cols)\n",
    "print(\"original:\", df[\"text\"].shape)\n",
    "print(\"vectorized:\", tmp.shape)\n",
    "y = df[\"target\"]\n",
    "X = df[\"text\"]\n",
    "tok_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA - Latent Semantic Analysis\n",
    "\n",
    "The TSVD performs somehting called latent semantic analysis. The process of LSA and the math behind it are not something we need to explore in detail. (LSA is often called LSI - Latent Semantic Indexing). The idea of LSA is that it can generate \"concepts\" in the text. These concepts are found by looking at which terms occur in which documents - documents that have the same terms repeated are likely related to the same concept; other documents that share other words with those documents are likely on the same concept as well.  \n",
    "\n",
    "An important part is the word \"Latent\" - i.e. the patterns detected are hidden, not explicit in the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement SVD to Trim Dataset\n",
    "\n",
    "We are starting with LOTS of feature inputs. Below we can loop through several models of different number of remaining components to see the accuracy depending on the number of features we keep in the feature set. The truncated part of truncated SVD trims the featureset down to the most significant features. \n",
    "\n",
    "We started with a lot of features - we can make predictions that are close to as accurate with far fewer, hopefully!\n",
    "\n",
    "<b>Note:</b> this might take a long time to run, depending on your computer. Change the \"for i in range()\" part to cut down on the number of iterations to make it run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\tfor i in range(1,15):\n",
    "\t\tn = i*10\n",
    "\t\tsteps = [('svd', TruncatedSVD(n_components=n)), ('m', LinearSVC(max_iter=100, tol=.01))]\n",
    "\t\tmodels[str(n)] = Pipeline(steps=steps)\n",
    "\treturn models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t#Splits cut for speed\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores\n",
    " \n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\tscores = evaluate_model(model, tok_df[0:1000], y[0:1000])\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "# plot model performance for comparison\n",
    "\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "tmp_vec = tf_idf.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2 = tf_idf.get_feature_names()\n",
    "tmp_df = pd.DataFrame(tmp_vec.toarray(), columns=tok_cols2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4327    Congrats! 2 mobile 3G Videophones R yours. cal...\n",
      "2135    Carlos took a while (again), we leave in a minute\n",
      "1414    So wats ur opinion abt him and how abt is char...\n",
      "1663                          S but mostly not like that.\n",
      "3751                               Why are u up so early?\n",
      "3525    \\HEY BABE! FAR 2 SPUN-OUT 2 SPK AT DA MO... DE...\n",
      "239     U 447801259231 have a secret admirer who is lo...\n",
      "3216             Come to mahal bus stop.. &lt;DECIMAL&gt;\n",
      "4918        Re your call; You didn't see my facebook huh?\n",
      "170     Sir, I need AXIS BANK account no and bank addr...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"text\"].sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(tmp_df, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Accuracy\n",
    "\n",
    "Since we are planning on dropping a bunch of data, we can try a model first to see what the baseline accuracy is. I'm also going to limit the number of features here, since using the entire dataset will take ages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9698492462311558"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_base = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\", max_features=2000)\n",
    "tmp_vec_base = tf_idf_base.fit_transform(df[\"text\"])\n",
    "\n",
    "tok_cols2_base = tf_idf_base.get_feature_names()\n",
    "tmp_df_base = pd.DataFrame(tmp_vec_base.toarray(), columns=tok_cols2_base)\n",
    "X_tr_base, X_te_base, y_tr_base, y_te_base = train_test_split(tmp_df_base, y)\n",
    "\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr_base, y_tr_base)\n",
    "pipe_test.score(X_te_base, y_te_base)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement LSA and Model\n",
    "\n",
    "We can use the truncated SVD to reduce the number of features in our dataset, in much the way we'd use any other data preparation step in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9310839913854989"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tmp = TruncatedSVD(n_components=1000)\n",
    "pipe_steps = [(\"scale\", StandardScaler()), (\"svd\", svd_tmp), (\"model\", SVC())]\n",
    "pipe_test = Pipeline(steps=pipe_steps)\n",
    "\n",
    "pipe_test.fit(X_tr, y_tr)\n",
    "pipe_test.score(X_te, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but TruncatedSVD was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000152</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.000897</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>-0.006557</td>\n",
       "      <td>-0.007882</td>\n",
       "      <td>-0.001473</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>-0.005707</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>-0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000217</td>\n",
       "      <td>-0.000237</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018239</td>\n",
       "      <td>-0.010503</td>\n",
       "      <td>-0.003316</td>\n",
       "      <td>-0.013584</td>\n",
       "      <td>-0.016161</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>0.016183</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>0.004889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>-0.000334</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.005725</td>\n",
       "      <td>-0.004937</td>\n",
       "      <td>-0.008284</td>\n",
       "      <td>0.002679</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000430</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.000503</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-0.000545</td>\n",
       "      <td>-0.000642</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>-0.000776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>-0.002075</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.001822</td>\n",
       "      <td>-0.000625</td>\n",
       "      <td>0.000979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>-0.000509</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>-0.000593</td>\n",
       "      <td>-0.000543</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>-0.001275</td>\n",
       "      <td>-0.000783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>0.026328</td>\n",
       "      <td>-0.017684</td>\n",
       "      <td>-0.004828</td>\n",
       "      <td>-0.012316</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.020530</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>-0.012384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000246</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000177</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>-0.003383</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>-0.004343</td>\n",
       "      <td>-0.003019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000111</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000119</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.000124</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.000157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.000519</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000280</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>-0.000175</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001193</td>\n",
       "      <td>-0.001443</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>-0.000525</td>\n",
       "      <td>-0.000968</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.003642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.000806</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>-0.001182</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>-0.003228</td>\n",
       "      <td>-0.000194</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>-0.000226</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>-0.000362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011857</td>\n",
       "      <td>-0.047597</td>\n",
       "      <td>0.036249</td>\n",
       "      <td>-0.028668</td>\n",
       "      <td>-0.028685</td>\n",
       "      <td>0.007561</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.015429</td>\n",
       "      <td>0.068451</td>\n",
       "      <td>0.033687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.000152 -0.000166 -0.000244 -0.000181 -0.000181  0.000695 -0.000177   \n",
       "1    -0.000217 -0.000237 -0.000372 -0.000271  0.000221  0.000147 -0.000271   \n",
       "2    -0.000240 -0.000250 -0.000334 -0.000268 -0.000269 -0.000300 -0.000271   \n",
       "3    -0.000430 -0.000443 -0.000627 -0.000514 -0.000503 -0.000525 -0.000545   \n",
       "4    -0.000254 -0.000449 -0.000343 -0.000509 -0.000517 -0.000593 -0.000543   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174 -0.000153 -0.000170 -0.000246 -0.000173 -0.000178 -0.000196 -0.000177   \n",
       "4175 -0.000108 -0.000111 -0.000149 -0.000119 -0.000118 -0.000140 -0.000124   \n",
       "4176 -0.000175  0.001179  0.003680  0.001796 -0.000174  0.000220 -0.000195   \n",
       "4177 -0.000062 -0.000063 -0.000092 -0.000067 -0.000066 -0.000075 -0.000063   \n",
       "4178 -0.000226 -0.000229 -0.000321 -0.000250 -0.000196 -0.000278 -0.000255   \n",
       "\n",
       "           7         8         9    ...       990       991       992  \\\n",
       "0    -0.000264  0.001882 -0.000190  ... -0.000408 -0.000897 -0.005000   \n",
       "1     0.000142 -0.000590  0.000398  ...  0.018239 -0.010503 -0.003316   \n",
       "2     0.000161 -0.000582 -0.000349  ... -0.003857  0.000922  0.004880   \n",
       "3    -0.000642 -0.000420 -0.000776  ...  0.000458 -0.001667  0.001727   \n",
       "4    -0.000608 -0.001275 -0.000783  ...  0.002468  0.026328 -0.017684   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4174 -0.000187 -0.000389 -0.000215  ... -0.000072  0.003895 -0.003383   \n",
       "4175  0.000835 -0.000240 -0.000157  ...  0.000092 -0.000685 -0.000519   \n",
       "4176 -0.000194 -0.000365 -0.000200  ... -0.001193 -0.001443  0.003579   \n",
       "4177 -0.000068 -0.000146 -0.000072  ... -0.000067  0.001695  0.000440   \n",
       "4178  0.000195 -0.000576 -0.000362  ... -0.011857 -0.047597  0.036249   \n",
       "\n",
       "           993       994       995       996       997       998       999  \n",
       "0    -0.006557 -0.007882 -0.001473  0.002977 -0.005707  0.006958 -0.000209  \n",
       "1    -0.013584 -0.016161 -0.001317  0.016183 -0.000988 -0.007597  0.004889  \n",
       "2    -0.000137 -0.005725 -0.004937 -0.008284  0.002679  0.001572  0.003929  \n",
       "3     0.000456 -0.002075 -0.000439  0.000505 -0.001822 -0.000625  0.000979  \n",
       "4    -0.004828 -0.012316 -0.009913  0.003025  0.020530  0.002195 -0.012384  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4174  0.001659  0.002400  0.000370  0.002347  0.000401 -0.004343 -0.003019  \n",
       "4175  0.000266 -0.000280 -0.000276  0.000241  0.000501  0.000883  0.000743  \n",
       "4176  0.001703 -0.000525 -0.000968 -0.001063  0.000312  0.000598  0.003642  \n",
       "4177 -0.000806  0.003015 -0.001182  0.000750 -0.003228 -0.000194  0.001211  \n",
       "4178 -0.028668 -0.028685  0.007561 -0.029773 -0.015429  0.068451  0.033687  \n",
       "\n",
       "[4179 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svd_tmp.transform(X_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['amp imf', 'amp imf loan', 'bank amp', 'bank amp imf', 'bank directors', 'bank directors says']\n",
      "Topic:1  ['boyfriend', 'aaniye', 'aaniye pudunga', 'aaniye pudunga venaam', 'athletic', 'athletic lt']\n",
      "Topic:2  ['drinking', 'thm', '2gthr', '2gthr drinking', '2gthr drinking boost', 'aproach']\n",
      "Topic:3  ['breath', 'day end', 'day like', 'love start', 'named', 'attraction']\n",
      "Topic:4  ['day happy', 'afternoons', 'afternoons evenings', 'afternoons evenings nights', 'amp happy birthday', 'approaching']\n",
      "Topic:5  ['bad problem', 'bad problem time', 'bed pillows', 'bed pillows floor', 'bed throw', 'bed throw laundry']\n",
      "Topic:6  ['bajarangabali', 'bajarangabali maruti', 'bajarangabali maruti pavanaputra', 'dodda', 'dodda problum', 'dodda problum nalli']\n",
      "Topic:7  ['cared', '4wrd', '4wrd dear', '4wrd dear loving', 'abt events', 'abt events espe']\n",
      "Topic:8  ['solve', 'afternoon wife', 'afternoon wife called', 'arrested murderer', 'arrested murderer immediately', 'called police']\n",
      "Topic:9  ['academic', 'department', 'academic department', 'academic department tell', 'academic secretary', 'academic secretary current']\n",
      "Topic:10  ['shahjahan', 'mahal', 'mumtaz', '4th wife', '4th wife wifes', 'arises']\n",
      "Topic:11  ['amp nice sleep', 'ayn', 'ayn mitsake', 'ayn mitsake goodnight', 'bt sitll', 'bt sitll yuo']\n",
      "Topic:12  ['bck black', 'bck black jealous', 'black jealous', 'black jealous brown', 'blue smile', 'blue smile face']\n",
      "Topic:13  ['avoids', 'avoids problems', 'avoids problems sent', 'becz', 'becz hav', 'becz hav gud']\n",
      "Topic:14  ['able pay', 'able pay charge', 'ahead month', 'ahead month end', 'askin ahead', 'askin ahead month']\n",
      "Topic:15  ['dont make promises', 'beg just', 'beg just listen', 'clear things', 'clear things easy', 'creativity']\n",
      "Topic:16  ['amp concern', 'amp concern prior', 'amp fuelled', 'amp fuelled love', 'amp pain', 'amp pain pls']\n",
      "Topic:17  ['angry left', 'angry left hit', 'bored wouldn', 'bored wouldn time', 'car wasn', 'car wasn said']\n",
      "Topic:18  ['waves', 'crab', 'footprints', 'asked frnd', 'asked frnd clearing', 'beautiful footprints']\n",
      "Topic:19  ['big files', 'big files download', 'come want', 'come want send', 'connection sucks', 'connection sucks remember']\n",
      "Topic:20  ['hai jo', 'hai jo ham', 'hai zindgi', 'hai zindgi wo', 'ham jeetey', 'ham jeetey hai']\n",
      "Topic:21  ['1st 5wkg', '1st 5wkg days', '22 65', '22 65 61', '382', '382 ubi']\n",
      "Topic:22  ['ali', 'lyf', 'lyfu', 'meow', 'ali halla', 'ali halla ke']\n",
      "Topic:23  ['chores', 'chores time', 'chores time joy', 'comes flowing', 'comes flowing dream', 'dirt']\n",
      "Topic:24  ['4get person', 'dat girl', 'dat girl margaret', 'girl margaret', 'girl margaret hello', 'girl yes']\n",
      "Topic:25  ['dont use', 'abt ur trusting', 'body uttered', 'body uttered word', 'chikku shared', 'chikku shared things']\n",
      "Topic:26  ['aid', 'aid usmle', 'aid usmle work', 'bam', 'bam aid', 'bam aid usmle']\n",
      "Topic:27  ['wer', 'aeronautics', 'aeronautics professors', 'aeronautics professors wer', 'aeroplane', 'aeroplane aftr']\n",
      "Topic:28  ['best choice yes', 'buy say', 'buy say happened', 'choice yes', 'choice yes wanted', 'choose room']\n",
      "Topic:29  ['medical insurance', '2waxsto', '2waxsto wat', '2waxsto wat want', 'able deliver', 'able deliver basic']\n",
      "Topic:30  ['limiting', 'children handle', 'children handle malaria', 'completely stop', 'days completely', 'days completely stop']\n",
      "Topic:31  ['lies', 'amazing rearrange', 'amazing rearrange letters', 'astronomer', 'astronomer moon', 'astronomer moon starer']\n",
      "Topic:32  ['arrange shipping', 'arrange shipping cut', 'bids online', 'bids online arrange', 'care rest', 'care rest wud']\n",
      "Topic:33  ['babe sorry', 'babe sorry didn', 'bucks don', 'bucks don know', 'cause thinks', 'cause thinks knows']\n",
      "Topic:34  ['itz', 'make use', 'sim card', 'amp asks', 'amp asks type', 'asks type']\n",
      "Topic:35  ['ur opinion', 'amplikater', 'amplikater fidalfication', 'amplikater fidalfication champlaxigating', 'atrocious', 'atrocious wotz']\n",
      "Topic:36  ['tell ur', 'tone ur', 'week just txt', 'nokia tone', 'nokia tone ur', 'getzed']\n",
      "Topic:37  ['colleagues did', 'colleagues did wish', 'day wife', 'day wife did', 'did kids', 'did kids went']\n",
      "Topic:38  ['1apple', '1apple day', '1apple day doctor', '1cup', '1cup milk', '1cup milk day']\n",
      "Topic:39  ['kiosk', 'warner village', '12 kiosk', '12 kiosk reply', '83118', '83118 colin']\n",
      "Topic:40  ['2nite tell', '2nite tell every1', 'ava', 'ava goodtime', 'ava goodtime oli', 'comin 2nite']\n",
      "Topic:41  ['grl', 'agalla boy hogli', 'agalla boy necklace', 'boy gold', 'boy gold chain', 'boy hogli']\n",
      "Topic:42  ['ec2a', 'laid', 'dogging network', 'largest', 'largest dogging', 'largest dogging network']\n",
      "Topic:43  ['got new', '08712317606', '2rcv', '82242', 'cam moby', 'cam moby wanna']\n",
      "Topic:44  ['blood blood', 'blood blood heart', 'blood heart', 'blood heart heart', 'friends including', 'friends including like']\n",
      "Topic:45  ['attitude', 'attitude romantic', 'attitude romantic shy', 'attractive', 'attractive funny', 'attractive funny lt']\n",
      "Topic:46  ['3rd', '1st ur lovely', '2nd ur', '2nd ur sms', '3rd ur', '3rd ur nature']\n",
      "Topic:47  ['differ', '9ja person', '9ja person sending', 'account bank remove', 'account details cos', 'bank remove']\n",
      "Topic:48  ['10p min mob', '1er', '1er stop', '1er stop ages', 'ages 18', 'aom box61']\n",
      "Topic:49  ['08701417012', '8007 nokias', '8007 nokias poly', 'alfie', 'alfie moon', 'alfie moon children']\n",
      "Topic:50  ['250 want', '250 want 800', '3mobile', '3mobile 0870', '3mobile 0870 chatlines', 'asked 3mobile']\n",
      "Topic:51  ['18 30pp', '18 30pp txt', '1st 5free', '1st 5free 50', '30pp', '30pp txt']\n",
      "Topic:52  ['starts saturday', '113', '113 bray', '113 bray wicklow', 'address envelope', 'address envelope drinks']\n",
      "Topic:53  ['150p wk', '150p wk club4', '25 free', '25 free credits', '2wt', '87070']\n",
      "Topic:54  ['amp said', 'amp said wait', 'crack', 'crack jokes', 'crack jokes gm', 'ge gn']\n",
      "Topic:55  ['87077 yahoo', '87077 yahoo pobox36504w45wq', 'adam', 'adam eve', 'adam eve 07123456789', 'ads']\n",
      "Topic:56  ['affection', 'affection care', 'affection care luv', 'angry childish', 'angry childish true', 'angry wid']\n",
      "Topic:57  ['16 remove txtx', '250 sms', '250 sms messages', '50p wk', '50p wk box139', '84025 use']\n",
      "Topic:58  ['abt half', 'abt half pages', 'brief description', 'brief description nuclear', 'brief history', 'brief history iter']\n",
      "Topic:59  ['tayseer', 'tissco', 'aig', 'aig joined', 'aig joined tissco', 'allah rakhesh']\n",
      "Topic:60  ['20 poboxox36504w45wq', '20 poboxox36504w45wq 16', '41', '41 20', '41 20 poboxox36504w45wq', '4txt i1']\n",
      "Topic:61  ['220', '220 cm2', '220 cm2 9ae', 'cm2', 'cm2 9ae', 'comuk 220']\n",
      "Topic:62  ['20 poboxox36504w45wq', '20 poboxox36504w45wq 16', '41', '41 20', '41 20 poboxox36504w45wq', '4txt i1']\n",
      "Topic:63  ['belovd', 'enemy', 'accept brother', 'accept brother sister', 'accept day accept', 'belovd swtheart']\n",
      "Topic:64  ['luv vth', 'vth', 'alwa', 'alwa gud', 'alwa gud eveb', 'bed fals']\n",
      "Topic:65  ['3g videophones', 'congrats mobile', 'congrats mobile 3g', 'dload', 'games dload', 'java']\n",
      "Topic:66  ['100 filthy', '100 filthy films', '450p', '450p days', '450p days stop2', '69669 saristar']\n",
      "Topic:67  ['2nhite', '2nhite ros', '2nhite ros xxxxxxx', 'basq', 'basq ihave', 'basq ihave fun']\n",
      "Topic:68  ['stop msg', '08714342399', '08714342399 2stop', '08714342399 2stop reply', '2stop reply', '2stop reply stop']\n",
      "Topic:69  ['stop msg', '08714342399', '2stop reply', '2stop reply stop', '50rcvd', 'club reply']\n",
      "Topic:70  ['10 ls1', '10 ls1 3aj', '1000 claim txt', '1000 claim', '18 50', '18 50 morefrmmob']\n",
      "Topic:71  ['calculation', 'practicing', '4years', '4years dental', '4years dental school', 'accent']\n",
      "Topic:72  ['drop tear', 'drop tear dsn', 'dsn hav prayrs', 'dsn hav words', 'dsn lik', 'dsn lik stay']\n",
      "Topic:73  ['ages ring', 'ages ring ur', 'calm downon', 'calm downon theacusations', 'cos iwana', 'cos iwana know']\n",
      "Topic:74  ['bcz', 'bcz mis', 'bcz mis love', 'dont mis', 'ennal', 'ennal prabha']\n",
      "Topic:75  ['01223585334 cum', '01223585334 cum wan', '2c', '2c pics', '2c pics gettin', '2end']\n",
      "Topic:76  ['bevies', 'bevies waz', 'bevies waz gona', 'bin watchng', 'bin watchng planet', 'comfey']\n",
      "Topic:77  ['prize draw', '05 prize transferred', '09041940223', '00 easter prize', '00 easter', '05 prize']\n",
      "Topic:78  ['coming playing', 'coming playing jay', 'decided lt', 'decided lt gt', 'don live', 'don live anymore']\n",
      "Topic:79  ['sae', 'await collection', 'await', 'collection', 'cash await', 'cash await collection']\n",
      "Topic:80  ['09066382422 calls', '09066382422 calls cost', '150ppm ave', '150ppm ave 3mins', '16 close', '16 close 300603']\n",
      "Topic:81  ['babe celebration', 'babe celebration rents', 'celebration rents', 'couch', 'couch link', 'couch link sent']\n",
      "Topic:82  ['cheers tex', 'cheers tex mecause', 'countinlots', 'countinlots loveme', 'countinlots loveme xxxxx', 'gr8fun']\n",
      "Topic:83  ['al salam', 'al salam wahleykkum', 'allah meet', 'allah meet rakhesh', 'evening sir al', 'fine inshah']\n",
      "Topic:84  ['al salam', 'al salam wahleykkum', 'allah meet', 'allah meet rakhesh', 'evening sir al', 'fine inshah']\n",
      "Topic:85  ['10 den hop', '1030 den', '1030 den hop', 'bugis vill', 'bugis vill frm', 'cine frm']\n",
      "Topic:86  ['acc wen', 'acc wen lt', 'credited lt', 'credited lt gt', 'dude fake', 'dude fake frnds']\n",
      "Topic:87  ['pobox36504w45wq 150p', '4u rply', '4u rply tone', '8007 titles', '8007 titles ghost', '8007 tone']\n",
      "Topic:88  ['try come', 'come till', 'come till goodnight', 'contacts remember', 'contacts remember venugopal', 'dileep']\n",
      "Topic:89  ['send hide', '21 aberdeen', '21 aberdeen united', '4u rodds1', '4u rodds1 21', 'aberdeen']\n",
      "Topic:90  ['chennai requires', 'chennai requires freshers', 'english needed', 'english needed salary', 'excellent english', 'excellent english needed']\n",
      "Topic:91  ['hubby', 'abi hw', 'abi hw posted', 'card abi', 'card abi hw', 'doc pls']\n",
      "Topic:92  ['games arcade', 'arcade', 'hubby', 'abi hw', 'abi hw posted', 'card abi']\n",
      "Topic:93  ['official', '84199', 'box39822', 'box39822 w111wx', 'box39822 w111wx 50', 'colour flag']\n",
      "Topic:94  ['boye need', 'boye need movies', 'cos dont', 'cos dont meet', 'day plus', 'day plus started']\n",
      "Topic:95  ['play send', '100percent', '100percent real', '500 pounds genuine', '86688 info', '86688 info www']\n",
      "Topic:96  ['answer den', 'answer den manage', 'asked girl', 'asked girl tell', 'boy asked', 'boy asked girl']\n",
      "Topic:97  ['2u 3wks', '2u 3wks pls', '3wks', '3wks pls', '3wks pls pls', 'bought test']\n",
      "Topic:98  ['aburo enjoy message', 'enjoy message', 'enjoy message sent', 'message sent days', 'sent days', 'sent days ago']\n",
      "Topic:99  ['89080', '89080 dont', '89080 dont miss', 'celeb pics', 'celeb pics pocketbabe', 'day pic']\n",
      "Topic:100  ['meeting today', 'send contact number', 'alaikkum', 'alaikkum pride', 'alaikkum pride pleasure', 'contact number']\n",
      "Topic:101  ['2mrw luv jaz', '4ward', '4ward 2mrw', '4ward 2mrw luv', 'ad nice', 'ad nice wkend']\n",
      "Topic:102  ['comes ll', 'comes ll gt', 'day send dis', 'dis lose', 'dis lose ur', 'dis ur valued']\n",
      "Topic:103  ['150p msg rcd', '80155', '80155 don', '80155 don like', 'bored speed', 'bored speed dating']\n",
      "Topic:104  ['50p day', 'txt porn', '24hrs free', '24hrs free just', '50p day stop', '69855']\n",
      "Topic:105  ['bt quiet', 'bt quiet leaves', 'close fight', 'close fight wit', 'coz somtimes dis', 'dem coz']\n",
      "Topic:106  ['000 pounds', '100 20', '100 20 000', '150p day', '150p day 6days', '16 tsandcs']\n",
      "Topic:107  ['16 gbp1', '16 gbp1 50', '200 week', '200 week weekly', '3uz 16', '3uz 16 gbp1']\n",
      "Topic:108  ['16 gbp1', '16 gbp1 50', '200 week', '200 week weekly', '3uz 16', '3uz 16 gbp1']\n",
      "Topic:109  ['amp promise', 'amp promise ll', 'bring ur', 'bring ur smile', 'gray', 'gray remembr']\n",
      "Topic:110  ['0845 2814032 16', '0845 2814032', '0845', '150pw', '150pw ea', '150pw ea nd']\n",
      "Topic:111  ['uncle john', 'dime', '2years', '2years sent', '2years sent know', 'dime expect']\n",
      "Topic:112  ['amt lt', 'amt lt gt', 'arng', 'arng dis', 'arng dis amt', 'bmw']\n",
      "Topic:113  ['0870141701216', '120p', '4txt 120p', '8007 www', '8007 www getzed', 'fans']\n",
      "Topic:114  ['120p', '4txt 120p', '8007 www', '8007 www getzed', 'fans', 'fans latest']\n",
      "Topic:115  ['bookmark', 'bookmark text', 'bookmark text wap', 'click themob', 'click themob wap', 'dizzee']\n",
      "Topic:116  ['3gbp', '3gbp network', '3gbp network operator', 'best tones', 'best tones 3gbp', 'network operator rates']\n",
      "Topic:117  ['expires', 'account statement', 'identifier', 'identifier code', 'statement', 'private']\n",
      "Topic:118  ['08709222922 national rate', '08709222922 national', '5p min', '5p min cheap', '8p', '8p min']\n",
      "Topic:119  ['askin i_', 'askin i_ leaving', 'cos mayb', 'cos mayb i_', 'cos meeting', 'cos meeting today']\n",
      "Topic:120  ['today ur', 'bags sister', 'bags sister things', 'bills really', 'bills really need', 'buying gucci']\n",
      "Topic:121  ['arise', 'arise hurricanes', 'arise hurricanes sway', 'aroundn', 'aroundn teaches', 'aroundn teaches hw']\n",
      "Topic:122  ['camry', 'camry like', 'camry like mr', 'clean need', 'clean need know', 'dough soon']\n",
      "Topic:123  ['camry', 'camry like', 'camry like mr', 'clean need', 'clean need know', 'dough soon']\n",
      "Topic:124  ['april les', 'april les got', 'bak college work', 'bristol st', 'bristol st week', 'college work']\n",
      "Topic:125  ['april les', 'april les got', 'bak college work', 'bristol st', 'bristol st week', 'college work']\n",
      "Topic:126  ['sp', '82277', '008704050406', '100 high', '50 008704050406', '50 008704050406 sp']\n",
      "Topic:127  ['just 25p', 'ur area', '25p free', '25p free receive', '89693', 'area just']\n",
      "Topic:128  ['09099726429', '09099726429 janinexx callsa', 'busty', 'busty married', 'busty married woman', 'chat sort']\n",
      "Topic:129  ['08704050406', '80122300p', '80122300p wk', '80122300p wk sp', 'awarded free', 'awarded free mini']\n",
      "Topic:130  ['08001950382', 'mobileupd8', 'linerental', 'mobileupd8 08001950382', 'call2optout', '12mths']\n",
      "Topic:131  ['2channel', '2channel 2day', '2channel 2day ur', '2day ur', '2day ur leadership', 'ac jsco']\n",
      "Topic:132  ['89070', 'phone details', 'send stop 89070', '89070 cancel', '89070 cancel send', 'chat mob']\n",
      "Topic:133  ['0870753331018', '09090900040 listen extreme', '24 7mp', '24 7mp 0870753331018', '60p min 24', '7mp']\n",
      "Topic:134  ['costa', '09050090044', '50 pm', '50 pm max10mins', '5000 await', '5000 await collection']\n",
      "Topic:135  ['150p rcvd', '150p rcvd stop2stop', '18 150p rcvd', 'bored work', 'bored work xxx', 'fones']\n",
      "Topic:136  ['cs www', 'ts cs www', 'cs www smsco', 'smsco', 'smsco net', 'www smsco']\n",
      "Topic:137  ['1st ur games', 'activ8', 'activ8 press', 'activ8 press key', 'arcade 1st', 'arcade 1st ur']\n",
      "Topic:138  ['dan', 'av wont', 'av wont just', 'bck luv', 'bck luv dan', 'evone']\n",
      "Topic:139  ['1st4terms', '1st4terms pobox84', '1st4terms pobox84 m26', '3uz cost', '3uz cost 50', '50 calls']\n",
      "Topic:140  ['09077818151 book time', '09077818151 book', '30s', '30s www', '30s www santacalling', '3mins 30s']\n",
      "Topic:141  ['09077818151 book time', '09077818151', '09077818151 book', '30s', '30s www', '30s www santacalling']\n",
      "Topic:142  ['crazy frog', 'frog', 'new tones', 'akon', 'akon lonely', 'akon lonely black']\n",
      "Topic:143  ['loose', 'compass', 'compass ur', 'compass ur mind', 'gnun', 'gnun sent']\n",
      "Topic:144  ['loose', 'compass', 'compass ur', 'compass ur mind', 'gnun', 'gnun sent']\n",
      "Topic:145  ['cut nails', 'cut nails oso', 'den weekdays', 'den weekdays got', 'drivin wat', 'drivin wat lunch']\n",
      "Topic:146  ['age supposed', 'age supposed support', 'child job', 'child job support', 'flaky parent', 'flaky parent snot']\n",
      "Topic:147  ['age supposed', 'age supposed support', 'child job', 'child job support', 'flaky parent', 'flaky parent snot']\n",
      "Topic:148  ['150p stop', '150p stop txt', '69969', '69969 msg', '69969 msg cost', 'action real']\n",
      "Topic:149  ['150p16', '84025 chance', '84025 chance win', 'cash wkent', 'cash wkent 150p16', 'chance win 100']\n",
      "Topic:150  ['aft lunch', 'aft lunch meetin', 'buy tmr', 'buy tmr lor', 'fr lei', 'fr lei ii']\n",
      "Topic:151  ['150p reply', '150p reply poly', 'boltblue', 'boltblue tones', 'boltblue tones 150p', 'cha cha']\n",
      "Topic:152  ['ages started', 'ages started driving', 'coz havenaot', 'coz havenaot sent', 'did hav', 'did hav nice']\n",
      "Topic:153  ['50p std', '50p std text', '83049', '83049 cost', '83049 cost 50p', 'cost 50p']\n",
      "Topic:154  ['actually talk', 'actually talk boss', 'boss morning', 'boss morning went', 'car fixed', 'car fixed cheaper']\n",
      "Topic:155  ['08701417012150p', '8007 zed', '8007 zed 08701417012150p', 'collection logo', 'collection logo pic', 'halloween collection logo']\n",
      "Topic:156  ['alrite good', 'alrite good nit', 'avent spoken', 'avent spoken long', 'chat coz', 'chat coz avent']\n",
      "Topic:157  ['alrite good', 'alrite good nit', 'avent spoken', 'avent spoken long', 'chat coz', 'chat coz avent']\n",
      "Topic:158  ['closed including', 'closed including post', 'coz man', 'coz man murdered', 'govt', 'govt instituitions']\n",
      "Topic:159  ['lily txt', 'lily txt 84025', '150p16', '84025 chance', '84025 chance win', 'cash wkent']\n",
      "Topic:160  ['rs lt gt', 'onion', 'shesil lt', 'shesil lt gt', 'beer rs', 'beer rs lt']\n",
      "Topic:161  ['10 man', 'anal', 'anal sex', 'anal sex want', 'come wait', 'come wait love']\n",
      "Topic:162  ['25p packs', '25p packs lucozade', '61200', '61200 25p', '61200 25p packs', 'energy text']\n",
      "Topic:163  ['000 prize jackpot', '100 000', '100 000 prize', '4403ldnw1a7rw18', '81010', '81010 www']\n",
      "Topic:164  ['000 prize jackpot', '100 000', '4403ldnw1a7rw18', '81010', '81010 www', '81010 www dbuk']\n",
      "Topic:165  ['shld', 'mei', 'dun work', 'ah waitin', 'ah waitin treat', 'den dun work']\n",
      "Topic:166  ['25p optout', '25p optout txt', '83021', '83021 reply', '83021 reply date', 'date join']\n",
      "Topic:167  ['boys pissed', 'boys pissed kids', 'boys told', 'boys told mark', 'cause getting', 'cause getting owed']\n",
      "Topic:168  ['boys pissed', 'boys pissed kids', 'boys told', 'boys told mark', 'cause getting', 'cause getting owed']\n",
      "Topic:169  ['tell bad', 'add tat', 'add tat new', 'bad character', 'bad character dnt', 'change lt']\n",
      "Topic:170  ['add tat', 'add tat new', 'bad character', 'bad character dnt', 'change lt', 'change lt gt']\n",
      "Topic:171  ['away gal', 'away gal jst', 'boy think run', 'boy walking', 'boy walking park', 'feels walking']\n",
      "Topic:172  ['cochin', 'left mobile', 'bakrid', 'car ente', 'car ente style', 'cochin home']\n",
      "Topic:173  ['cochin', 'left mobile', 'bakrid', 'car ente', 'car ente style', 'cochin home']\n",
      "Topic:174  ['03 2nd', '03 2nd attempt', 'bonus caller', 'bonus caller prize', '000 bonus', '000 bonus caller']\n",
      "Topic:175  ['ringtoneking uk', '08701237397', '08701237397 16', '08701237397 16 club', '16 club', '16 club credits']\n",
      "Topic:176  ['application schools', 'application schools think', 'applying', 'applying research', 'applying research cost', 'contact joke']\n",
      "Topic:177  ['slo', '4msgs', 'babe want', 'babe want dont', 'baby im nasty', 'bitch slo']\n",
      "Topic:178  ['cross ntwk', 'cross ntwk mins', 'ntwk mins', '08001950382 call2optout', 'ntwk', '08001950382']\n",
      "Topic:179  ['dun mind', 'den shld', 'den shld went', 'dun mind goin', 'gee nvm', 'gee nvm la']\n",
      "Topic:180  ['accomodations', 'accomodations thought', 'accomodations thought liked', 'best happy', 'best happy cave', 'cave']\n",
      "Topic:181  ['week 16', '16 apply', '88888 crazy', '88888 crazy sounds', 'crazy frog sound', 'crazy sounds']\n",
      "Topic:182  ['aries', 'career flyng', 'career flyng start', 'dream partner', 'dream partner soon', 'flyng']\n",
      "Topic:183  ['aries', 'career flyng', 'career flyng start', 'dream partner', 'dream partner soon', 'flyng']\n",
      "Topic:184  ['bills settle', 'bills settle month', 'bother bills', 'bother bills settle', 'cash know', 'cash know challenging']\n",
      "Topic:185  ['dancing', 'amp sayin', 'amp sayin bite', 'arngd', 'arngd marriage', 'arngd marriage walkin']\n",
      "Topic:186  ['satisfy', '25 love', '25 love satisfy', 'buffy', 'buffy 25', 'buffy 25 love']\n",
      "Topic:187  ['loyalty', 'offer new', '10 txtauction txt', '10 txtauction', '4t', '81151']\n",
      "Topic:188  ['iaom', 'care spk', 'care spk sn', 'cause doin', 'cause doin loads', 'cool ta']\n",
      "Topic:189  ['paris', 'soiree', '07946746291 07880867867', 'zouk', 'zouk nichols', 'zouk nichols paris']\n",
      "Topic:190  ['aboutas', 'aboutas chance', 'aboutas chance merememberin', 'asthere', 'asthere ofsi', 'asthere ofsi breakin']\n",
      "Topic:191  ['dun work', 'ah waitin', 'ah waitin treat', 'den dun work', 'dun work frm', 'ex wat']\n",
      "Topic:192  ['convinced', 'care sleep need', 'change life', 'change life need', 'conversations', 'conversations convinced']\n",
      "Topic:193  ['ami', 'ami parchi', 'ami parchi na', 'iccha', 'iccha korche', 'iccha korche na']\n",
      "Topic:194  ['ami', 'ami parchi', 'ami parchi na', 'iccha', 'iccha korche', 'iccha korche na']\n",
      "Topic:195  ['100 music', '150ppermesssubscription', '1winaweek', '1winaweek age16', '1winaweek age16 150ppermesssubscription', '87066 tscs']\n",
      "Topic:196  ['100 music', '100 music gift', 'draw 87066', 'draw 87066 tscs', '150ppermesssubscription', '1winaweek']\n",
      "Topic:197  ['beehoon', 'beehoon eat', 'beehoon eat cheap', 'cafe tok', 'cafe tok nydc', 'cake fishhead']\n",
      "Topic:198  ['love pete', 'fancy meetin', 'að hav', 'að hav lil', 'beverage', 'beverage txt']\n",
      "Topic:199  ['animation', 'animation badass', 'animation badass hoody', 'badass', 'badass hoody', 'badass hoody wallpaper']\n",
      "Topic:200  ['50 500', '50 500 optout', '50 play', '50 play instant', '500 optout', '500 optout 08718727870']\n",
      "Topic:201  ['best cheap', 'best cheap gd', 'cheap gd', 'cheap gd food', 'chinese food den', 'den prefer']\n",
      "Topic:202  ['08715203028 claim 9th', '08715203028 claim', '50 500', '50 500 optout', '50 play', '50 play instant']\n",
      "Topic:203  ['bangb', 'bangb tv', 'bangb tv ur', 'bangbabes', 'bangbabes ur', 'bangbabes ur order']\n",
      "Topic:204  ['i_ neva', 'properly', 'aiya later', 'aiya later come', 'brin', 'brin lar']\n",
      "Topic:205  ['bcoz dont', 'bcoz dont force', 'called falling', 'called falling love', 'dont force', 'dont search']\n",
      "Topic:206  ['abel', 'air1', 'air1 hilarious', 'air1 hilarious boughtaobraindanceaoa', 'album quite', 'album quite gd']\n",
      "Topic:207  ['asleep iaom', 'asleep iaom knackered', 'babes hope', 'babes hope doing', 'doing ok', 'doing ok shit']\n",
      "Topic:208  ['abel', 'air1', 'air1 hilarious', 'air1 hilarious boughtaobraindanceaoa', 'album quite', 'album quite gd']\n",
      "Topic:209  ['unsubscribe txt', 'unsubscribe txt stop', '150p msg unsubscribe', '250 wkly comp', '84128 custcare', '84128 custcare 08712405020']\n",
      "Topic:210  ['10 bid', '10 83383', '10 bid 10', '10 83383 good', '54 maximum', '54 maximum bid']\n",
      "Topic:211  ['0844 861 85', '0844 861', '0844', 'access', '85', '85 85']\n",
      "Topic:212  ['10 bid', '10 83383', '10 bid 10', '54 maximum', '54 maximum bid', '71']\n",
      "Topic:213  ['ac sptv', 'ac sptv new', 'correct incorrect', 'correct incorrect end', 'detroit red', 'detroit red wings']\n",
      "Topic:214  ['2004', 'ac sptv', 'ac sptv new', 'correct incorrect', 'correct incorrect end', 'detroit red']\n",
      "Topic:215  ['rays', 'sunny', 'bay', 'blue bay', 'blue blue', 'blue blue bay']\n",
      "Topic:216  ['cds stdtxtrate', 'cds stdtxtrate reply', 'chance win lotr', 'end txts', 'june chance', 'june chance win']\n",
      "Topic:217  ['bad itaos', 'bad itaos rock', 'canaot', 'canaot wait', 'canaot wait cornwall', 'cornwall hope']\n",
      "Topic:218  ['day miracle', 'day miracle god', 'dont ve', 'dont ve looked', 'giving really', 'giving really wish']\n",
      "Topic:219  ['camcorder reply', 'camcorder', '3510i', '3510i colour', '3510i colour phone', 'colour phone']\n",
      "Topic:220  ['ask cos', 'ask cos ba', 'ba gua', 'ba gua went', 'cos ba', 'cos ba gua']\n",
      "Topic:221  ['free polyphonic', '16 sn', '16 sn pobox202', '450pw', '7zs', '7zs subscription']\n",
      "Topic:222  ['happy morning', 'amp conducts', 'amp conducts exam', 'amp teaches', 'amp teaches lessons', 'bcoz teacher']\n",
      "Topic:223  ['bathe dun', 'bathe dun disturb', 'bathing bathe', 'bathing bathe dun', 'card dunno', 'card dunno network']\n",
      "Topic:224  ['affairs ask', 'affairs ask asking', 'ask asking', 'ask asking shuhui', 'asking shuhui', 'asking shuhui oso']\n",
      "Topic:225  ['affairs ask', 'affairs ask asking', 'ask asking', 'ask asking shuhui', 'asking shuhui', 'asking shuhui oso']\n",
      "Topic:226  ['dont reserves', 'dont reserves completely', 'balance lt', 'balance lt gt', 'completely gone', 'completely gone loan']\n",
      "Topic:227  ['club tones', '50 week mfl', 'club tones cost', 'club tones replying', 'com enjoy', 'com enjoy html']\n",
      "Topic:228  ['ll haf', 'coming sun', 'coming sun finish', 'dinner going', 'dinner going hotel', 'exactly mth']\n",
      "Topic:229  ['display okay', 'firefox', 'firefox loads', 'firefox loads just', 'just plus', 'just plus sign']\n",
      "Topic:230  ['087016248', '16 wk', '16 wk 087016248', '88600 wk', '88600 wk sexy', 'babe 88600']\n",
      "Topic:231  ['08081560665 speak live', '08081560665', '08081560665 speak', 'bahamas', '18 opt', '18 opt txt']\n",
      "Topic:232  ['16 stop', '16 stop txtstop', '3hrs', '3hrs 16', '3hrs 16 stop', '86888']\n",
      "Topic:233  ['really shocking', 'shocking', 'al sudn', 'bt hmm', 'bt hmm njan', 'engagement']\n",
      "Topic:234  ['drink soon', 'drink soon mite', 'bit shud', 'bit shud drink', 'da works', 'da works laugh']\n",
      "Topic:235  ['available want', 'available want book', 'book ticket', 'book ticket tackle', 'chennai sunday', 'chennai sunday eve']\n",
      "Topic:236  ['08081560665 speak live', '08081560665', '08081560665 speak', 'bahamas', '18 opt', '18 opt txt']\n",
      "Topic:237  ['14thmarch', '14thmarch apply', '14thmarch apply opt', '153 9996', '153 9996 offer', '9996']\n",
      "Topic:238  ['abt ur reaction', 'ask abt ur', 'cos telling', 'cos telling shuhui', 'dat know', 'dat know liao']\n",
      "Topic:239  ['jesus', 'ahead amp', 'ahead amp wish', 'amp wish', 'amp wish merry', 'blessed']\n",
      "Topic:240  ['account balance', 'account balance rs', 'account refilled', 'account refilled successfully', 'balance rs', 'balance rs lt']\n",
      "Topic:241  ['25 year', '25 year old', 'bed boatin', 'bed boatin docks', 'boatin', 'boatin docks']\n",
      "Topic:242  ['25 year', '25 year old', 'bed boatin', 'bed boatin docks', 'boatin', 'boatin docks']\n",
      "Topic:243  ['150p vat', '150p vat cancel', 'cancel text', 'cancel text stop', 'cum text', 'cum text im']\n",
      "Topic:244  ['1450', '1450 prize', '1450 prize claim', 'claim just', 'cs stop', 'cs stop sms']\n",
      "Topic:245  ['send luv', 'send care', 'send care msg', 'care msg', 'care msg gud', 'cloud']\n",
      "Topic:246  ['09061743806', '09061743806 landline', '5000 cash award', 'award 09061743806', 'award 09061743806 landline', 'box326']\n",
      "Topic:247  ['boston men', 'boston men changed', 'changed cuz', 'changed cuz signin', 'changed search', 'changed search location']\n",
      "Topic:248  ['ill phone', 'coming home fri', 'course expect', 'course expect welcome', 'expect welcome', 'expect welcome party']\n",
      "Topic:249  ['ill phone', 'coming home fri', 'course expect', 'course expect welcome', 'expect welcome', 'expect welcome party']\n",
      "Topic:250  ['actually necessity', 'actually necessity wait', 'drama', 'drama pls', 'drama pls family', 'ego']\n",
      "Topic:251  ['chat send', '08717890890a', '08717890890a 50 msg', 'send stopcs', '1000s real', '1000s real people']\n",
      "Topic:252  ['88039', '88039 skilgme', 'skilgme', 'store 88039', 'store 88039 skilgme', 'chat send']\n",
      "Topic:253  ['change poet', 'change poet needs', 'frndship forever', 'imagination', 'imagination phone', 'imagination phone needs']\n",
      "Topic:254  ['aunt', 'bc', 'aunt flow', 'aunt flow didn', 'bc way', 'bc way shrink']\n",
      "Topic:255  ['08452810071 16', 'send stop 08452810071', '1st ringtone', '1st ringtone free', '20 tones', '20 tones phone']\n",
      "Topic:256  ['08452810071 16', '1st ringtone', '1st ringtone free', '20 tones', '20 tones phone', '50 wk opt']\n",
      "Topic:257  ['16 help', '16 help 09701213186', 'club credits new', 'credits new', 'credits new videosounds', 'enjoy jamster']\n",
      "Topic:258  ['bears', '10p min stop', 'bears pic', 'bears pic nick', 'chat photo', 'chat photo upload']\n",
      "Topic:259  ['4u didnaot', '4u didnaot intend', 'careabout', 'didnaot intend', 'didnaot intend hurt', 'felt iwas']\n",
      "Topic:260  ['bears', '10p min stop', 'bears pic', 'bears pic nick', 'chat photo', 'chat photo upload']\n",
      "Topic:261  ['oh ya', 'zoom cine actually', 'actually tonight', 'actually tonight free', 'cine actually', 'cine actually tonight']\n",
      "Topic:262  ['like wil care', 'bit ur', 'bit ur smile', 'care forevr', 'care forevr goodfriend', 'drop ur']\n",
      "Topic:263  ['com text', 'com text stop', 'visionsms', 'visionsms com', 'visionsms com text', '08712400603']\n",
      "Topic:264  ['2mrw afternoon', '2mrw afternoon town', 'afternoon town', 'afternoon town mall', 'aint tho', 'aint tho saw']\n",
      "Topic:265  ['150ppm 16', '300 nasdaq', '300 nasdaq symbol', 'bank granite', 'bank granite issues', 'buy explosive']\n",
      "Topic:266  ['02073162414', '30 secs', '30 secs ring', 'costs 20p', 'costs 20p min', 'explicit']\n",
      "Topic:267  ['02073162414 costs', '02073162414 costs 20p', '02073162414', '30 secs', '30 secs ring', 'costs 20p']\n",
      "Topic:268  ['ask join', 'ask join bday', 'bday feel', 'bday feel free', 'booking fri', 'definite nos']\n",
      "Topic:269  ['4thnov', '4thnov ourbacks', '4thnov ourbacks að', 'aletter', 'aletter thatmum', 'aletter thatmum gotmarried']\n",
      "Topic:270  ['03 marsms log', '08717168528', '27 03 marsms', 'b4u', 'b4u voucher', 'b4u voucher 27']\n",
      "Topic:271  ['54 weeks', '54 weeks pls', 'expiry', 'expiry reply', 'expiry reply themob', 'min term']\n",
      "Topic:272  ['accommodation', 'accommodation various', 'accommodation various global', 'address receive', 'address receive post', 'com ph']\n",
      "Topic:273  ['accommodation', 'accommodation various', 'accommodation various global', 'address receive', 'address receive post', 'com ph']\n",
      "Topic:274  ['scold', 'act', 'act real', 'ask b4', 'ask b4 ask', 'ask ll']\n",
      "Topic:275  ['1million', '1million away', '1million away ppt150x3', '88600 1million', '88600 1million away', 'away ppt150x3']\n",
      "Topic:276  ['brings ringtones', 'brings ringtones time', 'chart heroes', 'chart heroes free', 'free hit', 'free hit week']\n",
      "Topic:277  ['knowing', 'address boss', 'address boss tell', 'boss tell', 'boss tell knowing', 'called texted']\n",
      "Topic:278  ['knowing', 'address boss', 'address boss tell', 'boss tell', 'boss tell knowing', 'called texted']\n",
      "Topic:279  ['knowing', 'address boss', 'address boss tell', 'boss tell', 'boss tell knowing', 'called texted']\n",
      "Topic:280  ['2wks way', '2wks way usps', '3days', '3days shipping', '3days shipping company', 'bribe']\n",
      "Topic:281  ['08718726270', '150p msgrcvd', '150p msgrcvd skip', '18 150p msgrcvd', 'auction subscription', 'auction subscription service']\n",
      "Topic:282  ['asks sensitive', 'asks sensitive information', 'atm sms', 'atm sms pin', 'email share', 'email share password']\n",
      "Topic:283  ['bbdeluxe', 'bbdeluxe challenge', 'block breaker', 'block breaker comes', 'breaker', 'breaker comes']\n",
      "Topic:284  ['82050', '82050 uk', 'exciting prizes', 'exciting prizes soon', 'eye ur', 'eye ur mobile']\n",
      "Topic:285  ['ask wat', 'ask wat present', 'called ask', 'called ask wat', 'cos shopping', 'cos shopping wif']\n",
      "Topic:286  ['2go thanx', '2go thanx xx', 'al arguments', 'al arguments fault', 'al does', 'al does moan']\n",
      "Topic:287  ['30 want', 'wld', '30 want cld', 'cld drink', 'cld drink wld', 'doing nxt']\n",
      "Topic:288  ['82050', '82050 uk', 'exciting prizes', 'exciting prizes soon', 'eye ur', 'eye ur mobile']\n",
      "Topic:289  ['area matched', 'area matched just', 'chat flirt', 'chat flirt sexy', 'fancy chat flirt', 'finally fancy']\n",
      "Topic:290  ['brand new mobile', 'arrive shortly', 'arrive shortly just', 'artists', 'browse', 'browse content']\n",
      "Topic:291  ['2007', '2007 band', '2007 band died', 'band', 'band died', 'band died start']\n",
      "Topic:292  ['ad crap', 'ad crap nite', 'ave ya', 'biatch', 'biatch thanx', 'biatch thanx wait']\n",
      "Topic:293  ['wins', 'dhorte', 'dhorte lage', 'dhorte lage thats', 'great phone', 'great phone receiving']\n",
      "Topic:294  ['concentrating', 'concentrating final', 'concentrating final yr', 'cos taking', 'cos taking modules', 'final yr']\n",
      "Topic:295  ['0800 1956669 text', '0800 1956669', '000 homeowners', '000 homeowners tenants', '1956669', '1956669 text']\n",
      "Topic:296  ['09061221061 landline', '09061221061', '150ppm 16 pa', '16 pa', '16 pa 99', '28days']\n",
      "Topic:297  ['2morow luv', '2morow luv millions', 'cum 2morow', 'cum 2morow luv', 'defo', 'defo try']\n",
      "Topic:298  ['11mths update', '11mths update free', 'calls mobile', 'calls mobile upd8', 'camera mobiles', 'camera mobiles unlimited']\n",
      "Topic:299  ['able value', 'able value pls', 'askin things', 'askin things laugh', 'brisk', 'brisk walks']\n",
      "Topic:300  ['hes', 'becaus', 'becaus hes', 'becaus hes verifying', 'called sms', 'called sms like']\n",
      "Topic:301  ['behalf', 'behalf stunning', 'behalf stunning exam', 'day moji', 'day moji told', 'easily great']\n",
      "Topic:302  ['appointment need', 'appointment need home', 'bit run', 'bit run forgot', 'cause prob', 'cause prob ham']\n",
      "Topic:303  ['added contact list', 'com great', 'com great place', 'contact list', 'contact list www', 'free sms people']\n",
      "Topic:304  ['blow', '18p', '18p txt', 'amazing things', 'amazing things blow', 'believe true']\n",
      "Topic:305  ['behalf', 'behalf stunning', 'behalf stunning exam', 'day moji', 'day moji told', 'easily great']\n",
      "Topic:306  ['local', 'born', 'yor', '2b', '2b terminated', '2b terminated sorry']\n",
      "Topic:307  ['08452810073', '08452810073 details', '80182', '80182 entry', '80182 entry std', 'apply 08452810073']\n",
      "Topic:308  ['bit l8', 'bit l8 cos', 'buses hav', 'buses hav gon', 'cos buses', 'cos buses hav']\n",
      "Topic:309  ['bit l8', 'bit l8 cos', 'buses hav', 'buses hav gon', 'cos buses', 'cos buses hav']\n",
      "Topic:310  ['exeter', 'bit worried', 'boyf', 'boyf ive', 'boyf ive gotta', 'exams march']\n",
      "Topic:311  ['dhorte', 'dhorte lage', 'dhorte lage thats', 'great phone', 'great phone receiving', 'khelate']\n",
      "Topic:312  ['just know', 'dont involve shouldn', 'bother sending', 'bother sending dont', 'dont involve', 'hasn sent']\n",
      "Topic:313  ['afternoon casualty', 'afternoon casualty means', 'casualty', 'casualty means', 'casualty means haven', 'haven stuff42moro']\n",
      "Topic:314  ['come smoke telling', 'doing things', 'doing things hesitant', 'drive come smoke', 'emerging', 'emerging friends']\n",
      "Topic:315  ['come smoke telling', 'doing things', 'doing things hesitant', 'drive come smoke', 'emerging', 'emerging friends']\n",
      "Topic:316  ['dont publish shall', 'april real', 'april real date', 'birthdate certificate', 'birthdate certificate april', 'certificate']\n",
      "Topic:317  ['tomorrow pls pls', 'amp miracle', 'amp miracle tomorrow', 'amp need', 'amp need clean', 'blood send']\n",
      "Topic:318  ['charged 50', 'charged', 'order reference', 'ringtone order reference', 'thanks ringtone', 'thanks ringtone order']\n",
      "Topic:319  ['send postcard look', '1im', '1im talkin', '1im talkin bout', 'bout xx', 'girls im']\n",
      "Topic:320  ['advice gary', 'advice gary split', 'gary split', 'gary split person', 'good babe love', 'hope good babe']\n",
      "Topic:321  ['com index', 'com index wml', 'http tms', 'http tms widelive', 'index', 'index wml']\n",
      "Topic:322  ['com index', 'com index wml', 'http tms', 'http tms widelive', 'index', 'index wml']\n",
      "Topic:323  ['02 claimcode m39m51', '09 02', '09066368327', '500 guaranteed', '500 guaranteed award', '50pmmorefrommobile2bremoved']\n",
      "Topic:324  ['becoz loves', 'becoz loves smiling', 'hurts smile', 'hurts smile becoz', 'loves smiling', 'pain smile']\n",
      "Topic:325  ['born', 'yor', '2b', '2b terminated', '2b terminated sorry', 'actually born']\n",
      "Topic:326  ['network customer', 'valued network', 'valued network customer', 'winner valued', 'winner valued network', 'like rain smile']\n",
      "Topic:327  ['2u soonlots', '2u soonlots loveme', 'babe enjoyin', 'babe enjoyin yourjob', 'boo babe', 'boo babe enjoyin']\n",
      "Topic:328  ['bridal', 'bridal petticoatdreams', 'bridal petticoatdreams uk', 'brought weddingfriend', 'choose superb', 'choose superb selection']\n",
      "Topic:329  ['awkward', 'awkward den', 'awkward den meet', 'cos later', 'cos later leave', 'da ge den']\n",
      "Topic:330  ['awkward', 'awkward den', 'awkward den meet', 'cos later', 'cos later leave', 'da ge den']\n",
      "Topic:331  ['ac smsrewards', 'ac smsrewards end', 'bid notifications', 'bid notifications reply', 'bid visit', 'bid visit sms']\n",
      "Topic:332  ['fall love', '83110', '83110 meet', 'dating service just', 'discreet text', 'discreet text dating']\n",
      "Topic:333  ['atlast iz', 'atlast iz lonlines', 'built', 'built dis', 'built dis world', 'dis world']\n",
      "Topic:334  ['500 texts', '500 texts use', '80160', '80160 www', '80160 www txt43', 'just send txt']\n",
      "Topic:335  ['dont know number', 'going sound', 'going sound promoting', 'got stuff sorted', 'haunt', 'haunt got']\n",
      "Topic:336  ['dont know number', 'going sound', 'going sound promoting', 'got stuff sorted', 'haunt', 'haunt got']\n",
      "Topic:337  ['actin', 'actin like', 'actin like spoilt', 'apologetic', 'apologetic fallen', 'apologetic fallen actin']\n",
      "Topic:338  ['bak terry', 'getting sat', 'getting sat ill', 'getting tickets', 'getting tickets walsall', 'ill pay']\n",
      "Topic:339  ['ask bring', 'ask bring meeting', 'bring meeting', 'da guys', 'da guys neva', 'guys neva']\n",
      "Topic:340  ['ask bring', 'ask bring meeting', 'bring meeting', 'da guys', 'da guys neva', 'guys neva']\n",
      "Topic:341  ['come home having', 'date sure', 'date sure ready', 'dinner help', 'dinner help ready', 'feet make']\n",
      "Topic:342  ['50 cancel', '50 cancel send', 'babe im hot', 'babe make', 'babe make horny', 'cmon']\n",
      "Topic:343  ['address dob', 'address dob asap', 'asap ta', 'birds chaps', 'birds chaps user', 'champneys']\n",
      "Topic:344  ['400 reward', '400 reward just', '400 xmas', '400 xmas reward', 'computer randomly', 'computer randomly picked']\n",
      "Topic:345  ['ur face', 'miles', 'away smiling', 'away smiling good', 'difference smile', 'difference smile ur']\n",
      "Topic:346  ['attempt failed', 'attempt failed network', 'content order', 'content order resent', 'customersqueries', 'customersqueries netvision']\n",
      "Topic:347  ['500 ur', '500 ur question', '80', '80 answer', '80 answer txt', '83600 good']\n",
      "Topic:348  ['blood pressure', 'blood pressure normal', 'blood sugar', 'blood sugar tests', 'gained', 'gained lt']\n",
      "Topic:349  ['bored ll', 'bored ll ring', 'chat things', 'chat things ok', 'drop text', 'drop text free']\n",
      "Topic:350  ['09061104283 ts', '09061104283 ts cs', '50pm', '50pm approx', '50pm approx 3mins', 'approx 3mins']\n",
      "Topic:351  ['21 matches', '21 matches 09056242159', 'alert messages', 'alert messages 21', 'messages 21', 'messages 21 matches']\n",
      "Topic:352  ['fat gettin', 'fat gettin thinks', 'gettin juicy', 'gettin juicy gossip', 'gettin thinks', 'gettin thinks shes']\n",
      "Topic:353  ['bawling', 'bawling eyes', 'bawling eyes feel', 'eyes feel', 'eyes feel like', 'failing']\n",
      "Topic:354  ['toot', 'abt rite', 'abt rite long', 'b4 prepared', 'b4 prepared wat', 'eat forgot']\n",
      "Topic:355  ['shot', 'cough', 'cough today', 'cough today dry', 'day got', 'day got really']\n",
      "Topic:356  ['toot', 'abt rite', 'abt rite long', 'b4 prepared', 'b4 prepared wat', 'eat forgot']\n",
      "Topic:357  ['bawling', 'bawling eyes', 'bawling eyes feel', 'eyes feel', 'eyes feel like', 'failing']\n",
      "Topic:358  ['cali great', 'cali great complexities', 'car freely', 'car freely taxes', 'complexities', 'complexities great']\n",
      "Topic:359  ['aust', 'aust bk', 'aust bk weight', 'bk', 'bk weight', 'den got']\n",
      "Topic:360  ['aust', 'aust bk', 'aust bk weight', 'bk', 'bk weight', 'den got']\n",
      "Topic:361  ['blessing times', 'breather', 'breather promise', 'breather promise wont', 'fulfil', 'fulfil promise']\n",
      "Topic:362  ['69988', '69988 age', '69988 age verify', 'age verify', 'age verify yr', 'chat svc']\n",
      "Topic:363  ['69988', '69988 age', '69988 age verify', 'age verify', 'age verify yr', 'chat svc']\n",
      "Topic:364  ['bt day', 'bt day wen', 'day wen', 'day wen ll', 'excellent thought', 'excellent thought misundrstud']\n",
      "Topic:365  ['amrca', 'amrca thing', 'amrca thing speak', 'camp', 'camp amrca', 'camp amrca thing']\n",
      "Topic:366  ['angry misbehaved', 'angry misbehaved hurt', 'basically good', 'bcoz ur', 'bcoz ur fault', 'day angry']\n",
      "Topic:367  ['rgent', 'rgent 2nd', 'rgent 2nd attempt', '09071512433 b4', '1250', '1250 09071512433']\n",
      "Topic:368  ['angry misbehaved', 'angry misbehaved hurt', 'basically good', 'bcoz ur', 'bcoz ur fault', 'day angry']\n",
      "Topic:369  ['08448350055 bt', '08448350055 bt line', '2p', '2p min', '2p min check', '2p min germany']\n",
      "Topic:370  ['50 choose', '50 choose 10', 'billing', 'billing msg', 'charged 50 choose', 'choose 10']\n",
      "Topic:371  ['ass facebook', 'ass facebook open', 'business pictures', 'business pictures ass', 'facebook open', 'facebook open people']\n",
      "Topic:372  ['09061209465 suprman', '09061209465', '150pm dont', '150pm dont miss', '5we 150pm dont', 'cinema pass']\n",
      "Topic:373  ['2000 pound', '2000 pound award', 'pound award', 'pound award 08712402050', 'receive 2000', 'receive 2000 pound']\n",
      "Topic:374  ['half price line', 'price line', 'price line rental', 'anytime network', 'anytime network mins', 'network mins']\n",
      "Topic:375  ['checkmate', 'checkmate chess', 'checkmate chess comes', 'chess', 'chess comes', 'chess comes persian']\n",
      "Topic:376  ['08448350055', '2p', '2p min', '2p min check', '2p min germany', 'bt line']\n",
      "Topic:377  ['accordin', 'accordin wat', 'accordin wat discussed', 'attend kb', 'attend kb sat', 'cos nt']\n",
      "Topic:378  ['checkmate', 'checkmate chess', 'checkmate chess comes', 'chess', 'chess comes', 'chess comes persian']\n",
      "Topic:379  ['donate', '50 added', '50 unicef', '50 unicef asian', '864233', '864233 50']\n",
      "Topic:380  ['able buy', 'able buy liquor', 'buy liquor', 'buy liquor guy', 'don hold', 'don hold somebody']\n",
      "Topic:381  ['2mrw love', '2mrw love janx', 'bout 2mrw', 'bout 2mrw love', 'broke knackered', 'broke knackered got']\n",
      "Topic:382  ['arty', 'arty collages', 'arty collages mo', 'bit arty', 'bit arty collages', 'collages']\n",
      "Topic:383  ['donate', '50 added', '50 unicef', '50 unicef asian', '864233', '864233 50']\n",
      "Topic:384  ['arty', 'arty collages', 'arty collages mo', 'bit arty', 'bit arty collages', 'collages']\n",
      "Topic:385  ['15541', '450', '450 uk', '450 uk break', '62735', '62735 450']\n",
      "Topic:386  ['ruin', 'amp step', 'amp step ruin', 'best msg', 'best msg hard', 'fall love amp']\n",
      "Topic:387  ['mobile txt', '2wks free', '2wks free goals', '87077 kick', '87077 kick new', '87077 villa']\n",
      "Topic:388  ['slice', 'eat slice', 'eat slice really', 'forced', 'forced eat', 'forced eat slice']\n",
      "Topic:389  ['cock', 'ass buzz', 'ass buzz chest', 'buzz ass', 'buzz ass buzz', 'buzz chest']\n",
      "Topic:390  ['08712466669 10p min', '08712466669', 'bears scallies', 'bears scallies skins', 'calling don', 'calling don miss']\n",
      "Topic:391  ['1x150p', '1x150p wk', 'chance win 250', 'ur chance', 'ur chance win', 'win 250']\n",
      "Topic:392  ['50 msg tncs', '69876', '69876 txts', '69876 txts cost', 'com txt xxuk', 'cost 50 msg']\n",
      "Topic:393  ['30 wanna somethin', 'baby im sat', 'bloody bus', 'bloody bus mo', 'bus mo', 'bus mo wont']\n",
      "Topic:394  ['donaot worry l8tr', 'alrite girl', 'alrite girl know', 'care sweet donaot', 'gail', 'gail neva']\n",
      "Topic:395  ['clean hee', 'clean room', 'clean room room', 'finish sch', 'finish sch lunch', 'home clean']\n",
      "Topic:396  ['lo', 'anot thk', 'anot thk got', 'dunno tahan power', 'forgot liao', 'got lo']\n",
      "Topic:397  ['1000 cash await', 'complimentary lux', 'holiday 1000', 'holiday 1000 cash', 'landline complimentary lux', 'lux']\n",
      "Topic:398  ['2stop txt', '2stop txt stop', '50p msg', '50p msg max6', 'age16 2stop', 'age16 2stop txt']\n",
      "Topic:399  ['bit didn', 'bit didn let', 'didn let', 'didn let fuck', 'evening french', 'evening french guy']\n",
      "Topic:400  ['cashbin', 'cashbin uk', 'best cash', 'best cash away', 'biggest best', 'biggest best cash']\n",
      "Topic:401  ['2stop txt', '2stop txt stop', '50p msg', '50p msg max6', 'age16 2stop', 'age16 2stop txt']\n",
      "Topic:402  ['9pm fab', '9pm fab new', 'better hear', 'better hear treated', 'cam good', 'cam good 9pm']\n",
      "Topic:403  ['baig', 'baig face', 'baig face watches', 'cos fr', 'cos fr thanx', 'dat baig']\n",
      "Topic:404  ['closeby', 'closeby tuesday', 'cool just raglan', 'cricket', 'cricket ground', 'cricket ground gimme']\n",
      "Topic:405  ['closeby', 'closeby tuesday', 'cool just raglan', 'cricket', 'cricket ground', 'cricket ground gimme']\n",
      "Topic:406  ['send bimbo', 'send bimbo ugo', 'appreciate safe', 'bimbo', 'bimbo ugo', 'bimbo ugo numbers']\n",
      "Topic:407  ['animal', 'animal just', 'animal just thought', 'buzz friends', 'buzz friends grins', 'friends grins']\n",
      "Topic:408  ['stick', 'entertaining', 'entertaining uo', 'entertaining uo getting', 'especially stick', 'getting hugh']\n",
      "Topic:409  ['blessings ur', 'blessings ur life', 'boundaries', 'boundaries god', 'boundaries god endless', 'endless']\n",
      "Topic:410  ['armand eventually', 'armand eventually build', 'build tolerance', 'build tolerance shit', 'considering smokes', 'considering smokes gets']\n",
      "Topic:411  ['complimentary lux', 'holiday 1000', 'holiday 1000 cash', 'landline complimentary lux', 'lux', '1000 cash await']\n",
      "Topic:412  ['bani', 'bani big', 'bani big pls', 'big pls', 'big pls love', 'called hoping']\n",
      "Topic:413  ['08712400602450p provided', '08712400602450p', '61610', '61610 unsubscribe', '61610 unsubscribe help', 'content text']\n",
      "Topic:414  ['like selling', 'break know', 'break know guy', 'doesn like', 'doesn like selling', 'fuck doesn']\n",
      "Topic:415  ['come taunton', 'come taunton tonight', 'day wudn', 'day wudn notice', 'didn mind', 'didn mind gonna']\n",
      "Topic:416  ['sending texts', 'atlanta friends', 'best mcat', 'best mcat got', 'got number atlanta', 'heard year']\n",
      "Topic:417  ['earning', 'earning money', 'earning money money', 'height recycling', 'height recycling read', 'money money spent']\n",
      "Topic:418  ['atlanta friends', 'best mcat', 'best mcat got', 'got number atlanta', 'heard year', 'heard year best']\n",
      "Topic:419  ['al post', 'al post ofice', 'becoz lt', 'becoz lt gt', 'cn fr', 'cn fr post']\n",
      "Topic:420  ['diamond', 'diamond stuff', 'diamond stuff leaving', 'dino', 'dino prem', 'excellent service']\n",
      "Topic:421  ['lim', 'lim look', 'lim look mp3', 'come shopping', 'come shopping course', 'course nice']\n",
      "Topic:422  ['accident claim', 'accident claim free', 'claim free reply', 'indicate', 'records indicate', 'accident']\n",
      "Topic:423  ['09099726481', '2morro half', '2morro half term', '2nite sexy', '2nite sexy passion', 'b4 chat']\n",
      "Topic:424  ['100 txts mobileupd8', '100 txts', 'line rental 12', '12 mths', '12 mths 500', '500 cross']\n",
      "Topic:425  ['click open', 'click open link', 'comes new', 'comes new selection', 'downloads members', 'downloads members free']\n",
      "Topic:426  ['350 award number', 'award number', 'award number matches', 'number matches', 'todays vodafone', 'todays vodafone numbers']\n",
      "Topic:427  ['worry ll', 'don worry ll', 'finished march', 'finished march u_', 'll finished', 'll finished march']\n",
      "Topic:428  ['02085076972 reply stop', '02085076972 reply', '150p msg netcollex', 'hear strt', 'hear strt 150p', 'horny willing']\n",
      "Topic:429  ['ago wat', 'ago wat copied', 'copied', 'copied jus', 'copied jus got', 'ended long']\n",
      "Topic:430  ['chance evaporated', 'chance evaporated soon', 'contact report', 'contact report supervisor', 'cool contact', 'cool contact report']\n",
      "Topic:431  ['amp best', 'amp best friends', 'best friends', 'best friends goodevening', 'care feel', 'care feel probably']\n",
      "Topic:432  ['kindly', 'check dlf', 'check dlf premarica', 'disturbance', 'disturbance receive', 'disturbance receive reference']\n",
      "Topic:433  ['01223585236', 'beg like', 'beg like did', 'did time 01223585236', 'heard u4 night', 'just knickers']\n",
      "Topic:434  ['alwys', 'alwys touch', 'alwys touch good', 'amp life', 'amp life frnds', 'dreams amp']\n",
      "Topic:435  ['amp best', 'amp best friends', 'best friends', 'best friends goodevening', 'care feel', 'care feel probably']\n",
      "Topic:436  ['0800 18', '18 legitimat', '18 legitimat efreefone', '32000', '32000 award', '32000 award maybe']\n",
      "Topic:437  ['kindly', 'check dlf', 'check dlf premarica', 'disturbance', 'disturbance receive', 'disturbance receive reference']\n",
      "Topic:438  ['day bslvyl', 'die plz', 'die plz rose', 'end life', 'end life die', 'grave']\n",
      "Topic:439  ['day bslvyl', 'die plz', 'die plz rose', 'end life', 'end life die', 'grave']\n",
      "Topic:440  ['150pm unsubscribe', '150pm unsubscribe text', '69698 text', '69698 text charged', 'charged 150pm', 'charged 150pm unsubscribe']\n",
      "Topic:441  ['desires', 'affections', 'affections amp', 'affections amp traditions', 'amp traditions', 'amp traditions ideal']\n",
      "Topic:442  ['aa', 'exhaust', 'aa exhaust', 'aa exhaust hanging', 'didn quite', 'didn quite plan']\n",
      "Topic:443  ['missing sent', 'date missing', 'date missing missing', 'lot thats', 'lot thats missing', 'message text']\n",
      "Topic:444  ['aa', 'exhaust', 'aa exhaust', 'aa exhaust hanging', 'didn quite', 'didn quite plan']\n",
      "Topic:445  ['answer questions', 'answer questions chance', 'book reply', 'book reply harry', 'chance readers', 'harry']\n",
      "Topic:446  ['cartoon person', 'cartoon person irritates', 'express gud', 'express gud nyt', 'fails', 'fails express']\n",
      "Topic:447  ['free ringtone', 'like wil getting', '30ish desperate', '30ish desperate company', 'bout 30ish desperate', 'company head']\n",
      "Topic:448  ['89105', '89105 free', '89105 free extra', 'apply 18', 'apply 18 yrs', 'collect text']\n",
      "Topic:449  ['depends individual', 'depends individual lor', 'dresser say', 'dresser say pretty', 'dunno wat collecting', 'gong']\n",
      "Topic:450  ['depends individual', 'depends individual lor', 'dresser say', 'dresser say pretty', 'dunno wat collecting', 'gong']\n",
      "Topic:451  ['1000 quiz partner', 'custcare 08718720201', 'day special', 'day special win', 'lifetime send', 'lifetime send 83600']\n",
      "Topic:452  ['hope know', '2day normal', '2day normal way', 'hi way', 'hi way 2day', 'hope know rest']\n",
      "Topic:453  ['alrite jod', 'alrite jod hows', 'bin doin', 'bin doin smidgin', 'college xx', 'cum college']\n",
      "Topic:454  ['loverboy', 'afternoon loverboy', 'afternoon loverboy goes', 'come way', 'come way think', 'day luck']\n",
      "Topic:455  ['falls', 'brothers', 'brothers head', 'brothers head whos', 'falls brothers', 'falls brothers head']\n",
      "Topic:456  ['double mins double', 'double txt', 'mins double', 'mins double txt', '08000839402 call2optout lf56', 'bluetooth mobiles']\n",
      "Topic:457  ['beta', 'beta ll', 'beta ll yan', 'den msg', 'enjoy ur tuition', 'gee thk']\n",
      "Topic:458  ['bt fightng', 'bt fightng some1', 'close dificult', 'close dificult lose', 'dificult', 'dificult lose']\n",
      "Topic:459  ['called ubandu', 'called ubandu run', 'copy important', 'copy important files', 'disk', 'disk use']\n",
      "Topic:460  ['beta', 'beta ll', 'beta ll yan', 'den msg', 'enjoy ur tuition', 'gee thk']\n",
      "Topic:461  ['89105', '89105 free', '89105 free extra', 'apply 18', 'apply 18 yrs', 'collect text']\n",
      "Topic:462  ['asjesus', 'asjesus com', 'asjesus com read', 'com read', 'com read wrote', 'created web']\n",
      "Topic:463  ['anyways gr8', 'anyways gr8 weekend', 'approaches', 'approaches hurts', 'approaches hurts studying', 'end month approaches']\n",
      "Topic:464  ['08081263000 charges refunded', '08081263000 charges', '83332', '83332 08081263000', '83332 08081263000 charges', 'billed mobile']\n",
      "Topic:465  ['87239', 'stop 87239', 'send stop 87239', '87239 customer', '87239 customer services', 'customer services 08708034412']\n",
      "Topic:466  ['anyways gr8', 'anyways gr8 weekend', 'approaches', 'approaches hurts', 'approaches hurts studying', 'end month approaches']\n",
      "Topic:467  ['having fun', 'bed girl', 'bed girl doesn', 'cooped', 'cooped bed', 'does stay']\n",
      "Topic:468  ['make ur', 'alive gud', 'alive gud night', 'feel alive', 'feel alive gud', 'friend feel']\n",
      "Topic:469  ['bein thot', 'bein thot great', 'enjoyed game', 'enjoyed game yesterday', 'fondly', 'fondly bein']\n",
      "Topic:470  ['bein thot', 'bein thot great', 'enjoyed game', 'enjoyed game yesterday', 'fondly', 'fondly bein']\n",
      "Topic:471  ['100 cash new', 'ans 80082', 'cash new', 'cash new president', 'continued', 'continued support']\n",
      "Topic:472  ['drms shesil', 'gd nt', 'gd nt swt', 'making think', 'making think moment', 'moment gd']\n",
      "Topic:473  ['drms shesil', 'gd nt', 'gd nt swt', 'making think', 'making think moment', 'moment gd']\n",
      "Topic:474  ['50 rcv', 'chgs', 'chgs send', 'chgs send 50', 'darling week', 'darling week word']\n",
      "Topic:475  ['50 rcv', 'chgs', 'chgs send', 'chgs send 50', 'darling week', 'darling week word']\n",
      "Topic:476  ['xxxmobilemovieclub', 'click http', 'click http wap', 'click wap', 'click wap link', 'com qjkgighjjgcbl']\n",
      "Topic:477  ['line said broken', 'broken heart', 'broken heart plz', 'cum times', 'cum times infront', 'don cum']\n",
      "Topic:478  ['babygoodbye', 'babygoodbye golddigger', 'babygoodbye golddigger webeburnin', 'dontcha', 'dontcha babygoodbye', 'dontcha babygoodbye golddigger']\n",
      "Topic:479  ['150p 18', 'club unsubscribe', 'club unsubscribe service', 'dogging club', 'dogging club unsubscribe', 'free welcome']\n",
      "Topic:480  ['drink tap', 'drink tap spile', 'bout drink', 'bout drink tap', 'thought bout', 'thought bout drink']\n",
      "Topic:481  ['08704439680 booking quote', '08704439680 booking', '08704439680', '1st class airport', 'airport lounge', 'airport lounge passes']\n",
      "Topic:482  ['asking practicum', 'asking practicum links', 'best ttyl', 'consistently', 'consistently intelligent', 'consistently intelligent kind']\n",
      "Topic:483  ['said ask', 'ah said', 'ah said ask', 'ask ah', 'ask ah said', 'ask fri']\n",
      "Topic:484  ['didn want', 'didn want hospital', 'don know stubborn', 'hospital kept', 'hospital kept telling', 'hospitals weak']\n",
      "Topic:485  ['definite greece', 'definite greece maybe', 'going suggest', 'going suggest setting', 'greece', 'greece maybe']\n",
      "Topic:486  ['choose friend', 'choose friend long', 'ends lets', 'ends lets friends', 'forever gud', 'forever gud nitz']\n",
      "Topic:487  ['ages yeah', 'canceled', 'canceled havn', 'canceled havn seen', 'concert tmw', 'concert tmw canceled']\n",
      "Topic:488  ['ages yeah', 'canceled', 'canceled havn', 'canceled havn seen', 'concert tmw', 'concert tmw canceled']\n",
      "Topic:489  ['crushes', 'dat person', 'dat person till', 'decided perfect', 'decided perfect time', 'den enjoy']\n",
      "Topic:490  ['ass did', 'ass did gym', 'boytoy smacks', 'boytoy smacks ass', 'buns sugar', 'buns sugar plum']\n",
      "Topic:491  ['understanding', 'actually start', 'actually start understanding', 'big things', 'big things actually', 'evening bslvyl']\n",
      "Topic:492  ['goodmorning amp', 'amp nice day', 'birds waiting', 'birds waiting wish', 'breeze', 'breeze bright']\n",
      "Topic:493  ['joanna', 'asking stuff', 'asking stuff myspace', 'freaking', 'freaking looked', 'freaking looked friends']\n",
      "Topic:494  ['cause today', 'cause today planning', 'didn eat', 'didn eat sweets', 'dieting', 'dieting week']\n",
      "Topic:495  ['joanna', 'omg joanna', 'omg joanna freaking', 'asking stuff', 'asking stuff myspace', 'freaking']\n",
      "Topic:496  ['deal stuff', 'deal stuff grownup', 'grownup', 'grownup stuff', 'grownup stuff don', 'kid supposed']\n",
      "Topic:497  ['cheer tear', 'cheer tear dear', 'dear dear', 'dear dear near', 'dear gud', 'dear gud ni8']\n",
      "Topic:498  ['4give', '4give think', '4give think shldxxxx', 'bout nite', 'bout nite wasnaot', 'fault spouse']\n",
      "Topic:499  ['crucial', 'crucial things', 'crucial things life', 'forget smile', 'forget smile smile', 'forget time']\n",
      "Topic:500  ['bring happiness', 'bring happiness stability', 'colourful', 'colourful life', 'family new', 'family new year']\n",
      "Topic:501  ['0121', '0121 2025050', '2025050', '2025050 visit', '2025050 visit www', '45']\n",
      "Topic:502  ['50 pounds', '50 pounds free', 'credit details', 'credit details great', 'details great', 'details great offers']\n",
      "Topic:503  ['lots love', 'a30', 'a30 divert', 'a30 divert wadebridge', 'accident a30', 'accident a30 divert']\n",
      "Topic:504  ['come kerala', 'come kerala days', 'days prepared', 'days prepared leave', 'dont plan', 'dont plan travel']\n",
      "Topic:505  ['age followed', 'text age', 'text age followed', '23f', '23f gay', '23f gay men']\n",
      "Topic:506  ['answers really', 'answers really loves', 'colour suits', 'colour suits best', 'dis msg', 'dis msg ur']\n",
      "Topic:507  ['double check', 'check wif', 'check wif da', 'cut look', 'cut look nice', 'cut short said']\n",
      "Topic:508  ['tonight ill', 'awww', 'awww dat', 'awww dat sweet', 'cos im lonely', 'dat sweet']\n",
      "Topic:509  ['cares tosend', 'cares tosend warm', 'daywith', 'daywith thoughts', 'daywith thoughts somewheresomeone', 'greeting']\n",
      "Topic:510  ['stuff like', 'called hey', 'called hey parking', 'collapsed', 'collapsed university', 'collapsed university hospital']\n",
      "Topic:511  ['certainly don', 'certainly don mind', 'don mind friend', 'mind friend', 'monkeys', 'monkeys wot']\n",
      "Topic:512  ['assume mindset', 'assume mindset believe', 'begin anytime', 'believe evening', 'believe evening wonderful', 'dont feel']\n",
      "Topic:513  ['250 weekly', 'entry 250', 'entry 250 weekly', 'free entry 250', '18 www', 'send word']\n",
      "Topic:514  ['abuse', 'anonymous', 'anonymous masked', 'anonymous masked messages', 'com lets', 'com lets send']\n",
      "Topic:515  ['abuse', 'anonymous', 'anonymous masked', 'anonymous masked messages', 'com lets', 'com lets send']\n",
      "Topic:516  ['150p text', 'babe chloe', 'babe chloe smashed', 'chloe', 'chloe smashed', 'chloe smashed saturday']\n",
      "Topic:517  ['dont ignore', 'exact prob', 'exact prob fixed', 'fixed gpu', 'fixed gpu replacement', 'forum post']\n",
      "Topic:518  ['dont ignore', 'exact prob', 'exact prob fixed', 'fixed gpu', 'fixed gpu replacement', 'forum post']\n",
      "Topic:519  ['strike', 'doing make', 'doing make strike', 'finally kerala', 'finally kerala version', 'indian version']\n",
      "Topic:520  ['sight', 'ends sh', 'ends sh jas', 'jas', 'life ends sh', 'sh jas']\n",
      "Topic:521  ['400 proze', '400 proze guaranteed', '50 mtmsgrcvd18', '83355 norcorp', '83355 norcorp 50', 'guaranteed reply']\n",
      "Topic:522  ['sat sun', '1405', '1405 1680', '1405 1680 1843', '1680', '1680 1843']\n",
      "Topic:523  ['accidant', 'accidant tookplace', 'accidant tookplace ghodbandar', 'amp don', 'amp don worry', 'ghodbandar']\n",
      "Topic:524  ['sat sun', 'buy 4d', 'buy 4d dad', '1405', '1405 1680', '1405 1680 1843']\n",
      "Topic:525  ['air opinions', 'air opinions categories', 'busy transcribing', 'categories', 'categories used', 'categories used measure']\n",
      "Topic:526  ['double coins', 'bed double', 'bed double coins', 'came bed', 'came bed double', 'coins factory']\n",
      "Topic:527  ['double coins', 'bed double', 'bed double coins', 'came bed', 'came bed double', 'coins factory']\n",
      "Topic:528  ['alright fone', 'alright fone fone', 'baby im cruisin', 'cruisin', 'cruisin girl', 'cruisin girl friend']\n",
      "Topic:529  ['contact weekends', 'contact weekends draw', 'trying contact weekends', 'weekends draw', 'weekends draw shows', 'shows won 1000']\n",
      "Topic:530  ['ahhhh', 'ahhhh just', 'ahhhh just woken', 'bad dream', 'bad dream tho', 'comedy night']\n",
      "Topic:531  ['beautiful truth gravity', 'carefully', 'carefully heart', 'carefully heart feels', 'feels heavy', 'feels heavy leaves']\n",
      "Topic:532  ['clubmoby', 'clubmoby com', 'clubmoby com 08717509990', 'com 08717509990', 'com 08717509990 poly', 'content www']\n",
      "Topic:533  ['receive 1000', 'selected receive 1000', 'receive 1000 cash', 'winner specially', 'winner specially selected', 'specially selected']\n",
      "Topic:534  ['clocks', 'clocks til', 'clocks til shouted', 'got room soon', 'hadn', 'hadn clocks']\n",
      "Topic:535  ['1thing', '1thing got', '1thing got answr', 'answer brilliant', 'answer brilliant 1thing', 'answr']\n",
      "Topic:536  ['darlin work', 'darlin work did', 'did trouble', 'did trouble ijust', 'goin soon', 'good time night']\n",
      "Topic:537  ['days satanic', 'days satanic imposter', 'destiny', 'destiny going', 'destiny going said', 'going said']\n",
      "Topic:538  ['amigos', 'amigos hoping', 'amigos hoping end', 'burn', 'burn think', 'burn think swing']\n",
      "Topic:539  ['amp taylor', 'amp taylor walmart', 'derek amp', 'derek amp taylor', 'desk', 'desk ll']\n",
      "Topic:540  ['14tcr', '14tcr w1', '14tcr w1 16', '6230', '6230 plus', '6230 plus free']\n",
      "Topic:541  ['amp taylor', 'amp taylor walmart', 'derek amp', 'derek amp taylor', 'desk', 'desk ll']\n",
      "Topic:542  ['cs box', 'sae cs box', 'abta', 'abta complimentary', 'complimentary tenerife', 'complimentary tenerife holiday']\n",
      "Topic:543  ['babes hunks', 'babes hunks straight', 'gotbabes', 'gotbabes uk', 'gotbabes uk subscriptions', 'http gotbabes']\n",
      "Topic:544  ['beautiful intelligent', 'amy', 'donaot like', 'donaot like like', 'amy ure', 'amy ure beautiful']\n",
      "Topic:545  ['address lt', 'address lt gt', 'hill address', 'hill address lt', 'left victors', 'left victors hill']\n",
      "Topic:546  ['afghanistan', 'afghanistan stable', 'afghanistan stable honest', 'air force', 'air force iraq', 'excellent spent']\n",
      "Topic:547  ['ability listen', 'ability listen unconditionally', 'confidence means', 'confidence means married', 'develop ability', 'develop ability listen']\n",
      "Topic:548  ['good hear omg', 'having problems provider', 'hear omg', 'hear omg missed', 'love soooo', 'love soooo good']\n",
      "Topic:549  ['good hear omg', 'having problems provider', 'hear omg', 'hear omg missed', 'love soooo', 'love soooo good']\n",
      "Topic:550  ['explain', 'actually mobile', 'actually mobile msg', 'doing work', 'doing work online', 'explain later']\n",
      "Topic:551  ['alright phews', 'alright phews miss', 'cell way', 'cell way stairs', 'day fuck', 'day fuck morning']\n",
      "Topic:552  ['dippeditinadew', 'dippeditinadew lovingly', 'dippeditinadew lovingly touched', 'flower dippeditinadew', 'flower dippeditinadew lovingly', 'friend 4u']\n",
      "Topic:553  ['0871', '07808726822 awarded', '0871 872 9758', 'attempt contact 0871', 'awarded 000', 'awarded 000 bonus']\n",
      "Topic:554  ['ll going', 'cos mys', 'cos mys sis', 'da morn', 'early lor', 'early lor cos']\n",
      "Topic:555  ['chip fat', 'chip fat thanks', 'coach', 'coach hot', 'coach hot smells', 'duvet']\n",
      "Topic:556  ['announced', 'announced blog', 'blog', 'case collecting', 'case collecting week', 'cashed']\n",
      "Topic:557  ['breathe', 'think love', 'afternoon sunshine', 'afternoon sunshine dawns', 'air smile', 'air smile think']\n",
      "Topic:558  ['book lesson', 'ah thk', 'ah thk mayb', 'anythin thk', 'anythin thk book', 'book lesson pilates']\n",
      "Topic:559  ['confused', 'ah opps', 'ah opps got', 'best choice den', 'choice den', 'choice den juz']\n",
      "Topic:560  ['dressed', 'celebrate magical', 'celebrate magical sight', 'days celebrate', 'days celebrate magical', 'dressed white']\n",
      "Topic:561  ['faith', 'makes things', 'beautiful christmas', 'beautiful christmas merry', 'christmas merry', 'christmas merry christmas']\n",
      "Topic:562  ['account thanks', 'account thanks got', 'balance don', 'balance don statements', 'checked balance', 'checked balance don']\n",
      "Topic:563  ['confused', 'ah opps', 'ah opps got', 'best choice den', 'choice den', 'choice den juz']\n",
      "Topic:564  ['09061701444 valid', '24 hours', '24 hours acl03530150pm', '900 reward', '900 reward collect', 'collect 09061701444']\n",
      "Topic:565  ['wifi', '3g lt', '3g lt gt', 'ago guy', 'ago guy wants', 'blanked']\n",
      "Topic:566  ['bloke', 'apologise cali', 'apologise cali sweet', 'bloke weddin', 'cali sweet', 'cali sweet come']\n",
      "Topic:567  ['mac', 'appeal', 'appeal tomo', 'appeal tomo fr', 'cinema plus', 'cinema plus drink']\n",
      "Topic:568  ['dearly missed', 'dearly missed good', 'didnt work', 'didnt work oh', 'fix ready', 'fix ready time']\n",
      "Topic:569  ['mac', 'appeal', 'appeal tomo', 'appeal tomo fr', 'cinema plus', 'cinema plus drink']\n",
      "Topic:570  ['wifi', '3g lt', '3g lt gt', 'ago guy', 'ago guy wants', 'blanked']\n",
      "Topic:571  ['ad row', 'ad row wiv', 'beautiful ok', 'beautiful ok ve', 'hello beautiful', 'hello beautiful ok']\n",
      "Topic:572  ['mac', 'appeal', 'appeal tomo', 'appeal tomo fr', 'cinema plus', 'cinema plus drink']\n",
      "Topic:573  ['allo', 'allo braved', 'allo braved buses', 'braved', 'braved buses', 'braved buses taken']\n",
      "Topic:574  ['careful', '09066358152', '5000 prize enter', 'claim 5000', 'claim 5000 prize', 'details prompts']\n",
      "Topic:575  ['double mins txts', 'mins txts', '6months free', '6months free bluetooth', 'available sony', 'available sony nokia']\n",
      "Topic:576  ['agidhane', 'aunty manege', 'aunty manege day', 'black agidhane', 'chinnu', 'chinnu weak']\n",
      "Topic:577  ['feel better', 'bad wanting', 'better ipads', 'better ipads worthless', 'cocksuckers', 'cocksuckers makes']\n",
      "Topic:578  ['21 coz', '21 coz thought', 'book think', 'book think 21', 'coz thought', 'coz thought wanna']\n",
      "Topic:579  ['16 free', '16 free msgs', '85555', '85555 16', '85555 16 free', 'chance 85555']\n",
      "Topic:580  ['baby great', 'baby great building', 'building map', 'building map imprtant', 'crying', 'crying baby']\n",
      "Topic:581  ['baby great', 'baby great building', 'building map', 'building map imprtant', 'crying', 'crying baby']\n",
      "Topic:582  ['baby great', 'baby great building', 'building map', 'building map imprtant', 'crying', 'crying baby']\n",
      "Topic:583  ['09066364311 collect award', '09066364311', '350 award pls', 'award pls', 'award pls claim', 'award selected']\n",
      "Topic:584  ['armenia', 'armenia ends', 'armenia ends swann', 'away house', 'away house ll', 'block away']\n",
      "Topic:585  ['armenia', 'armenia ends', 'armenia ends swann', 'away house', 'away house ll', 'block away']\n",
      "Topic:586  ['christmas hug', 'christmas hug lik', 'cute luvd', 'cute luvd lucky', 'den hug', 'den hug cute']\n",
      "Topic:587  ['aftr decades', 'aftr decades beer', 'beer cheaper', 'beer cheaper petrol', 'cheaper petrol', 'cheaper petrol goverment']\n",
      "Topic:588  ['wat makes', 'dat feel', 'dat feel meet', 'dearer', 'dearer just', 'dearer just happiness']\n",
      "Topic:589  ['eighth', 'usually', 'eighth smarter', 'eighth smarter gets', 'gets second', 'gets second gram']\n",
      "Topic:590  ['email log', 'email log usc', 'explain things', 'explain things home', 'home great', 'home great weekend']\n",
      "Topic:591  ['experience reply', 'experience reply toll', 'free yes', 'hi just', 'hi just spoke', 'just spoke']\n",
      "Topic:592  ['eighth', 'usually', 'eighth smarter', 'eighth smarter gets', 'gets second', 'gets second gram']\n",
      "Topic:593  ['3d imp', 'imp', '3d imp point', 'actually flies', 'actually flies room', 'avatar 3d']\n",
      "Topic:594  ['bell ya', 'bell ya finish', 'couple gays', 'couple gays mean', 'games bell', 'games bell ya']\n",
      "Topic:595  ['eighth', 'usually', 'eighth smarter', 'eighth smarter gets', 'gets second', 'gets second gram']\n",
      "Topic:596  ['3d imp', 'imp', '3d imp point', 'actually flies', 'actually flies room', 'avatar 3d']\n",
      "Topic:597  ['home tomorrow', '5ish', '7am', '7am day', '7am day party', 'day party']\n",
      "Topic:598  ['bell ya', 'bell ya finish', 'couple gays', 'couple gays mean', 'games bell', 'games bell ya']\n",
      "Topic:599  ['great deal', '50 deposit', '50 deposit 16', '5pm 95', '5pm 95 pax', '95']\n",
      "Topic:600  ['advice thanks', 'advice thanks hope', 'bit longer crowd', 'crowd', 'crowd try', 'crowd try later']\n",
      "Topic:601  ['driving sure park', 'asthma', 'asthma attack', 'asthma attack nxt', 'attack', 'attack nxt']\n",
      "Topic:602  ['ah i_', 'ah i_ wkg', 'btw i_', 'btw i_ nus', 'i_ nus', 'i_ nus sc']\n",
      "Topic:603  ['ah sat', 'ah sat night', 'booked kb', 'booked kb sat', 'confirm lodging', 'free need meet']\n",
      "Topic:604  ['beauty life', 'beauty life second', 'gud n8', 'hides', 'hides thousands', 'hides thousands secrets']\n",
      "Topic:605  ['09066612661 landline complementary', '150ppm 18 sender', '18 sender', '18 sender hol', '2px', '2px 150ppm']\n",
      "Topic:606  ['ah i_', 'ah i_ wkg', 'btw i_', 'btw i_ nus', 'i_ nus', 'i_ nus sc']\n",
      "Topic:607  ['determine', 'determine smokes', 'determine smokes entire', 'entire', 'entire month', 'entire month february']\n",
      "Topic:608  ['amp fills', 'amp fills gaps', 'comes amp', 'comes amp fills', 'created gap fingers', 'fills gaps']\n",
      "Topic:609  ['best mobile', 'best mobile content', 'content service', 'content service uk', 'days send', 'days send stop']\n",
      "Topic:610  ['world running', 'admit mad', 'admit mad correction', 'correction let', 'correction let life', 'feeling admit']\n",
      "Topic:611  ['afternoon sexy', 'afternoon sexy buns', 'buns goes', 'buns goes job', 'fine happy know', 'goes job']\n",
      "Topic:612  ['18 8007', '18 8007 join', '8007 join', '8007 join chatting', 'age girl', 'age girl zoe']\n",
      "Topic:613  ['10 did', '50 shd', '50 shd ok', 'did i_ leave', 'i_ leave', 'i_ leave line']\n",
      "Topic:614  ['10 did i_', '50 shd', '50 shd ok', 'did i_ leave', 'i_ leave', 'i_ leave line']\n",
      "Topic:615  ['easier', '5years', '5years salary', '5years salary makes', 'apply phd', 'apply phd 5years']\n",
      "Topic:616  ['albi', 'albi mufti', 'albi mufti mahfuuz', 'allah tohar', 'allah tohar beeen', 'aslamalaikkum']\n",
      "Topic:617  ['alright babe worry', 'babe worry', 'babe worry felt', 'bit desparate', 'bit desparate learned', 'come love']\n",
      "Topic:618  ['dear urgnt', 'dear urgnt don', 'don know whats', 'hi dear urgnt', 'know whats', 'know whats problem']\n",
      "Topic:619  ['alright babe worry', 'babe worry', 'babe worry felt', 'bit desparate', 'bit desparate learned', 'come love']\n",
      "Topic:620  ['08717205546', 'code rp176781', 'code rp176781 stop', 'customer services 08717205546', 'discount code', 'discount code rp176781']\n",
      "Topic:621  ['don want work', 'dear urgnt', 'dear urgnt don', 'don know whats', 'hi dear urgnt', 'know whats']\n",
      "Topic:622  ['08717205546', 'code rp176781', 'code rp176781 stop', 'customer services 08717205546', 'discount code', 'discount code rp176781']\n",
      "Topic:623  ['did sleep', 'hope day goes', 'babe long', 'babe long moment', 'buzz hey', 'buzz hey love']\n",
      "Topic:624  ['dear urgnt', 'dear urgnt don', 'don know whats', 'hi dear urgnt', 'know whats', 'know whats problem']\n",
      "Topic:625  ['attributed', 'attributed coming', 'attributed coming making', 'come smoke day', 'coming making', 'coming making smoke']\n",
      "Topic:626  ['laughing simple', 'laughing simple winning', 'living simple', 'living simple loving', 'loving simple', 'loving simple laughing']\n",
      "Topic:627  ['activities planned', 'activities planned debating', 'debating', 'debating play', 'debating play football', 'eve feeling']\n",
      "Topic:628  ['1000s choose', 'send yr', 'send yr buddys', '1000s choose send', 'buddys', 'buddys txt']\n",
      "Topic:629  ['deliver', 'benefits thanks', 'coming deliver', 'coming deliver know', 'costs risks', 'costs risks benefits']\n",
      "Topic:630  ['bleh', 'better friday', 'better friday stuffed', 'bleh writhing', 'bleh writhing pain', 'feel bleh']\n",
      "Topic:631  ['adjustable', 'adjustable cooperative', 'adjustable cooperative wife', 'allows', 'allows wife', 'beautiful intelligent caring']\n",
      "Topic:632  ['burnt', 'burnt fingers', 'burnt fingers getting', 'cup tea', 'cup tea promptly', 'dropped keys']\n",
      "Topic:633  ['hurts talk', 'stop calling', 'answering everyones', 'answering everyones calls', 'babysitting', 'babysitting monday']\n",
      "Topic:634  ['hurts talk', 'stop calling', 'calling hurts', 'calling hurts talk', 'cancer sister', 'cancer sister won']\n",
      "Topic:635  ['burnt', 'burnt fingers', 'burnt fingers getting', 'cup tea', 'cup tea promptly', 'dropped keys']\n",
      "Topic:636  ['pack', 'dont pack', 'dont pack buy', '9ja miss', 'cereals', 'cereals pack']\n",
      "Topic:637  ['6hrs appendix', '6hrs appendix age', 'age range', 'age range impossible', 'appendix', 'appendix age']\n",
      "Topic:638  ['2go did', '2go did little', 'did little', 'did little thing', 'doctors', 'doctors reminds']\n",
      "Topic:639  ['boston personal', 'boston personal statement', 'changed realized', 'changed realized said', 'especially talk', 'especially talk boston']\n",
      "Topic:640  ['150p min 18', 'datebox1282essexcm61xn', 'datebox1282essexcm61xn 150p', 'datebox1282essexcm61xn 150p min', 'fancies wanna', 'guess somebody']\n",
      "Topic:641  ['canada eh', 'canada eh wow', 'day extra', 'day extra time', 'eh wow', 'eh wow work']\n",
      "Topic:642  ['accept life', 'accept life way', 'comes happiness', 'comes happiness moment', 'happiness main', 'happiness main sources']\n",
      "Topic:643  ['boston personal', 'boston personal statement', 'changed realized', 'changed realized said', 'especially talk', 'especially talk boston']\n",
      "Topic:644  ['free mobileupd8', 'hv9d', 'linerental motorola', 'mins txts orange', 'mobileupd8 08000839402 or2optout', 'motorola sonyericsson tooth']\n",
      "Topic:645  ['address occurs', 'address occurs quite', 'aight ve', 'aight ve set', 'blake address occurs', 'doing thought']\n",
      "Topic:646  ['40mph', '40mph hey', '40mph hey ho', '63miles', '63miles good', '63miles good thing']\n",
      "Topic:647  ['dreams stars', 'dreams stars make', 'peaceful', 'color dreams', 'color dreams stars', 'come color']\n",
      "Topic:648  ['food social', 'food social support', 'friends school', 'friends school things', 'like food', 'like food social']\n",
      "Topic:649  ['accept life', 'accept life way', 'comes happiness', 'comes happiness moment', 'happiness main', 'happiness main sources']\n",
      "Topic:650  ['bars', 'bars ptbo', 'bars ptbo blue', 'blue heron', 'blue heron going', 'heron']\n",
      "Topic:651  ['address occurs', 'address occurs quite', 'aight ve', 'aight ve set', 'blake address occurs', 'doing thought']\n",
      "Topic:652  ['2nd lesson', '2nd lesson 8am', 'going learn', 'going learn 2nd', 'home look', 'home look timings']\n",
      "Topic:653  ['ask different', 'ask different did', 'common hearin', 'common hearin wat', 'day let', 'day let ask']\n",
      "Topic:654  ['better evening', 'better evening im', 'costume', 'costume im', 'costume im sure', 'doing costume']\n",
      "Topic:655  ['atm just', 'atm just gre', 'carolina', 'carolina texas', 'carolina texas atm', 'gre']\n",
      "Topic:656  ['food social', 'food social support', 'friends school', 'friends school things', 'like food', 'like food social']\n",
      "Topic:657  ['40mph', '40mph hey', '40mph hey ho', '63miles', '63miles good', '63miles good thing']\n",
      "Topic:658  ['crisis', 'cool iknow', 'cool iknow wellda', 'crisis spk', 'crisis spk l8r', 'das']\n",
      "Topic:659  ['wave', '44345', 'alto18', 'alto18 uk', 'alto18 uk wave', 'asp 44345']\n",
      "Topic:660  ['crisis', 'cool iknow', 'cool iknow wellda', 'crisis spk', 'crisis spk l8r', 'das']\n",
      "Topic:661  ['afford', 'afford tells', 'afford tells apparently', 'apparently happens', 'apparently happens somebody', 'fwiw']\n",
      "Topic:662  ['40mph', '40mph hey', '40mph hey ho', '63miles', '63miles good', '63miles good thing']\n",
      "Topic:663  ['amp answered', 'amp answered fine', 'answer asked', 'answer asked boy', 'answered fine', 'answered fine gudnite']\n",
      "Topic:664  ['amp answered', 'amp answered fine', 'answer asked', 'answer asked boy', 'answered fine', 'answered fine gudnite']\n",
      "Topic:665  ['98321561', '98321561 familiar', '98321561 familiar i_', 'carry meh', 'carry meh heavy', 'da num']\n",
      "Topic:666  ['bac', 'bac train', 'black trainners', 'black trainners save', 'carryin', 'carryin bac']\n",
      "Topic:667  ['bored wat', 'bored wat cos', 'car dat', 'car dat bored', 'cos wait', 'cos wait outside']\n",
      "Topic:668  ['bac', 'bac train', 'black trainners', 'black trainners save', 'carryin', 'carryin bac']\n",
      "Topic:669  ['sept', '24th', '24th sept', '24th sept talk', 'deliver 24th', 'deliver 24th sept']\n",
      "Topic:670  ['arms minutes', 'bandages', 'bandages shit', 'bandages shit arms', 'despite fact', 'despite fact gets']\n",
      "Topic:671  ['langport', '9pm sucks', '9pm sucks ill', 'bed 9pm', 'bed 9pm sucks', 'doing langport']\n",
      "Topic:672  ['lor goin', 'b4 liao', 'b4 liao nice', 'deer', 'deer huh', 'deer huh watch']\n",
      "Topic:673  ['dunno leh', 'bag goigng', 'bag goigng dat', 'dat small', 'dat small jus', 'dunno leh mayb']\n",
      "Topic:674  ['lor goin', 'b4 liao', 'b4 liao nice', 'deer', 'deer huh', 'deer huh watch']\n",
      "Topic:675  ['arrange pick', 'arrange pick lamp', 'care hello', 'care hello caroline', 'cos need arrange', 'doing moving']\n",
      "Topic:676  ['dunno leh', 'bag goigng', 'bag goigng dat', 'dat small', 'dat small jus', 'dunno leh mayb']\n",
      "Topic:677  ['langport', '9pm sucks', '9pm sucks ill', 'bed 9pm', 'bed 9pm sucks', 'doing langport']\n",
      "Topic:678  ['ache speak', 'ache speak miss', 'coming queen', 'coming queen hmmm', 'desparately', 'does keeps']\n",
      "Topic:679  ['forgt', 'forgt previous', 'forgt previous problem', 'friends help', 'friends help problems', 'help problems']\n",
      "Topic:680  ['12 30 max', '30 max', '30 max easy', 'going yeovil', 'going yeovil motor', 'home 12']\n",
      "Topic:681  ['ate chicken', 'ate chicken rice', 'chicken rice', 'earlier lor', 'earlier lor ate', 'eat earlier']\n",
      "Topic:682  ['beautiful tomorrow comes', 'beautiful tomorrow don', 'comes comes', 'comes comes today', 'comes today', 'comes today hunt']\n",
      "Topic:683  ['lor goin', 'b4 liao', 'b4 liao nice', 'deer', 'deer huh', 'deer huh watch']\n",
      "Topic:684  ['12 30 max', '30 max', '30 max easy', 'going yeovil', 'going yeovil motor', 'home 12']\n",
      "Topic:685  ['ache speak', 'ache speak miss', 'coming queen', 'coming queen hmmm', 'desparately', 'does keeps']\n",
      "Topic:686  ['alright did', 'alright did job', 'baby alright', 'baby alright did', 'did job hope', 'fine send']\n",
      "Topic:687  ['alright did', 'alright did job', 'baby alright', 'baby alright did', 'did job hope', 'fine send']\n",
      "Topic:688  ['ur rite', '2day goodmate', '2day goodmate think', 'asusual', 'asusual cheered', 'asusual cheered love']\n",
      "Topic:689  ['dom', 'dom told', 'dom told yesterday', 'got touch', 'got touch night', 'know old dom']\n",
      "Topic:690  ['beautiful tomorrow comes', 'beautiful tomorrow don', 'comes comes', 'comes comes today', 'comes today', 'comes today hunt']\n",
      "Topic:691  ['checkboxes', 'checkboxes know', 'checkboxes know hardcore', 'custom checkboxes', 'custom checkboxes know', 'days week']\n",
      "Topic:692  ['come took', 'come took long', 'got words', 'got words ym', 'happy sad left', 'leave zaher']\n",
      "Topic:693  ['april audition', 'april audition season', 'chile', 'chile lt', 'chile lt decimal', 'come time subletting']\n",
      "Topic:694  ['appt week', 'appt week thinks', 'check worried', 'check worried didn', 'didn listen', 'die told']\n",
      "Topic:695  ['april audition', 'april audition season', 'chile', 'chile lt', 'chile lt decimal', 'come time subletting']\n",
      "Topic:696  ['appt week', 'appt week thinks', 'check worried', 'check worried didn', 'didn listen', 'die told']\n",
      "Topic:697  ['2moro needs', '2moro needs blokes', 'blokes', 'buddy', 'buddy ya', 'buddy ya 2moro']\n",
      "Topic:698  ['boytoy home', 'boytoy home constant', 'come home kiss', 'constant thought', 'constant thought love', 'having nice visit']\n",
      "Topic:699  ['2000 bonus', '2000 bonus caller', '09064019788 box42wr29c', '09064019788 box42wr29c 150ppm', '03 final try', 'box42wr29c']\n",
      "Topic:700  ['08712405022', '08712405022 1x150p wk', '08712405022 1x150p', '80608', '80608 www', '80608 www movietrivia']\n",
      "Topic:701  ['80488 cs', '80488 cs www', 'account credited', 'account credited 500', 'activate just', 'activate just txt']\n",
      "Topic:702  ['depressed', 'bored depressed', 'bored depressed sittin', 'depressed sittin', 'depressed sittin waitin', 'drops']\n",
      "Topic:703  ['appear', 'appear forget', 'appear forget paths', 'days waiting', 'days waiting ideal', 'forget paths']\n",
      "Topic:704  ['avent killed', 'cutie', 'cutie goes', 'cutie goes wales', 'goes wales', 'goes wales kinda']\n",
      "Topic:705  ['faggy', 'acknowledgement', 'acknowledgement astoundingly', 'acknowledgement astoundingly tactless', 'astoundingly', 'astoundingly tactless']\n",
      "Topic:706  ['depressed', 'bored depressed', 'bored depressed sittin', 'depressed sittin', 'depressed sittin waitin', 'drops']\n",
      "Topic:707  ['ah den', 'ah den plus', 'den orchard', 'den orchard lor', 'den plus', 'den plus lor']\n",
      "Topic:708  ['analysis', 'analysis starts', 'analysis starts monday', 'data', 'data analysis', 'data analysis starts']\n",
      "Topic:709  ['congrats great', 'congrats great wanted', 'cos make', 'cos make relax', 'great wanted', 'great wanted tell']\n",
      "Topic:710  ['depressed', 'bored depressed', 'bored depressed sittin', 'depressed sittin', 'depressed sittin waitin', 'drops']\n",
      "Topic:711  ['faggy', 'acknowledgement', 'acknowledgement astoundingly', 'acknowledgement astoundingly tactless', 'astoundingly', 'astoundingly tactless']\n",
      "Topic:712  ['ur goin', 'bother ur', 'bother ur goin', 'goin mite', 'ilol', 'ilol let']\n",
      "Topic:713  ['ah den', 'ah den plus', 'den orchard', 'den orchard lor', 'den plus', 'den plus lor']\n",
      "Topic:714  ['appy', 'appy fizz', 'appy fizz contains', 'cancer causing', 'cancer causing age', 'causing age']\n",
      "Topic:715  ['bit party', 'bit party doesn', 'charge probably', 'charge probably come', 'come served', 'cover charge']\n",
      "Topic:716  ['collect content', 'content paid', 'content paid goto', 'doit', 'doit mymoby', 'doit mymoby tv']\n",
      "Topic:717  ['ur goin', 'bother ur', 'bother ur goin', 'goin mite', 'ilol', 'ilol let']\n",
      "Topic:718  ['09061221066 fromm', '28 days', 'camera 09061221066', 'camera 09061221066 fromm', 'camera awarded', 'camera awarded sipix']\n",
      "Topic:719  ['congrats student', 'congrats student enna', 'enna', 'enna kalaachutaarama', 'enna kalaachutaarama prof', 'kalaachutaarama']\n",
      "Topic:720  ['listen just', 'listen just hear', 'day listen', 'day listen just', 'desparate recorded', 'desparate recorded message']\n",
      "Topic:721  ['kano trishul', 'kano trishul theatre', 'mandara', 'mandara movie', 'mandara movie kano', 'movie kano']\n",
      "Topic:722  ['coins paypal', 'coins paypal voila', 'coins sell', 'coins sell coins', 'life pockets', 'll tons']\n",
      "Topic:723  ['dont mean', 'dont mean intrude', 'hi happy new', 'intrude', 'intrude pls', 'intrude pls let']\n",
      "Topic:724  ['prolly', 'dead slow', 'dead slow ll', 'ill try send', 'll prolly tomorrow', 'lol ok ill']\n",
      "Topic:725  ['2day text', '2day text keyword', 'amazing xxx', 'amazing xxx picsfree1', 'enjoy vid', 'enjoy vid 2day']\n",
      "Topic:726  ['anythingtomorrow', 'anythingtomorrow myparents', 'anythingtomorrow myparents aretaking', 'aretaking', 'aretaking outfor', 'aretaking outfor meal']\n",
      "Topic:727  ['collect content', 'content paid', 'content paid goto', 'doit', 'doit mymoby', 'doit mymoby tv']\n",
      "Topic:728  ['know later', 'cat house', 'cat house let', 'dog cat', 'dog cat house', 'exposed']\n",
      "Topic:729  ['appy', 'appy fizz', 'appy fizz contains', 'cancer causing', 'cancer causing age', 'causing age']\n",
      "Topic:730  ['amp express', 'amp express love', 'express love', 'express love hurt', 'hard true', 'hard true amp']\n",
      "Topic:731  ['calls friends', 'calls friends world', 'friends world', 'friends world ge', 'life love people', 'life means']\n",
      "Topic:732  ['bloomberg', '447797706009', '447797706009 wait', '447797706009 wait apply', 'apply future', 'apply future http']\n",
      "Topic:733  ['congrats student', 'congrats student enna', 'enna', 'enna kalaachutaarama', 'enna kalaachutaarama prof', 'kalaachutaarama']\n",
      "Topic:734  ['boytoy does miss', 'does miss', 'finding', 'finding job', 'finding job lazy', 'getting net']\n",
      "Topic:735  ['congrats student', 'congrats student enna', 'enna', 'enna kalaachutaarama', 'enna kalaachutaarama prof', 'kalaachutaarama']\n",
      "Topic:736  ['coins paypal', 'coins paypal voila', 'coins sell', 'coins sell coins', 'life pockets', 'll tons']\n",
      "Topic:737  ['amp express', 'amp express love', 'express love', 'express love hurt', 'hard true', 'hard true amp']\n",
      "Topic:738  ['aka pap', 'aka pap smear', 'big deal regular', 'cancer moms', 'cancer moms making', 'checkup']\n",
      "Topic:739  ['aka pap', 'aka pap smear', 'big deal regular', 'cancer moms', 'cancer moms making', 'checkup']\n",
      "Topic:740  ['dat feelin', 'dat feelin pete', 'em nuther', 'em nuther place', 'feelin pete', 'feelin pete wuld']\n",
      "Topic:741  ['bein good', 'bein good u_', 'bout bein', 'bout bein good', 'don know thing', 'everyso']\n",
      "Topic:742  ['bear', 'bear defeat', 'bear defeat losing', 'courage', 'courage earth', 'courage earth bear']\n",
      "Topic:743  ['going nap', 'going nap yup', 'liao picking', 'liao picking ard', 'mean wif', 'mean wif sleeping']\n",
      "Topic:744  ['beneath', 'beneath pale', 'beneath pale moon', 'come true goodnite', 'goodnite amp', 'goodnite amp sweet']\n",
      "Topic:745  ['im gonna', 'come tb', 'come tb love', 'darlin im helens', 'fone im', 'fone im gonna']\n",
      "Topic:746  ['bday ill', 'bday ill prob', 'decide wot', 'decide wot bday', 'did decide', 'did decide wot']\n",
      "Topic:747  ['dat feelin', 'dat feelin pete', 'em nuther', 'em nuther place', 'feelin pete', 'feelin pete wuld']\n",
      "Topic:748  ['airtel', 'airtel broadband', 'airtel broadband processed', 'application airtel', 'application airtel broadband', 'broadband']\n",
      "Topic:749  ['2mrw luv franxx', 'babes think', 'babes think got', 'bring 2mrw', 'bring 2mrw luv', 'brolly']\n",
      "Topic:750  ['ask i_ smth', 'card da', 'card da present', 'da present', 'da present lei', 'forgot ask']\n",
      "Topic:751  ['come i_', 'come i_ lucky', 'earlier later', 'earlier later pple', 'finish i_', 'good later']\n",
      "Topic:752  ['airtel', 'airtel broadband', 'airtel broadband processed', 'application airtel', 'application airtel broadband', 'broadband']\n",
      "Topic:753  ['150p sms', 'age opt', 'age opt enjoy', 'community 150p', 'community 150p sms', 'company just']\n",
      "Topic:754  ['send teasing', 'send teasing kiss', 'teasing kiss sea', 'goes new', 'goes new job', 'hello lover']\n",
      "Topic:755  ['cross cut', 'cross cut road', 'cut road', 'cut road right', 'gandhipuram', 'gandhipuram walk']\n",
      "Topic:756  ['2mrw luv franxx', 'babes think', 'babes think got', 'bring 2mrw', 'bring 2mrw luv', 'brolly']\n",
      "Topic:757  ['complaint', 'abusers', 'abusers lives', 'complaint drug', 'complaint drug abusers', 'derp']\n",
      "Topic:758  ['send txt stop', '22 days', '22 days kick', 'daily removed', 'daily removed send', 'date latest']\n",
      "Topic:759  ['doing hair', 'appt shame', 'appt shame missed', 'girls night', 'girls night quizzes', 'missed girls']\n",
      "Topic:760  ['im gonna', 'come tb', 'come tb love', 'darlin im helens', 'fone im', 'fone im gonna']\n",
      "Topic:761  ['complaint', 'abusers', 'abusers lives', 'complaint drug', 'complaint drug abusers', 'derp']\n",
      "Topic:762  ['doing hair', 'appt shame', 'appt shame missed', 'girls night', 'girls night quizzes', 'missed girls']\n",
      "Topic:763  ['txt82228', 'com questions', 'com questions info', 'games www', 'games www txt82228', 'info txt82228']\n",
      "Topic:764  ['gt worth', 'lt gt worth', 'smoke lt', 'smoke lt gt', 'won til', 'ahead smoke']\n",
      "Topic:765  ['22 days', '22 days kick', 'daily removed', 'daily removed send', 'date latest', 'date latest news']\n",
      "Topic:766  ['amp reach', 'amp reach clock', 'area urgnt', 'area urgnt vasai', 'clock plz', 'coveragd']\n",
      "Topic:767  ['woot', 'celebrate stuffing', 'celebrate stuffing face', 'doc visit', 'doc visit week', 'gonna celebrate']\n",
      "Topic:768  ['125', '125 gift', '125 gift guaranteed', '4u congratulations', '4u congratulations ur', 'cds 4u']\n",
      "Topic:769  ['txt82228', 'com questions', 'com questions info', 'games www', 'games www txt82228', 'info txt82228']\n",
      "Topic:770  ['09057039994', 'anytime calling', 'anytime calling customer', 'calling customer', 'calling customer services', 'charged gbp']\n",
      "Topic:771  ['abt muz', 'abt muz discuss', 'ard ya', 'ard ya free', 'discuss liao', 'free abt']\n",
      "Topic:772  ['10 06 03', '09066368753 asap', '10 06', '09066368753 asap box', '97n7qp', '97n7qp 150ppm']\n",
      "Topic:773  ['actually lt', 'actually lt gt', 'cos fees', 'cos fees university', 'fees university', 'fees university florida']\n",
      "Topic:774  ['alright hows', 'alright hows school', 'cheers oh', 'cheers oh yeah', 'coming thursday', 'coming thursday yay']\n",
      "Topic:775  ['i_ dun', 'dat i_', 'dat i_ juz', 'dun haf passport', 'email account', 'haf passport']\n",
      "Topic:776  ['belt', 'belt know', 'belt know cribbs', 'cribbs', 'hat', 'hat belt']\n",
      "Topic:777  ['ask cooked', 'big butt', 'big butt hang', 'butt hang', 'butt hang caller', 'caller food']\n",
      "Topic:778  ['accounts', 'accounts executive', 'accounts executive ur', 'cum accounts', 'cum accounts executive', 'customer service cum']\n",
      "Topic:779  ['amp reach', 'amp reach clock', 'area urgnt', 'area urgnt vasai', 'clock plz', 'coveragd']\n",
      "Topic:780  ['good morning love', 'say good', 'beautiful morning library', 'believe said', 'believe said things', 'library later']\n",
      "Topic:781  ['abt muz', 'abt muz discuss', 'ard ya', 'ard ya free', 'discuss liao', 'free abt']\n",
      "Topic:782  ['belt', 'belt know', 'belt know cribbs', 'cribbs', 'hat', 'hat belt']\n",
      "Topic:783  ['say good', 'beautiful morning library', 'believe said', 'believe said things', 'library later', 'love beautiful']\n",
      "Topic:784  ['flame', 'big rawring', 'big rawring flame', 'flame big', 'flame big rawring', 'flame xoxo']\n",
      "Topic:785  ['bottle', 'bring bottle', 'amused just', 'amused just joking', 'bottle red', 'bottle red white']\n",
      "Topic:786  ['flame', 'big rawring', 'big rawring flame', 'flame big', 'flame big rawring', 'flame xoxo']\n",
      "Topic:787  ['alright hows', 'alright hows school', 'cheers oh', 'cheers oh yeah', 'coming thursday', 'coming thursday yay']\n",
      "Topic:788  ['dinner lor haha', 'goin got', 'goin got somethin', 'got somethin', 'got somethin unless', 'haha wonder']\n",
      "Topic:789  ['couldn sleep', 'couldn sleep nite', 'favourite person', 'favourite person today', 'hard couldn', 'hard couldn sleep']\n",
      "Topic:790  ['foregate', 'foregate street', 'foregate street shrub', 'fun night', 'hill fun', 'hill fun night']\n",
      "Topic:791  ['abt muz', 'abt muz discuss', 'ard ya', 'ard ya free', 'discuss liao', 'free abt']\n",
      "Topic:792  ['barkleys', 'barkleys crazy', 'barkleys crazy ringtone', 'crazy ringtone', 'crazy ringtone totally', 'free just reply']\n",
      "Topic:793  ['1013', '1013 ig11', '150p netcollex', '150p netcollex po', 'billed 150p netcollex', 'box 1013']\n",
      "Topic:794  ['i_ dun', 'dat i_', 'dat i_ juz', 'dun haf passport', 'email account', 'haf passport']\n",
      "Topic:795  ['day sitting', 'day sitting staring', 'eat meds', 'eat meds ruining', 'eaten day', 'eaten day sitting']\n",
      "Topic:796  ['best cricketer', 'best cricketer sachin', 'bhaji', 'bhaji told', 'bhaji told kallis', 'cricketer']\n",
      "Topic:797  ['flame', 'big rawring', 'big rawring flame', 'flame big', 'flame big rawring', 'flame xoxo']\n",
      "Topic:798  ['edu', 'argentina', 'argentina sad', 'argentina sad secretary', 'edu great', 'edu great time']\n",
      "Topic:799  ['belong', 'cause gyno', 'cause gyno shoving', 'don belong', 'gt minutes cause', 'gyno']\n",
      "Topic:800  ['best cricketer', 'best cricketer sachin', 'bhaji', 'bhaji told', 'bhaji told kallis', 'cricketer']\n",
      "Topic:801  ['day sitting', 'day sitting staring', 'eat meds', 'eat meds ruining', 'eaten day', 'eaten day sitting']\n",
      "Topic:802  ['barkleys', 'barkleys crazy', 'barkleys crazy ringtone', 'crazy ringtone', 'crazy ringtone totally', 'free just reply']\n",
      "Topic:803  ['dinner lor haha', 'goin got', 'goin got somethin', 'got somethin', 'got somethin unless', 'haha wonder']\n",
      "Topic:804  ['edu', 'olowoyey', 'olowoyey usc', 'olowoyey usc edu', 'argentina', 'argentina sad']\n",
      "Topic:805  ['bristol road', 'bristol road touch', 'flight good', 'flight good week', 'great ll guild', 'guild']\n",
      "Topic:806  ['2morow wil', '2morow wil cme', 'cme got', 'cme got dear', 'cme want', 'cme want hos']\n",
      "Topic:807  ['decided won', 'decided won let', 'elliot', 'elliot kissing', 'elliot kissing damn', 'engalnd']\n",
      "Topic:808  ['american pie', 'american pie like', 'eek lot', 'eek lot time', 'especially american', 'especially american pie']\n",
      "Topic:809  ['dinner lor haha', 'goin got', 'goin got somethin', 'got somethin', 'got somethin unless', 'haha wonder']\n",
      "Topic:810  ['26', '15 26', '15 26 sorry', '26 sorry', '26 sorry just', 'cafe sit']\n",
      "Topic:811  ['700 900', '700 900 nights', '900 nights', '900 nights excellent', 'breakfast', 'breakfast hamper']\n",
      "Topic:812  ['ahmad wait', 'ahmad wait year', 'begin second', 'begin second takes', 'closer happy', 'closer happy new']\n",
      "Topic:813  ['beggar', 'cos shakara', 'cos shakara beggar', 'labor', 'labor look', 'labor look month']\n",
      "Topic:814  ['yogasana oso', '26', '15 26', '15 26 sorry', '26 sorry', '26 sorry just']\n",
      "Topic:815  ['american pie', 'american pie like', 'eek lot', 'eek lot time', 'especially american', 'especially american pie']\n",
      "Topic:816  ['able friday', 'able friday hope', 'alternative', 'alternative hope', 'alternative hope yr', 'friday hope']\n",
      "Topic:817  ['bristol road', 'bristol road touch', 'flight good', 'flight good week', 'great ll guild', 'guild']\n",
      "Topic:818  ['send tm', 'send tm remind', 'boytoy geeee missing', 'geeee missing', 'geeee missing today', 'like send']\n",
      "Topic:819  ['3750', '3750 pounds', '3750 pounds accident', 'entitled 3750', 'entitled 3750 pounds', 'free reply yes']\n",
      "Topic:820  ['decided won', 'decided won let', 'elliot', 'elliot kissing', 'elliot kissing damn', 'engalnd']\n",
      "Topic:821  ['08701213186', '16 need', '16 need help', '88888 apply', '88888 apply 16', 'apply 16']\n",
      "Topic:822  ['ahmad wait', 'ahmad wait year', 'begin second', 'begin second takes', 'closer happy', 'closer happy new']\n",
      "Topic:823  ['16 need', '16 need help', '88888 apply', '88888 apply 16', 'apply 16', 'apply 16 need']\n",
      "Topic:824  ['boss nanny', 'boss nanny raise', 'game okay', 'game okay boss', 'mr sheffield', 'mr sheffield wanna']\n",
      "Topic:825  ['blastin', 'blastin tsunamis', 'blastin tsunamis occur', 'indian ocean', 'mind blastin', 'mind blastin tsunamis']\n",
      "Topic:826  ['aft ur lect', 'ur lect', 'cinema wot', 'dancin', 'dancin eatin', 'dancin eatin cinema']\n",
      "Topic:827  ['birthday wrking', 'birthday wrking nxt', 'did good', 'did good birthday', 'good birthday', 'good birthday wrking']\n",
      "Topic:828  ['wa', 'congratulations ore', 'congratulations ore mo', 'enjoy wish', 'enjoy wish happy', 'fro']\n",
      "Topic:829  ['200 prize', '09066361921', '200 prize just', '2nd attempt contract', 'attempt contract', 'attempt contract won']\n",
      "Topic:830  ['want know', 'rhythm', 'body learn', 'body learn use', 'establish', 'establish rhythm']\n",
      "Topic:831  ['dark', 'dark liao', 'dark liao raining', 'excuse run', 'excuse run rite', 'got excuse']\n",
      "Topic:832  ['booked half', 'booked half let', 'day good', 'day good walk', 'good walk', 'good walk table']\n",
      "Topic:833  ['birthday wrking', 'birthday wrking nxt', 'did good', 'did good birthday', 'good birthday', 'good birthday wrking']\n",
      "Topic:834  ['apparently drive', 'apparently drive inch', 'closed tomorrow', 'closed tomorrow apparently', 'dramatic', 'dramatic schools']\n",
      "Topic:835  ['wa', 'congratulations ore', 'congratulations ore mo', 'enjoy wish', 'enjoy wish happy', 'fro']\n",
      "Topic:836  ['birthday wrking', 'birthday wrking nxt', 'did good', 'did good birthday', 'good birthday', 'good birthday wrking']\n",
      "Topic:837  ['barely doing', 'barely doing stay', 'constantly isn', 'constantly isn helping', 'doing stay', 'doing stay sane']\n",
      "Topic:838  ['buy drop', 'buy drop place', 'cool tyler', 'cool tyler gonna', 'drop place', 'drop place later']\n",
      "Topic:839  ['200 prize', '200 prize just', '2nd attempt contract', 'attempt contract', 'attempt contract won', 'cash 200']\n",
      "Topic:840  ['century', 'century cm', 'century cm frwd', 'cm frwd', 'cm frwd thnk', 'dnt live']\n",
      "Topic:841  ['begins', 'believe forgot', 'believe forgot surname', 'bloody hell', 'bloody hell believe', 'clue']\n",
      "Topic:842  ['700 900', '700 900 nights', '900 nights', '900 nights excellent', 'breakfast', 'breakfast hamper']\n",
      "Topic:843  ['pouts', 'pouts stomps', 'pouts stomps feet', 'stomps', 'stomps feet', 'feet pouts']\n",
      "Topic:844  ['able half', 'able half track', 'breathing', 'breathing neck', 'breathing neck bud', 'bud able']\n",
      "Topic:845  ['earlier tot', 'earlier tot parkin', 'going sch earlier', 'got lesson lei', 'huh got lesson', 'lei thinkin']\n",
      "Topic:846  ['yogasana oso', 'dun thk ll', 'em lessons', 'em lessons den', 'hmmm jazz', 'hmmm jazz yogasana']\n",
      "Topic:847  ['aww nearly', 'aww nearly dead', 'dead jez', 'dead jez iscoming', 'iscoming todo', 'iscoming todo workand']\n",
      "Topic:848  ['want know', 'rhythm', 'body learn', 'body learn use', 'establish', 'establish rhythm']\n",
      "Topic:849  ['emily', 'emily rose', 'emily rose sleep', 'exorcism', 'exorcism emily', 'exorcism emily rose']\n",
      "Topic:850  ['sufficient', '81303 delivered', '81303 delivered sufficient', 'credit receive', 'credit receive service', 'delivered sufficient']\n",
      "Topic:851  ['babysit', 'babysit xx', 'darlin supose', 'darlin supose ok', 'film stuff', 'film stuff mate']\n",
      "Topic:852  ['buy drop', 'buy drop place', 'cool tyler', 'cool tyler gonna', 'drop place', 'drop place later']\n",
      "Topic:853  ['700 900', '700 900 nights', '900 nights', '900 nights excellent', 'breakfast', 'breakfast hamper']\n",
      "Topic:854  ['bunch', 'bunch lotto', 'bunch lotto tickets', 'buy bunch', 'buy bunch lotto', 'gt deal']\n",
      "Topic:855  ['blastin', 'blastin tsunamis', 'blastin tsunamis occur', 'indian ocean', 'mind blastin', 'mind blastin tsunamis']\n",
      "Topic:856  ['line rental free', 'rental free', 'bored da', 'bored da lecturer', 'da lecturer', 'da lecturer repeating']\n",
      "Topic:857  ['bro creative', 'bro creative neva', 'check review', 'check review online', 'creative', 'creative neva']\n",
      "Topic:858  ['lists make list', 'chat click', 'chat click friend', 'click friend', 'click friend lists', 'easy pie']\n",
      "Topic:859  ['affair cheers', 'befor meet', 'befor meet greet', 'greet kind', 'greet kind affair', 'im snowboarding']\n",
      "Topic:860  ['suppose', 'couple different', 'couple different people', 'didn set', 'didn set dates', 'different people']\n",
      "Topic:861  ['poop', '2morrow care', '2morrow care soon', 'bday drinks', 'bday drinks nite', 'care soon xxx']\n",
      "Topic:862  ['slots', 'driving lessons', 'got tuition', 'got tuition haha', 'haha looking', 'haha looking slots']\n",
      "Topic:863  ['cancelled', 'cat wanted', 'cat wanted come', 'come worry', 'come worry cold', 'got lousy']\n",
      "Topic:864  ['late unemployed', 'sleep late', 'sleep late unemployed', 'unemployed', 'actor work', 'actor work work']\n",
      "Topic:865  ['blastin', 'blastin tsunamis', 'blastin tsunamis occur', 'indian ocean', 'mind blastin', 'mind blastin tsunamis']\n",
      "Topic:866  ['begging come', 'begging come smoke', 'don tell friend', 'friend sure', 'friend sure want', 'hours begging']\n",
      "Topic:867  ['late unemployed', 'sleep late', 'sleep late unemployed', 'unemployed', 'actor work', 'actor work work']\n",
      "Topic:868  ['ask cut', 'ask cut short', 'darren ask', 'darren ask cut', 'face look', 'face look longer']\n",
      "Topic:869  ['okay lor', 'def wont', 'def wont let', 'did say terms', 'haha did', 'haha did say']\n",
      "Topic:870  ['ass srt', 'ass srt thnk', 'dont know supports', 'know supports', 'know supports ass', 'play usb']\n",
      "Topic:871  ['aww nearly', 'aww nearly dead', 'dead jez', 'dead jez iscoming', 'iscoming todo', 'iscoming todo workand']\n",
      "Topic:872  ['ask cut', 'ask cut short', 'darren ask', 'darren ask cut', 'face look', 'face look longer']\n",
      "Topic:873  ['dont rush', 'dont rush home', 'eating nachos', 'eating nachos let', 'hello sort', 'hello sort town']\n",
      "Topic:874  ['tell sura', 'download movies', 'send message download', 'battery hont', 'battery hont pls', 'download movies thanks']\n",
      "Topic:875  ['babysit', 'babysit xx', 'darlin supose', 'darlin supose ok', 'film stuff', 'film stuff mate']\n",
      "Topic:876  ['emily', 'emily rose', 'emily rose sleep', 'exorcism', 'exorcism emily', 'exorcism emily rose']\n",
      "Topic:877  ['bathing dog', 'going rain soon', 'like going', 'like going rain', 'rain soon', 'going rain']\n",
      "Topic:878  ['bettr', 'bettr directly', 'bettr directly bsnl', 'bsnl', 'bsnl offc', 'bsnl offc nd']\n",
      "Topic:879  ['dont rush', 'dont rush home', 'eating nachos', 'eating nachos let', 'hello sort', 'hello sort town']\n",
      "Topic:880  ['coincidence', 'connection think', 'connection think coincidence', 'dating started', 'dating started sent', 'radio week']\n",
      "Topic:881  ['amp mobile', 'amp mobile problem', 'asa', 'asa ll', 'asa ll free', 'cann voice']\n",
      "Topic:882  ['address sandiago', 'address sandiago parantella', 'computer shipped', 'computer shipped address', 'lane wtf', 'lane wtf poop']\n",
      "Topic:883  ['buy hanger', 'buy hanger lor', 'comes mind', 'comes mind ii', 'hanger', 'hanger lor']\n",
      "Topic:884  ['called coming', 'called coming makes', 'coming makes', 'coming makes feel', 'diwali called', 'diwali called coming']\n",
      "Topic:885  ['tell sura', 'battery hont', 'battery hont pls', 'download movies thanks', 'expecting battery', 'expecting battery hont']\n",
      "Topic:886  ['2nite want', '2nite want real', 'direct ur mobile', 'laid 2nite', 'laid 2nite want', 'mobile join uk']\n",
      "Topic:887  ['barely stand wonder', 'day goes love', 'goes love', 'goes love think', 'know barely', 'know barely stand']\n",
      "Topic:888  ['ammae', 'ammae life', 'ammae life takes', 'hold steering', 'life takes', 'life takes lot']\n",
      "Topic:889  ['coming outstanding', 'coming outstanding invoices', 'did months', 'did months ago', 'invoices', 'invoices work']\n",
      "Topic:890  ['afraid dark', 'afraid dark teenager', 'child afraid', 'child afraid dark', 'come takes', 'come takes little']\n",
      "Topic:891  ['ask workin', 'ask workin lor', 'days tues', 'days tues wed', 'free days', 'free days tues']\n",
      "Topic:892  ['definitely need', 'definitely need module', 'dis sem', 'dis sem izzit', 'humanities', 'humanities dis']\n",
      "Topic:893  ['came truly', 'came truly special', 'enjoy gbp', 'enjoy gbp sms', 'forget enjoy', 'forget enjoy gbp']\n",
      "Topic:894  ['begging come', 'begging come smoke', 'don tell friend', 'friend sure', 'friend sure want', 'hours begging']\n",
      "Topic:895  ['definitely need', 'definitely need module', 'dis sem', 'dis sem izzit', 'humanities', 'humanities dis']\n",
      "Topic:896  ['anymore like', 'anymore like said', 'ditto', 'ditto won', 'ditto won worry', 'like said']\n",
      "Topic:897  ['sleepin', 'aiyo lesson', 'aiyo lesson early', 'confirm lor', 'den confirm', 'den confirm lor']\n",
      "Topic:898  ['called coming', 'called coming makes', 'coming makes', 'coming makes feel', 'diwali called', 'diwali called coming']\n",
      "Topic:899  ['coincidence', 'connection think', 'connection think coincidence', 'dating started', 'dating started sent', 'radio week']\n",
      "Topic:900  ['09058091870 revealed', '09058091870', '3uz 150p', 'asked contact', 'asked contact shy', 'contact shy']\n",
      "Topic:901  ['drinks nite', 'drinks nite 2morrow', '2morrow care', '2morrow care soon', 'bday drinks', 'bday drinks nite']\n",
      "Topic:902  ['flaky friend', 'flaky friend interested', 'friend interested', 'friend interested picking', 'gt worth tonight', 'guy kinda']\n",
      "Topic:903  ['sleepin', 'aiyo lesson', 'aiyo lesson early', 'confirm lor', 'den confirm', 'den confirm lor']\n",
      "Topic:904  ['bian', 'bian watch', 'bian watch da', 'da glass', 'da glass exhibition', 'exhibition']\n",
      "Topic:905  ['ask workin', 'ask workin lor', 'days tues', 'days tues wed', 'free days', 'free days tues']\n",
      "Topic:906  ['abt leona', 'abt leona oops', 'ben going msg', 'decide lor', 'decide lor abt', 'dunno lei i_']\n",
      "Topic:907  ['530 afternoon', '530 afternoon finish', 'afternoon finish', 'afternoon finish work', 'buy later', 'buy later check']\n",
      "Topic:908  ['dad said', 'bring lunch', 'bring lunch yup', 'coming home bring', 'dad said coming', 'dunno dad']\n",
      "Topic:909  ['actual', 'just talk', 'actual guy', 'actual guy wants', 'friend friend', 'friend friend stuff']\n",
      "Topic:910  ['asda', 'asda counts', 'asda counts celebration', 'celebration thats', 'celebration thats doing', 'counts']\n",
      "Topic:911  ['cbe really', 'cbe really good', 'city shaping', 'city shaping good', 'good nowadays', 'good nowadays lot']\n",
      "Topic:912  ['amp kills', 'amp kills don', 'away frm', 'away frm walk', 'care stop', 'don care stop']\n",
      "Topic:913  ['shd haf', 'address carlos', 'address carlos wanted', 'answering phone', 'blake address carlos', 'carlos wanted']\n",
      "Topic:914  ['actually send pic', 'comb', 'comb hair', 'comb hair dryer', 'dryer', 'hair dryer']\n",
      "Topic:915  ['cos sis got', 'dad went dunno', 'dunno eat', 'dunno eat sch', 'eat sch', 'eat sch wat']\n",
      "Topic:916  ['send message ll', 'send mobile', 'send mobile mode', 'just send message', 'll reply', 'll send mobile']\n",
      "Topic:917  ['sleepin', 'aiyo lesson', 'aiyo lesson early', 'confirm lor', 'den confirm', 'den confirm lor']\n",
      "Topic:918  ['bring light', 'bring light just', 'bye abiola', 'great trip', 'great trip india', 'india bring']\n",
      "Topic:919  ['afternoon boytoy', 'good afternoon boytoy', 'abstract', 'abstract wake', 'abstract wake miss', 'afternoon boytoy goes']\n",
      "Topic:920  ['cuddle', 'soup', 'aah cuddle', 'aah cuddle lush', 'cuddle lush', 'cuddle lush need']\n",
      "Topic:921  ['mmm', 'better drinks', 'better drinks good', 'better got', 'better got roast', 'drinks good']\n",
      "Topic:922  ['bring light', 'bring light just', 'bye abiola', 'great trip', 'great trip india', 'india bring']\n",
      "Topic:923  ['believe didnt', 'believe didnt know', 'booked cancelled', 'booked cancelled needs', 'cancelled needs', 'cancelled needs sacked']\n",
      "Topic:924  ['got wisdom', 'got wisdom teeth', 'hidden', 'hidden inside', 'hidden inside mayb', 'inside mayb']\n",
      "Topic:925  ['babe love kiss', 'better opportunity', 'better opportunity thought', 'day feeling', 'day feeling better', 'feeling better opportunity']\n",
      "Topic:926  ['baby doll', 'baby doll vehicle', 'barolla', 'brainless', 'brainless baby', 'brainless baby doll']\n",
      "Topic:927  ['send pic right', 'actually send pic', 'comb', 'comb hair', 'comb hair dryer', 'dryer']\n",
      "Topic:928  ['amp kills', 'amp kills don', 'away frm', 'away frm walk', 'care stop', 'don care stop']\n",
      "Topic:929  ['baby doll', 'baby doll vehicle', 'barolla', 'brainless', 'brainless baby', 'brainless baby doll']\n",
      "Topic:930  ['going wildlife', 'going wildlife talk', 'heard job', 'heard job going', 'job going', 'job going wildlife']\n",
      "Topic:931  ['check mails', 'congrats treat', 'congrats treat pending', 'days mail', 'days mail respect', 'home check']\n",
      "Topic:932  ['likely bed', 'likely bed im', 'tomorrow love xxx', 'babe likely', 'babe likely bed', 'bed im']\n",
      "Topic:933  ['cbe really', 'cbe really good', 'city shaping', 'city shaping good', 'good nowadays', 'good nowadays lot']\n",
      "Topic:934  ['credit goin2bed', 'credit goin2bed night', 'goin2bed', 'goin2bed night', 'goin2bed night night', 'havin credit']\n",
      "Topic:935  ['credit goin2bed', 'credit goin2bed night', 'goin2bed', 'goin2bed night', 'goin2bed night night', 'havin credit']\n",
      "Topic:936  ['09071517866', '1000 winner', '09071517866 150ppmpobox10183bhamb64xe', '150ppmpobox10183bhamb64xe', 'attempt contact claim', 'caller prize final']\n",
      "Topic:937  ['going wildlife', 'going wildlife talk', 'heard job', 'heard job going', 'job going', 'job going wildlife']\n",
      "Topic:938  ['bad timing', 'bad timing great', 'fingers trains', 'fingers trains play', 'great fingers', 'great fingers trains']\n",
      "Topic:939  ['want talk', 'barmed', 'barmed want', 'barmed want talk', 'didnaot txt', 'didnaot txt remember']\n",
      "Topic:940  ['diff', 'completely diff', 'completely diff word', 'diff word', 'diff word figure', 'does time']\n",
      "Topic:941  ['brats', 'brats pulling', 'brats pulling hair', 'hair loving', 'houseful', 'houseful screaming']\n",
      "Topic:942  ['figuring', 'figuring shit', 'figuring shit second', 'guess ll work', 'hour supposed', 'hour supposed leave']\n",
      "Topic:943  ['afternoon love job', 'bleak', 'bleak hmmm', 'bleak hmmm happy', 'filled love', 'happy filled']\n",
      "Topic:944  ['avoiding making', 'avoiding making unhappy', 'come slave', 'come slave doing', 'doing going', 'doing going shell']\n",
      "Topic:945  ['diff', 'completely diff', 'completely diff word', 'diff word', 'diff word figure', 'does time']\n",
      "Topic:946  ['free oso', 'free oso gee', 'fren shop', 'gee thgt', 'gee thgt workin', 'oso gee']\n",
      "Topic:947  ['cbe really', 'cbe really good', 'city shaping', 'city shaping good', 'good nowadays', 'good nowadays lot']\n",
      "Topic:948  ['bank tomorrow', 'bank tomorrow tough', 'decisions make', 'decisions make great', 'great people', 'honesty']\n",
      "Topic:949  ['29 inviting', '29 inviting friend', '762 762', '762 762 www', '762 www', '762 www sms']\n",
      "Topic:950  ['babe miss', 'account details money', 'account just', 'account just send', 'details money', 'details money sent']\n",
      "Topic:951  ['da wanna', 'administrator network', 'administrator network administrator', 'change field', 'change field quickly', 'da wanna administrator']\n",
      "Topic:952  ['haha yup', 'haha yup hopefully', 'hip hop orchard', 'hop orchard', 'hop orchard weigh', 'hopefully lose']\n",
      "Topic:953  ['did use', 'did use soc', 'dunno type', 'dunno type word', 'home ii', 'home ii dunno']\n",
      "Topic:954  ['cuddle', 'soup', 'aah cuddle', 'aah cuddle lush', 'cuddle lush', 'cuddle lush need']\n",
      "Topic:955  ['ave good', 'ave good holiday', 'did ave', 'did ave good', 'gimmi', 'gimmi goss']\n",
      "Topic:956  ['cuddle', 'soup', 'aah cuddle', 'aah cuddle lush', 'cuddle lush', 'cuddle lush need']\n",
      "Topic:957  ['blame really', 'blame really just', 'just long', 'just long time', 'just read', 'just read shame']\n",
      "Topic:958  ['barmed', 'barmed want', 'barmed want talk', 'didnaot txt', 'didnaot txt remember', 'jade']\n",
      "Topic:959  ['know details', 'chinese thanks', 'cos tom', 'cos tom fri', 'details fri', 'details fri cos']\n",
      "Topic:960  ['coming hill', 'coming hill monster', 'day things', 'day things going', 'fine busy', 'going fine']\n",
      "Topic:961  ['figuring', 'figuring shit', 'figuring shit second', 'guess ll work', 'hour supposed', 'hour supposed leave']\n",
      "Topic:962  ['going cinema', 'ing tomorrow', 'ing tomorrow tuesday', 'ok problem taxi', 'problem taxi', 'problem taxi ing']\n",
      "Topic:963  ['dun rem', 'silver', 'car thk', 'car thk saw', 'cos dun', 'cos dun rem']\n",
      "Topic:964  ['dull', 'dull easy', 'dull easy guys', 'easy guys', 'easy guys fun', 'fine started']\n",
      "Topic:965  ['come lt decimal', 'come tht', 'come tht time', 'decimal gt pm', 'gt pm vikky', 'il come']\n",
      "Topic:966  ['forgot i_', 'forgot i_ lect', 'got lazy', 'got lazy type', 'i_ lect', 'i_ lect saw']\n",
      "Topic:967  ['aint coming', 'bb longer', 'bb longer comin', 'comin money', 'comin money expecting', 'expecting aint']\n",
      "Topic:968  ['cousin said', 'cousin said walk', 'ha walk', 'ha walk tram', 'market hotel', 'said walk']\n",
      "Topic:969  ['10 30', '10 30 wanna', '30 wanna dosomething', 'arestaurant', 'arestaurant eating', 'arestaurant eating squid']\n",
      "Topic:970  ['140', '140 ard', '140 ard ia', '180', '180 price', '180 price bedrm']\n",
      "Topic:971  ['home lor', 'dun wan stay', 'finish lunch way', 'home lor tot', 'jus finish lunch', 'lor tot']\n",
      "Topic:972  ['bull', 'bull plan', 'bull plan floating', 'care world', 'care world live', 'floating']\n",
      "Topic:973  ['bank tomorrow', 'bank tomorrow tough', 'decisions make', 'decisions make great', 'great people', 'honesty']\n",
      "Topic:974  ['coming hill', 'coming hill monster', 'day things', 'day things going', 'fine busy', 'going fine']\n",
      "Topic:975  ['hi kate', 'babyjontet', 'babyjontet txt', 'babyjontet txt xxx', 'bit bloody', 'bit bloody babyjontet']\n",
      "Topic:976  ['steam', 'batch isn', 'batch isn sweetie', 'email sent batch', 'isn sweetie', 'just opened']\n",
      "Topic:977  ['came home went', 'good time work', 'home went sleep', 'just came', 'just came home', 'knackered just']\n",
      "Topic:978  ['08717111821', 'bad credit', 'bad credit tenants', 'com 08717111821', 'credit tenants', 'credit tenants welcome']\n",
      "Topic:979  ['ctagg', 'ctargg', 'ctargg ctagg', 'cttargg', 'cttargg ctargg', 'cttargg ctargg ctagg']\n",
      "Topic:980  ['ave time', 'ave time oh', 'catch ave', 'catch ave time', 'cer', 'cer nice']\n",
      "Topic:981  ['changes da', 'big cos', 'big cos ve', 'changes da previous', 'changes da report', 'cos ve']\n",
      "Topic:982  ['07046744435 arrange delivery', '07046744435', 'annoncement', 'annoncement new', 'annoncement new years', 'arrange delivery']\n",
      "Topic:983  ['6hrs sleep', '6hrs sleep pain', 'chill 6hrs', 'chill 6hrs sleep', 'emergency', 'emergency unfolds']\n",
      "Topic:984  ['ask darren', 'ask darren pick', 'darren pick', 'darren pick lor', 'haf meet', 'haf meet lect']\n",
      "Topic:985  ['adress', 'beendropping', 'beendropping red', 'beendropping red wine', 'hi missed', 'hi missed mumhas']\n",
      "Topic:986  ['bad people', 'bad people silence', 'good people', 'lot violence', 'lot violence bad', 'people silence']\n",
      "Topic:987  ['bad people', 'bad people silence', 'good people', 'lot violence', 'lot violence bad', 'people silence']\n",
      "Topic:988  ['babe squishy', 'babe squishy mwahs', 'good babe squishy', 'gorgeous man work', 'hey gorgeous', 'hey gorgeous man']\n",
      "Topic:989  ['bad people', 'bad people silence', 'good people', 'lot violence', 'lot violence bad', 'people silence']\n",
      "Topic:990  ['6hrs sleep', '6hrs sleep pain', 'chill 6hrs', 'chill 6hrs sleep', 'emergency', 'emergency unfolds']\n",
      "Topic:991  ['chad', 'chad gymnastics', 'chad gymnastics class', 'christians', 'christians class', 'class wanna']\n",
      "Topic:992  ['actually deleted', 'actually deleted old', 'blogging', 'blogging magicalsongs', 'blogging magicalsongs blogspot', 'blogspot']\n",
      "Topic:993  ['did friends', 'did friends imma', 'flip shit like', 'friends imma', 'friends imma flip', 'goes like']\n",
      "Topic:994  ['argh ok', 'argh ok lool', 'lool', 'massage', 'massage tie', 'massage tie pos']\n",
      "Topic:995  ['lou', 'came ya', 'came ya gailxx', 'cheers lou', 'cheers lou yeah', 'gailxx']\n",
      "Topic:996  ['beautiful day', 'fujitsu ibm', 'fujitsu ibm hp', 'got fujitsu', 'got fujitsu ibm', 'got lot model']\n",
      "Topic:997  ['erm', 'avenue', 'avenue parish', 'avenue parish magazine', 'erm woodland', 'erm woodland avenue']\n",
      "Topic:998  ['allow', 'allow work', 'allow work come', 'come home days', 'gt rs wont', 'home days']\n",
      "Topic:999  ['barcelona', 'barcelona way', 'barcelona way ru', 'easiest', 'easiest way', 'easiest way barcelona']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA Results\n",
    "\n",
    "In the second model, our feature set is far smaller, but we're still getting a very high accuracy. If the original dataset that we started with was very large, this impact would be magnified greatly. In general, NLP models use a lot of data, so this dimesionality reduction can help reduce training datasets that are massive and may even be impractical to process. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topics\n",
    "\n",
    "One of the things that LSA can do is to find \"topics\" in the text. We can use the components of the SVD to find the most important words in each topic. A \"topic\" is something that is not explicitly stated in the text, but is implied by the words that are used - if we have several documents that tend to use the same words, they are likely to be about the same topic. The LSA process is able to look for these cooccuring words and the documents that contain them, and group them as being about the same topic. The mechanics of this are some matrix math that is beyond what we neeed to know, but we can picture it like this.\n",
    "\n",
    "![LSA Math](images/lsa_math.webp \"LSA Math\")\n",
    "\n",
    "The topic extraction is also an example of unsupervised learning - something we'll look at more soon with clustering. We don't provide the topics to the mode in advance like we woud with a normal classification - we just give the LSA process the data, and it figures it out on its own.\n",
    "\n",
    "The model doesn't \"understand\" what each topic is, but it is able to pick up on trends of tokens that tend to occur together in documents. Text that contains \"ball\", \"game\", \"football\", \"play\", \"quarterback\" is likely to be about football - the model won't know it is football, but it will know that those words tend to occur together, and documetns that contain those words are likely to be about the same topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:0  ['gt good', 'lt gt good', 'boyfriend', 'driver', 'gt clean', 'gt true']\n",
      "Topic:1  ['amp imf', 'amp imf loan', 'bank amp', 'bank amp imf', 'bank directors', 'bank directors says']\n",
      "Topic:2  ['cn', '2gthr', '2gthr drinking', '2gthr drinking boost', 'aproach', 'aproach gal']\n",
      "Topic:3  ['breath', 'craziest', 'curry', 'planet', 'singing', 'thoughts']\n",
      "Topic:4  ['afternoon wife', 'afternoon wife called', 'arrested murderer', 'arrested murderer immediately', 'called police', 'called police police']\n",
      "Topic:5  ['atlast', 'waited', '1stone', '1stone sun', '1stone sun rose', 'amp sack']\n",
      "Topic:6  ['academic', 'department', 'academic department', 'academic department tell', 'academic secretary', 'academic secretary current']\n",
      "Topic:7  ['lovers', 'whn', 'avoids', 'avoids problems', 'avoids problems sent', 'becz']\n",
      "Topic:8  ['shahjahan', 'mumtaz', '4th wife', '4th wife wifes', 'arises', 'arises hell']\n",
      "Topic:9  ['shahjahan', 'mumtaz', '4th wife', '4th wife wifes', 'arises', 'arises hell']\n"
     ]
    }
   ],
   "source": [
    "for index, component in enumerate(svd_tmp.components_):\n",
    "    zipped = zip(tok_cols2, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:6]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    print(\"Topic:\"+str(index)+\" \", top_terms_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Truncated SVD\n",
    "\n",
    "Try to use the same text for predictions from the newsgroups last time. Try to use the TSVD with a limited number of components and see if the accuracy can stay similar to what we got last time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "remove = (\"headers\", \"footers\", \"quotes\")\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\"]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42, remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, shuffle=True, random_state=42, remove=remove)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorize and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (x,y): (857, 153056)   Test (x,y): (570, 153056)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize and prep datasets\n",
    "news_tf = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3), stop_words=\"english\", strip_accents=\"unicode\")\n",
    "X_train = news_tf.fit_transform(data_train.data)\n",
    "y_train = data_train.target\n",
    "X_test = news_tf.transform(data_test.data)\n",
    "y_test = data_test.target\n",
    "print(\"Train (x,y):\", X_train.shape, \"  Test (x,y):\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5140350877192983"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Models\n",
    "tsvd = TruncatedSVD(n_components=20)\n",
    "news_steps = [(\"scale\", StandardScaler(with_mean=False)), ('svd', tsvd), ('m', RandomForestClassifier())]\n",
    "news_model = Pipeline(steps=news_steps)\n",
    "news_model.fit(X_train, y_train)\n",
    "news_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>-0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>-0.001073</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>-0.000180</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.000490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>857 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.000388  0.000826  0.000519  0.000964  0.000555  0.000270  0.000458   \n",
       "1    0.000549  0.000779  0.001020  0.000972  0.000489  0.000440  0.000611   \n",
       "2    0.000671  0.000397  0.000511  0.000361  0.000516  0.000285  0.000089   \n",
       "3    0.000389  0.000460  0.000312  0.000368  0.000328  0.000148  0.000337   \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "852  0.000274  0.000801  0.001144  0.000395  0.000539  0.000203  0.000530   \n",
       "853  0.000938  0.000459  0.000591  0.000275  0.000494  0.001337  0.000420   \n",
       "854  0.000427  0.000753  0.000371  0.000836  0.000989  0.000207  0.000403   \n",
       "855  0.000628  0.000693  0.000216  0.000594  0.000351  0.000637  0.000156   \n",
       "856  0.000610  0.001143  0.000601  0.000477  0.000627  0.000396  0.000356   \n",
       "\n",
       "           7         8         9         10        11        12        13  \\\n",
       "0    0.000350  0.000579  0.000433  0.000569  0.000291  0.000706  0.000762   \n",
       "1    0.000956  0.001621  0.003187  0.000865  0.000616  0.001983  0.000166   \n",
       "2    0.000339  0.000656  0.000692  0.000435  0.000341  0.000731  0.000579   \n",
       "3    0.000139  0.000879  0.000539  0.000766  0.000291  0.000525  0.000831   \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "852  0.001075  0.000604  0.000317  0.000898  0.000495  0.000220  0.000804   \n",
       "853  0.000520  0.001067  0.000356  0.002006  0.001405  0.001969  0.000342   \n",
       "854  0.001117  0.000708  0.000771  0.000425  0.000353  0.000353  0.000877   \n",
       "855  0.000903  0.000777  0.000763  0.000672  0.000117  0.000379  0.001399   \n",
       "856  0.000476  0.000678  0.001168  0.000840  0.000991  0.001690  0.000630   \n",
       "\n",
       "           14        15        16        17        18        19  \n",
       "0    0.000496  0.000564  0.000270  0.000143  0.001342  0.000753  \n",
       "1    0.000177  0.001389  0.001244  0.000810  0.001812  0.000361  \n",
       "2    0.000208  0.002418  0.000339  0.000105  0.000609 -0.000009  \n",
       "3    0.000269  0.001352  0.000317 -0.000070  0.000625  0.000130  \n",
       "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "852  0.000502  0.000764  0.000811  0.000982  0.000945  0.000448  \n",
       "853  0.000066  0.000466  0.000458 -0.000089  0.002580  0.000946  \n",
       "854  0.000402  0.000727  0.001254 -0.000005  0.001292  0.000060  \n",
       "855  0.000014  0.001038  0.001007 -0.001073  0.002495  0.000277  \n",
       "856 -0.000180  0.001571  0.001168  0.000471  0.001531  0.000490  \n",
       "\n",
       "[857 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tsvd.transform(X_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at the Topics\n",
    "\n",
    "We can also take a look at what the topics identified in the data are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['000 years christians', '000 years prophecies', '000 years waited', '10 throwing', '10 throwing pieces']\n",
      "Topic 1:  ['according definition', 'according definition atheism', 'adopt', 'aren atheists', 'ask atheists']\n",
      "Topic 2:  ['10 1797', '10 1797 recently', '1070', '1070 1080', '1070 1080 english']\n",
      "Topic 3:  ['argument premises', 'fallacy occurs', 'general rule', 'abusive', 'accent']\n",
      "Topic 4:  ['02 32', '02 32 11', '11 1993', '11 1993 basically', '11 1993 owe']\n",
      "Topic 5:  ['12 old', '12 old persian', '12 saoshyant', '12 saoshyant sasanid', '1938 boyce']\n",
      "Topic 6:  ['071', '071 430', '1000 berlin', '1000 berlin 41', '41 germany']\n",
      "Topic 7:  ['1981 hell', '1981 hell broke', '1985 david', '1985 david caligiuri', '1985 months']\n",
      "Topic 8:  ['local zoroastrians', '000 years benign', '000 zoroastrians', '000 zoroastrians entire', '10 acre']\n",
      "Topic 9:  ['introns', '1992 petri', '_believe_ healthy', '_believe_ healthy sign', '_evidence_']\n",
      "Topic 10:  ['1066', '1066 z1dan', '1066 z1dan exnet', '15 years prove', 'abounded']\n",
      "Topic 11:  ['2nd coming', 'audible', 'audible life', 'audible life stream', 'buzzing']\n",
      "Topic 12:  ['jesus ra', '38', 'child god', 'differing', 'granted mormon']\n",
      "Topic 13:  ['100 cities', '100 cities settlements', '1838', '1838 governor', '1838 governor missouri']\n",
      "Topic 14:  ['10_ edited hymenaeus', '10_ real', '10_ real point', '1850', '1850 1905']\n",
      "Topic 15:  ['000 servicemen', '000 servicemen ww2', '230 000', '230 000 servicemen', 'absolutist position']\n",
      "Topic 16:  ['announce', 'asked questions', '25 children 64', '25 children dead', '25 dead']\n",
      "Topic 17:  ['ac uk', 'mail mail', 'mail mail server', 'mail server', 'mit']\n",
      "Topic 18:  ['abolitionists', 'abolitionists oppression', 'abolitionists oppression african', 'actively working', 'actively working christian']\n",
      "Topic 19:  ['_any_', '_any_ _including_', '_any_ _including_ gods', '_empire_', '_empire_ longer']\n"
     ]
    }
   ],
   "source": [
    "terms = news_tf.get_feature_names()\n",
    "topics = []\n",
    "for index, component in enumerate(tsvd.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Results\n",
    "\n",
    "Using LSA is a good way to condense our feature set that is often extremely large and extremely sparse, especially when we are dealing a large dataset, as is common with NLP. \n",
    "\n",
    "Simple applications in which this technique is used are documented clustering in text analysis, recommender systems, and information retrieval. More detailed use-cases of topic modeling are:\n",
    "<ul>\n",
    "<li> <b>Resume Summarization:</b> It can help recruiters to evaluate resumes by a quick glance. They can reduce effort in filtering pile of resume.\n",
    "<li> <b>Search Engine Optimization:</b> online articles, blogs, and documents can be tag easily by identifying the topics and associated keywords, which can improve optimize search results.\n",
    "<li> <b>Recommender System Optimization:</b> recommender systems act as an information filter and advisor according to the user profile and previous history. It can help us to discover unvisited relevant content based on past visits.\n",
    "<li> <b>Improving Customer Support:</b> Discovering relevant topics and associated keywords in customer complaints and feedback for examples product and service specifications, department, and branch details. Such information help company to directly rotated the complaint in respective department.\n",
    "<li> <b>Healthcare Industry:</b> topic modeling can help us to extract useful and valuable information from unstructured medical reports. This information can be used for patients treatment and medical science research purpose.\n",
    "</ul>\n",
    "\n",
    "In general, non-neural network approaches to NLP tend to be present in areas where we need to be able to process text quickly, without lots of processing. Spam filters are the classic example - we need to say yes or no, without spending ages to do so or burdening an email service with lots of processing. The examples above are similar - we are trying to draw a simple-ish conclusion. This is also somewhere that our old friend Bayes and his classifiers are most commonly seen - they are very fast at generating predictions once trained, so for something like emails, that's likely to be a good choice.\n",
    "\n",
    "An important concept from this example is the idea of condesing multiple features down into a smaller feature set while attempting to maintain the information in the original, that is something we'll revisit with Principal Component Analysis (PCA), a similar technique that is more generally applicable, later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Classification\n",
    "\n",
    "In addition to calculating things solely directly from our data, we can also use some external tools that can help create embeddings that are a little better (hopefully). This is also a neural network running behind the scenes to help us out. Word2Vec is an algorithm made by Google that can help process text and produce embeddings. Word2Vec looks for associations of words that occur with each other. This is an excellent illustrated description of Word2Vec: https://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "### Word2Vec in Process\n",
    "\n",
    "Word2Vec generates its embeddings by looking at words in a sentence, and the surrounding words in that same sentence. This differs quite a bit from the data that we've generated with the vectorization, as this model is better able to capture the strutucre of a sentence, beyond only looking at the individual words. We will use word2vec for a couple of different things:\n",
    "<ul>\n",
    "<li> Primarily, we'll use word2vec in a \"two model\" sequence to set us up to do classifications. The word2vec model will replace the count/tf-idf scores that we previously used for our feature set with embeddings that it calculates as the w2v model trains. The w2v model is \"learning\" how to represent words with numbers, in this case dimensions in a multidimensional space.\n",
    "    <ul>\n",
    "    <li> The w2v training is what creates the N-dimension measurements of each token, those then feed into our feature set for our modelling. \n",
    "    </ul>\n",
    "<li> After the word2vec model is created, we can do things like check the similarity of words. \n",
    "</ul>\n",
    "\n",
    "#### Gensim\n",
    "\n",
    "Gensim is a package that we can install that has an implementation of Word2Vec that we can use pretty easily. This part just downloads some of the stuff we'll need, like stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dlee2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "nltk.download('all')\n",
    "import nltk\n",
    "for package in ['stopwords','punkt','wordnet', 'omw-1.4']:\n",
    "    nltk.download(package) \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Since we are not using the vecorizer from sklearn, we need to provide our own tokenization. We can use the nltk based one from last time. We can also do any other types of processing here that we may want - stemming, customized stop words, etc... For this one I chopped out any 1 character tokens and added a regex filter to get rid of punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lemmaTokenizer(object):\n",
    "    def __init__(self, stop_words):\n",
    "        self.stop_words = stop_words\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        filtered_tok = []\n",
    "        for tok in tokens:\n",
    "            if tok not in stop_words:\n",
    "                tok = re.sub('\\W+','', tok) #Punctuation strip\n",
    "                tmp = self.lemmatizer.lemmatize(tok)\n",
    "                if len(tmp) >= 2:\n",
    "                    filtered_tok.append(tmp)\n",
    "        return filtered_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Clan Text - Tokenize and Lemmatize\n",
    "\n",
    "Prep some data. The \"second half\" of the dataframe is what we can use with the Word2Vec prediction models - we have cleaned up lists of tokens as well as translating the targets to 1 and 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, jurong, point, crazy, Available, bugis, g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[Ok, lar, Joking, wif, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, wkly, comp, win, FA, Cup, final,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, early, hor, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, nt, think, go, usf, life, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                               text  \\\n",
       "0    ham  Go until jurong point, crazy.. Available only ...   \n",
       "1    ham                      Ok lar... Joking wif u oni...   \n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3    ham  U dun say so early hor... U c already then say...   \n",
       "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                          clean_text  target2  \n",
       "0  [Go, jurong, point, crazy, Available, bugis, g...        0  \n",
       "1                        [Ok, lar, Joking, wif, oni]        0  \n",
       "2  [Free, entry, wkly, comp, win, FA, Cup, final,...        1  \n",
       "3               [dun, say, early, hor, already, say]        0  \n",
       "4    [Nah, nt, think, go, usf, life, around, though]        0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = lemmaTokenizer(stop_words)\n",
    "df[\"clean_text\"] = df[\"text\"].apply(lambda x: tok(x))\n",
    "df[\"target2\"] = pd.get_dummies(df[\"target\"], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word2Vec Ebmeddings\n",
    "\n",
    "Now comes the word2vec model - instead of taking our clean data and counting it to extract features, we can train our Word2Vec model with our cleaned up data and the output of that model is our set of features. This will have Word2Vec do its magic behind the scenes and perform the training. W2V works in one of two ways, which are roughly opposites of each other, when doing this training:\n",
    "<ul>\n",
    "<li> Continuous Bag of Words: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at words surrounding target to try to predict it. \n",
    "</ul>\n",
    "\n",
    "We'll revisit the details of some of this stuff later on when we look at neural networks, since W2V is a neural network algorithm, it will make more sense in context. \n",
    "\n",
    "<b>Note:</b> this training is not making a model that we are using to make predictions. This is training inside the W2V algorithm to generate representations of our tokens. \n",
    "\n",
    "#### Embeddings\n",
    "\n",
    "The embeddings that we are generating are vectors that represent the words in our text. We can look at the embeddings for a word to see what they look like, but they aren't comprehensible to humans. Our count vectors or the td-idf calculations we made previously are also embeddings, those are just far more simple. Word2Vec will generate embeddings that attempt to group words that are similar together in multidimensional space. We can look at a simple example in 2D:\n",
    "\n",
    "![Similarity](images/similarity.png \"Similarity\")\n",
    "\n",
    "The values here aren't calculated, they are chosen arbitrarily, but each word is represented here in two dimensions - x and y. Words that are similar in meaning should be close to each other in the vector representation, such as \"King\" and \"Queen\". Words that are not similar should be far apart, such as \"King\" and \"Rutabaga\". The embeddings that word2vec will generate from our data as a result of the training below will aim to represent each word in 200 dimension space. We feed the word2vec model our tokens, and it will generate a N-dimension vector for each token. We can use comparisons in this N dimensional space to determine how similar two words are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import inspect\n",
    "  \n",
    "# use signature()\n",
    "print(inspect.signature(Word2Vec))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model Architecture\n",
    "\n",
    "We've mentioned that the word2vec model we are making is a neural network. Neural networks, as we'll see later, have an architecture, or basically a size and design. The details don't matter too much to us yet, but one thing that we can change when determing our model in word2vec is that architecture - we can choose between CBOW and Skip-Gram. These two options are roughly opposites of each other. The details of how they work and how they differ are neural network details, so we'll set those details aside for now.\n",
    "\n",
    "<b>Continuous Bag of Words</b></br>\n",
    "![CBOW](images/cbow.webp \"CBOW\")\n",
    "\n",
    "<b>Skip-gram</b><br>\n",
    "![Skip-Gram](images/skip_gram.webp \"Skip-Gram\")\n",
    "\n",
    "These two models look like mirror images of each other, but what do they mean? Each does the same thing, though in a slightly different way. \n",
    "<ul>\n",
    "<li> CBOW: looks at a window around each target word to try to predict surrounding words.\n",
    "<li> Skip-Gram: looks at a word and tries to predict the surrounding words.\n",
    "</ul>\n",
    "\n",
    "For us, we can ignore the details of the differnece and think of the two options similarly to other options like regularization or entropy/gini. The way the internal neural network learns is different in the different architectures. \n",
    "\n",
    "#### Which to Use?\n",
    "\n",
    "For the most part, the real answer is our favorite one - test and choose the best. In general:\n",
    "<ul>\n",
    "<li> Skip Gram tends to work well with small amount of data and is found to represent rare words well.\n",
    "<li> CBOW is normally faster and has better representations for more frequent words.\n",
    "</ul>\n",
    "\n",
    "Parameters other than the ones we have listed here can be tweaked, but we'll somewhat ignore them for now, we're ok with the defaults. The \"sg\" parameter is the one that controls the architecture - 1 is skip-gram, 0 is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model = Word2Vec(df['clean_text'],min_count=3, vector_size=200, sg=1)\n",
    "#min_count=1 means word should be present at least across all documents,\n",
    "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
    "\n",
    "#combination of word and its vector\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  \n",
    "\n",
    "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model\n",
    "\n",
    "Each word in the vocabulary now has a vector representing it - of size 200. We can make a dataframe and see each token in our text and its vector representation. This vector is the internal representation of each token that is generated by Word2Vec. This is how the algorithm calculates things like similarity...\n",
    "\n",
    "The word2vec result that we are printing out here is each word in our vocabulary and its vector representation - or all of its dimensions in the 200D space we created while the model was trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call</th>\n",
       "      <th>nt</th>\n",
       "      <th>get</th>\n",
       "      <th>gt</th>\n",
       "      <th>lt</th>\n",
       "      <th>ur</th>\n",
       "      <th>You</th>\n",
       "      <th>go</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>...</th>\n",
       "      <th>apologise</th>\n",
       "      <th>Yun</th>\n",
       "      <th>butt</th>\n",
       "      <th>jesus</th>\n",
       "      <th>Hav</th>\n",
       "      <th>kerala</th>\n",
       "      <th>terrible</th>\n",
       "      <th>fan</th>\n",
       "      <th>regret</th>\n",
       "      <th>bottle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.038102</td>\n",
       "      <td>0.101524</td>\n",
       "      <td>0.062556</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.085290</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.040163</td>\n",
       "      <td>0.121427</td>\n",
       "      <td>0.079847</td>\n",
       "      <td>0.076359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016198</td>\n",
       "      <td>0.020784</td>\n",
       "      <td>0.019953</td>\n",
       "      <td>0.021905</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.018106</td>\n",
       "      <td>0.018767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.085316</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>-0.044181</td>\n",
       "      <td>-0.225550</td>\n",
       "      <td>-0.194640</td>\n",
       "      <td>-0.064691</td>\n",
       "      <td>-0.042950</td>\n",
       "      <td>0.041468</td>\n",
       "      <td>-0.038765</td>\n",
       "      <td>-0.040765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018494</td>\n",
       "      <td>-0.017916</td>\n",
       "      <td>-0.019584</td>\n",
       "      <td>-0.024098</td>\n",
       "      <td>-0.021798</td>\n",
       "      <td>-0.020666</td>\n",
       "      <td>-0.014950</td>\n",
       "      <td>-0.010544</td>\n",
       "      <td>-0.018724</td>\n",
       "      <td>-0.024930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038260</td>\n",
       "      <td>0.026226</td>\n",
       "      <td>0.023569</td>\n",
       "      <td>0.203368</td>\n",
       "      <td>0.224102</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.039247</td>\n",
       "      <td>0.041851</td>\n",
       "      <td>0.074875</td>\n",
       "      <td>0.068682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028527</td>\n",
       "      <td>0.022678</td>\n",
       "      <td>0.027095</td>\n",
       "      <td>0.024008</td>\n",
       "      <td>0.026574</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>0.013585</td>\n",
       "      <td>0.024615</td>\n",
       "      <td>0.025799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058929</td>\n",
       "      <td>0.043327</td>\n",
       "      <td>0.017787</td>\n",
       "      <td>0.196125</td>\n",
       "      <td>0.260589</td>\n",
       "      <td>-0.038024</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.060993</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>-0.001645</td>\n",
       "      <td>-0.005005</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>-0.000629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.223057</td>\n",
       "      <td>0.092523</td>\n",
       "      <td>0.138341</td>\n",
       "      <td>0.205707</td>\n",
       "      <td>0.223449</td>\n",
       "      <td>0.128874</td>\n",
       "      <td>0.143579</td>\n",
       "      <td>0.082531</td>\n",
       "      <td>0.110568</td>\n",
       "      <td>0.107105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071646</td>\n",
       "      <td>0.054240</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.075602</td>\n",
       "      <td>0.074960</td>\n",
       "      <td>0.060691</td>\n",
       "      <td>0.062762</td>\n",
       "      <td>0.040702</td>\n",
       "      <td>0.057343</td>\n",
       "      <td>0.058549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.129832</td>\n",
       "      <td>0.011184</td>\n",
       "      <td>0.056998</td>\n",
       "      <td>-0.068960</td>\n",
       "      <td>-0.114064</td>\n",
       "      <td>0.072971</td>\n",
       "      <td>0.057431</td>\n",
       "      <td>0.072325</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.019506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031469</td>\n",
       "      <td>0.029930</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.034099</td>\n",
       "      <td>0.039855</td>\n",
       "      <td>0.024704</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>0.020857</td>\n",
       "      <td>0.029324</td>\n",
       "      <td>0.034142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.114018</td>\n",
       "      <td>0.112490</td>\n",
       "      <td>0.120456</td>\n",
       "      <td>-0.021113</td>\n",
       "      <td>-0.078696</td>\n",
       "      <td>0.126303</td>\n",
       "      <td>0.122783</td>\n",
       "      <td>0.118190</td>\n",
       "      <td>0.067599</td>\n",
       "      <td>0.075919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058242</td>\n",
       "      <td>0.044233</td>\n",
       "      <td>0.047979</td>\n",
       "      <td>0.058014</td>\n",
       "      <td>0.062245</td>\n",
       "      <td>0.045013</td>\n",
       "      <td>0.053004</td>\n",
       "      <td>0.038220</td>\n",
       "      <td>0.043132</td>\n",
       "      <td>0.048769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.210501</td>\n",
       "      <td>-0.142424</td>\n",
       "      <td>-0.174004</td>\n",
       "      <td>-0.259689</td>\n",
       "      <td>-0.225004</td>\n",
       "      <td>-0.175270</td>\n",
       "      <td>-0.169832</td>\n",
       "      <td>-0.104597</td>\n",
       "      <td>-0.145419</td>\n",
       "      <td>-0.133579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080439</td>\n",
       "      <td>-0.067442</td>\n",
       "      <td>-0.076271</td>\n",
       "      <td>-0.088285</td>\n",
       "      <td>-0.089734</td>\n",
       "      <td>-0.069988</td>\n",
       "      <td>-0.077284</td>\n",
       "      <td>-0.048240</td>\n",
       "      <td>-0.074025</td>\n",
       "      <td>-0.076776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.029528</td>\n",
       "      <td>-0.008345</td>\n",
       "      <td>-0.009615</td>\n",
       "      <td>0.251096</td>\n",
       "      <td>0.283413</td>\n",
       "      <td>-0.060964</td>\n",
       "      <td>-0.048285</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.041499</td>\n",
       "      <td>0.020620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017833</td>\n",
       "      <td>-0.011234</td>\n",
       "      <td>-0.017892</td>\n",
       "      <td>-0.021037</td>\n",
       "      <td>-0.019451</td>\n",
       "      <td>-0.004492</td>\n",
       "      <td>-0.018725</td>\n",
       "      <td>-0.013954</td>\n",
       "      <td>-0.016992</td>\n",
       "      <td>-0.016489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.091387</td>\n",
       "      <td>-0.055638</td>\n",
       "      <td>-0.026244</td>\n",
       "      <td>-0.010172</td>\n",
       "      <td>0.026935</td>\n",
       "      <td>-0.019464</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>-0.022749</td>\n",
       "      <td>-0.040581</td>\n",
       "      <td>-0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.006145</td>\n",
       "      <td>-0.005243</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>-0.003405</td>\n",
       "      <td>0.003520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         call        nt       get        gt        lt        ur       You  \\\n",
       "0   -0.038102  0.101524  0.062556 -0.001003 -0.085290  0.050358  0.040163   \n",
       "1   -0.085316  0.006080 -0.044181 -0.225550 -0.194640 -0.064691 -0.042950   \n",
       "2    0.038260  0.026226  0.023569  0.203368  0.224102  0.040578  0.039247   \n",
       "3    0.058929  0.043327  0.017787  0.196125  0.260589 -0.038024 -0.003279   \n",
       "4    0.223057  0.092523  0.138341  0.205707  0.223449  0.128874  0.143579   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.129832  0.011184  0.056998 -0.068960 -0.114064  0.072971  0.057431   \n",
       "196  0.114018  0.112490  0.120456 -0.021113 -0.078696  0.126303  0.122783   \n",
       "197 -0.210501 -0.142424 -0.174004 -0.259689 -0.225004 -0.175270 -0.169832   \n",
       "198 -0.029528 -0.008345 -0.009615  0.251096  0.283413 -0.060964 -0.048285   \n",
       "199  0.091387 -0.055638 -0.026244 -0.010172  0.026935 -0.019464  0.003716   \n",
       "\n",
       "           go      know      like  ...  apologise       Yun      butt  \\\n",
       "0    0.121427  0.079847  0.076359  ...   0.016198  0.020784  0.019953   \n",
       "1    0.041468 -0.038765 -0.040765  ...  -0.018494 -0.017916 -0.019584   \n",
       "2    0.041851  0.074875  0.068682  ...   0.028527  0.022678  0.027095   \n",
       "3    0.021347  0.060993  0.042641  ...   0.002563  0.002361  0.005638   \n",
       "4    0.082531  0.110568  0.107105  ...   0.071646  0.054240  0.070248   \n",
       "..        ...       ...       ...  ...        ...       ...       ...   \n",
       "195  0.072325  0.011140  0.019506  ...   0.031469  0.029930  0.033281   \n",
       "196  0.118190  0.067599  0.075919  ...   0.058242  0.044233  0.047979   \n",
       "197 -0.104597 -0.145419 -0.133579  ...  -0.080439 -0.067442 -0.076271   \n",
       "198  0.000533  0.041499  0.020620  ...  -0.017833 -0.011234 -0.017892   \n",
       "199 -0.022749 -0.040581 -0.015625  ...  -0.001678  0.003806 -0.000175   \n",
       "\n",
       "        jesus       Hav    kerala  terrible       fan    regret    bottle  \n",
       "0    0.021905  0.015265  0.012744  0.019767  0.008395  0.018106  0.018767  \n",
       "1   -0.024098 -0.021798 -0.020666 -0.014950 -0.010544 -0.018724 -0.024930  \n",
       "2    0.024008  0.026574  0.020410  0.023743  0.013585  0.024615  0.025799  \n",
       "3   -0.000320 -0.003241  0.009732 -0.001645 -0.005005  0.000771 -0.000629  \n",
       "4    0.075602  0.074960  0.060691  0.062762  0.040702  0.057343  0.058549  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.034099  0.039855  0.024704  0.035530  0.020857  0.029324  0.034142  \n",
       "196  0.058014  0.062245  0.045013  0.053004  0.038220  0.043132  0.048769  \n",
       "197 -0.088285 -0.089734 -0.069988 -0.077284 -0.048240 -0.074025 -0.076776  \n",
       "198 -0.021037 -0.019451 -0.004492 -0.018725 -0.013954 -0.016992 -0.016489  \n",
       "199 -0.006145 -0.005243  0.001787  0.002105  0.001140 -0.003405  0.003520  \n",
       "\n",
       "[200 rows x 3151 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(w2v)\n",
    "vectors = model.wv\n",
    "tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Similarity\n",
    "\n",
    "One of the things that Word2Vec allows us to do is to look at the similarity of words. This similarity is calculated via the cosine distance of the vectors. Cosine similarity is a technique to calculate the distance between two vectors - smaller distance, more similar. \n",
    "\n",
    "![Cosine Similarity](images/cosine_sim.png \"Cosine Similarity\" )\n",
    "\n",
    "Once the vectors are derived by in the training process, these similarity calculations are pretty easy and quick. \n",
    "\n",
    "<b>Note:</b> the similarites here are calculated by the values derived from our trained model. So they are based on the relationships in our text. Word2Vec and other NLP packages also commonly have pretrained models that can be downloaded that are based on large amounts of text. Words may be represented very differently in those vs whatever we train here - the more data we have, the more consistent they'll be; the more \"unique\" our text is, the more different it will be. If we were, for example, working in a specific domain such as patent law, we could use a large amount of patent law text to train a model that would be more consistent with our domain. Or, perhaps more likely, we could use a pretrained model that has been created with massive amounts of training data. \n",
    "\n",
    "### Types of Similarity\n",
    "\n",
    "When looking at the similarity of different words, we can measure that similarity in a couple of ways - lexical and semantic, that we mentioned before. Here, the model is looking at semantic similarity, the \"meaning\" of each word, in the context of our text, is being compared and the most similar words are returned. Note that we can only calculate similarity here for words that we have in our vocabulary. This is one place where large language models like chat GPT have a massive advantage, their vocabulary is huge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Do', 0.9927803874015808),\n",
       " ('like', 0.9925779104232788),\n",
       " ('want', 0.9910417795181274)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the most similar word to anything in our vocabulary \n",
    "vectors.most_similar(\"know\")[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da see\n",
      "0.9755088\n"
     ]
    }
   ],
   "source": [
    "# We can also see how similar different words are. \n",
    "# I will grab two arbitrary words from the vocabulary and see how similar they are.\n",
    "# you could use anything in the vocabulary here, try some other ones!\n",
    "\n",
    "word_a = tmp.columns[30]\n",
    "word_b = tmp.columns[40]\n",
    "\n",
    "print(word_a, word_b)\n",
    "print(vectors.similarity(word_a, word_b))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "We can take our actual data now and transform it through the Word2Vec model that we've made. This will generate our smaller feature set that we can build our models from, one of the things that the MeanEmbeddingVectorizer does is to collapse the data down to those 200 dimensions in the vector. Our dataset is spit out the other end, each row of text is now represented by a single vector of those 200 dimensions of our embedding values from the word2vec model (the columns).\n",
    "\n",
    "<b>Note:</b> if this is confusing, please ignore it, this is a bit of a tangent. The meanembeddingvectorizer thing is needed to \"flatten\" our data down from 200D to 1D for each token. This is because our models can only dal with data that is in that format (instances x features). We can't have a 200D vector for each token, we need to collapse it down to a single value. Later, when we look at neural networks, we'll see models with differnet architectures that can accomadate data that is multidimensional like this. That's one of the reasons that neural networks are so powerful, they can accomadate data that is multidimensional, so something like an image can be treated like an image, not just a bunch of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 200) (1393, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.036092</td>\n",
       "      <td>0.033547</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.128486</td>\n",
       "      <td>-0.132441</td>\n",
       "      <td>-0.088683</td>\n",
       "      <td>0.194571</td>\n",
       "      <td>-0.013639</td>\n",
       "      <td>0.103944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110202</td>\n",
       "      <td>-0.116040</td>\n",
       "      <td>-0.024132</td>\n",
       "      <td>-0.042529</td>\n",
       "      <td>0.173978</td>\n",
       "      <td>0.069524</td>\n",
       "      <td>0.103567</td>\n",
       "      <td>-0.149962</td>\n",
       "      <td>-0.031302</td>\n",
       "      <td>0.003084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.014563</td>\n",
       "      <td>-0.064444</td>\n",
       "      <td>0.027276</td>\n",
       "      <td>-0.008208</td>\n",
       "      <td>0.179249</td>\n",
       "      <td>-0.192483</td>\n",
       "      <td>-0.077489</td>\n",
       "      <td>0.199713</td>\n",
       "      <td>0.025444</td>\n",
       "      <td>0.151777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119965</td>\n",
       "      <td>-0.135827</td>\n",
       "      <td>0.017612</td>\n",
       "      <td>-0.035862</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>0.104366</td>\n",
       "      <td>0.125950</td>\n",
       "      <td>-0.182974</td>\n",
       "      <td>-0.061897</td>\n",
       "      <td>0.038447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016355</td>\n",
       "      <td>-0.019404</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.056757</td>\n",
       "      <td>-0.058710</td>\n",
       "      <td>-0.035067</td>\n",
       "      <td>0.085175</td>\n",
       "      <td>-0.007503</td>\n",
       "      <td>0.046486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044204</td>\n",
       "      <td>-0.048861</td>\n",
       "      <td>-0.008578</td>\n",
       "      <td>-0.007064</td>\n",
       "      <td>0.080626</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>0.043335</td>\n",
       "      <td>-0.065316</td>\n",
       "      <td>-0.012376</td>\n",
       "      <td>-0.003829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031242</td>\n",
       "      <td>-0.027079</td>\n",
       "      <td>0.029857</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.089194</td>\n",
       "      <td>-0.087159</td>\n",
       "      <td>-0.053202</td>\n",
       "      <td>0.139307</td>\n",
       "      <td>-0.015542</td>\n",
       "      <td>0.067257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069769</td>\n",
       "      <td>-0.070814</td>\n",
       "      <td>-0.019610</td>\n",
       "      <td>-0.018386</td>\n",
       "      <td>0.126962</td>\n",
       "      <td>0.044637</td>\n",
       "      <td>0.069126</td>\n",
       "      <td>-0.100721</td>\n",
       "      <td>-0.017349</td>\n",
       "      <td>-0.001690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020488</td>\n",
       "      <td>-0.086588</td>\n",
       "      <td>0.088789</td>\n",
       "      <td>0.073551</td>\n",
       "      <td>0.141953</td>\n",
       "      <td>-0.113830</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.207210</td>\n",
       "      <td>-0.093233</td>\n",
       "      <td>0.115402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>-0.042094</td>\n",
       "      <td>-0.031758</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>0.231547</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.055329</td>\n",
       "      <td>-0.170917</td>\n",
       "      <td>0.065593</td>\n",
       "      <td>-0.007961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.035079</td>\n",
       "      <td>-0.027595</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.086860</td>\n",
       "      <td>-0.089104</td>\n",
       "      <td>-0.057580</td>\n",
       "      <td>0.142416</td>\n",
       "      <td>-0.014529</td>\n",
       "      <td>0.070292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082455</td>\n",
       "      <td>-0.079716</td>\n",
       "      <td>-0.013286</td>\n",
       "      <td>-0.021307</td>\n",
       "      <td>0.127696</td>\n",
       "      <td>0.043906</td>\n",
       "      <td>0.073764</td>\n",
       "      <td>-0.108406</td>\n",
       "      <td>-0.025797</td>\n",
       "      <td>-0.004273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.023696</td>\n",
       "      <td>-0.043705</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>0.120234</td>\n",
       "      <td>-0.118519</td>\n",
       "      <td>-0.059255</td>\n",
       "      <td>0.163734</td>\n",
       "      <td>-0.014929</td>\n",
       "      <td>0.090919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087063</td>\n",
       "      <td>-0.086537</td>\n",
       "      <td>-0.015112</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>0.154346</td>\n",
       "      <td>0.050127</td>\n",
       "      <td>0.078817</td>\n",
       "      <td>-0.133573</td>\n",
       "      <td>-0.021794</td>\n",
       "      <td>0.001243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.038787</td>\n",
       "      <td>-0.034107</td>\n",
       "      <td>0.048469</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.120873</td>\n",
       "      <td>-0.121756</td>\n",
       "      <td>-0.084047</td>\n",
       "      <td>0.195943</td>\n",
       "      <td>-0.024222</td>\n",
       "      <td>0.093252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104439</td>\n",
       "      <td>-0.109743</td>\n",
       "      <td>-0.022798</td>\n",
       "      <td>-0.029307</td>\n",
       "      <td>0.180394</td>\n",
       "      <td>0.061976</td>\n",
       "      <td>0.099231</td>\n",
       "      <td>-0.148066</td>\n",
       "      <td>-0.030023</td>\n",
       "      <td>-0.002072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>0.047031</td>\n",
       "      <td>-0.031859</td>\n",
       "      <td>0.043867</td>\n",
       "      <td>0.013835</td>\n",
       "      <td>0.110661</td>\n",
       "      <td>-0.108729</td>\n",
       "      <td>-0.074370</td>\n",
       "      <td>0.190132</td>\n",
       "      <td>-0.029901</td>\n",
       "      <td>0.090389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097539</td>\n",
       "      <td>-0.090722</td>\n",
       "      <td>-0.028470</td>\n",
       "      <td>-0.028436</td>\n",
       "      <td>0.182162</td>\n",
       "      <td>0.051982</td>\n",
       "      <td>0.095136</td>\n",
       "      <td>-0.139893</td>\n",
       "      <td>-0.018794</td>\n",
       "      <td>-0.009659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>0.058364</td>\n",
       "      <td>-0.015426</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.112972</td>\n",
       "      <td>-0.104620</td>\n",
       "      <td>-0.096118</td>\n",
       "      <td>0.212207</td>\n",
       "      <td>-0.029470</td>\n",
       "      <td>0.089468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107036</td>\n",
       "      <td>-0.100674</td>\n",
       "      <td>-0.042893</td>\n",
       "      <td>-0.040191</td>\n",
       "      <td>0.183915</td>\n",
       "      <td>0.064188</td>\n",
       "      <td>0.102659</td>\n",
       "      <td>-0.136289</td>\n",
       "      <td>-0.017093</td>\n",
       "      <td>-0.008787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.040318 -0.036092  0.033547  0.003449  0.128486 -0.132441 -0.088683   \n",
       "1    -0.014563 -0.064444  0.027276 -0.008208  0.179249 -0.192483 -0.077489   \n",
       "2     0.016355 -0.019404  0.025000 -0.000043  0.056757 -0.058710 -0.035067   \n",
       "3     0.031242 -0.027079  0.029857  0.008038  0.089194 -0.087159 -0.053202   \n",
       "4     0.020488 -0.086588  0.088789  0.073551  0.141953 -0.113830  0.002489   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4174  0.035079 -0.027595  0.032385  0.001245  0.086860 -0.089104 -0.057580   \n",
       "4175  0.023696 -0.043705  0.038128  0.006069  0.120234 -0.118519 -0.059255   \n",
       "4176  0.038787 -0.034107  0.048469  0.002890  0.120873 -0.121756 -0.084047   \n",
       "4177  0.047031 -0.031859  0.043867  0.013835  0.110661 -0.108729 -0.074370   \n",
       "4178  0.058364 -0.015426  0.038778  0.015083  0.112972 -0.104620 -0.096118   \n",
       "\n",
       "           7         8         9    ...       190       191       192  \\\n",
       "0     0.194571 -0.013639  0.103944  ...  0.110202 -0.116040 -0.024132   \n",
       "1     0.199713  0.025444  0.151777  ...  0.119965 -0.135827  0.017612   \n",
       "2     0.085175 -0.007503  0.046486  ...  0.044204 -0.048861 -0.008578   \n",
       "3     0.139307 -0.015542  0.067257  ...  0.069769 -0.070814 -0.019610   \n",
       "4     0.207210 -0.093233  0.115402  ...  0.039414 -0.042094 -0.031758   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4174  0.142416 -0.014529  0.070292  ...  0.082455 -0.079716 -0.013286   \n",
       "4175  0.163734 -0.014929  0.090919  ...  0.087063 -0.086537 -0.015112   \n",
       "4176  0.195943 -0.024222  0.093252  ...  0.104439 -0.109743 -0.022798   \n",
       "4177  0.190132 -0.029901  0.090389  ...  0.097539 -0.090722 -0.028470   \n",
       "4178  0.212207 -0.029470  0.089468  ...  0.107036 -0.100674 -0.042893   \n",
       "\n",
       "           193       194       195       196       197       198       199  \n",
       "0    -0.042529  0.173978  0.069524  0.103567 -0.149962 -0.031302  0.003084  \n",
       "1    -0.035862  0.188477  0.104366  0.125950 -0.182974 -0.061897  0.038447  \n",
       "2    -0.007064  0.080626  0.027764  0.043335 -0.065316 -0.012376 -0.003829  \n",
       "3    -0.018386  0.126962  0.044637  0.069126 -0.100721 -0.017349 -0.001690  \n",
       "4     0.040351  0.231547  0.007314  0.055329 -0.170917  0.065593 -0.007961  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4174 -0.021307  0.127696  0.043906  0.073764 -0.108406 -0.025797 -0.004273  \n",
       "4175 -0.018608  0.154346  0.050127  0.078817 -0.133573 -0.021794  0.001243  \n",
       "4176 -0.029307  0.180394  0.061976  0.099231 -0.148066 -0.030023 -0.002072  \n",
       "4177 -0.028436  0.182162  0.051982  0.095136 -0.139893 -0.018794 -0.009659  \n",
       "4178 -0.040191  0.183915  0.064188  0.102659 -0.136289 -0.017093 -0.008787  \n",
       "\n",
       "[4179 rows x 200 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
    " \n",
    "# Split data - using the new dataframe parts that we cleaned up. \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_text\"],df[\"target2\"])\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "X_train_vectors_w2v = modelw.transform(X_train)\n",
    "X_test_vectors_w2v = modelw.transform(X_test)\n",
    "print(X_train_vectors_w2v.shape, X_test_vectors_w2v.shape)\n",
    "pd.DataFrame(X_train_vectors_w2v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model\n",
    "\n",
    "We now have a pretty normal dataset and can use the new data to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9910589975635224\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1224\n",
      "           1       0.95      0.88      0.91       169\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.97      0.94      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions\n",
    "lr_w2v = RandomForestClassifier()\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_test_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_test_vectors_w2v)[:,1]\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  \n",
    "\n",
    "print(classification_report(y_test, y_predict))\n",
    "sns.heatmap(confusion_matrix(y_test, y_predict), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Word2Vec\n",
    "\n",
    "Use the newsgroup data and Word2Vec to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datsets and Tokenize\n",
    "tok = lemmaTokenizer(stop_words)\n",
    "X_w2v_news_train = [tok(x) for x in data_train.data]\n",
    "X_w2v_news_test = [tok(x) for x in data_test.data]\n",
    "\n",
    "y_train_news = data_train.target\n",
    "y_test_news = data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benedikt',\n",
       " 'Rosenau',\n",
       " 'writes',\n",
       " 'great',\n",
       " 'authority',\n",
       " 'Contradictory',\n",
       " 'property',\n",
       " 'language',\n",
       " 'If',\n",
       " 'correct',\n",
       " 'THINGS',\n",
       " 'DEFINED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 'object',\n",
       " 'definition',\n",
       " 'reality',\n",
       " 'If',\n",
       " 'amend',\n",
       " 'THINGS',\n",
       " 'DESCRIBED',\n",
       " 'BY',\n",
       " 'CONTRADICTORY',\n",
       " 'LANGUAGE',\n",
       " 'DO',\n",
       " 'NOT',\n",
       " 'EXIST',\n",
       " 've',\n",
       " 'come',\n",
       " 'something',\n",
       " 'plainly',\n",
       " 'false',\n",
       " 'Failures',\n",
       " 'description',\n",
       " 'merely',\n",
       " 'failure',\n",
       " 'description',\n",
       " 'objectivist',\n",
       " 'remember']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview\n",
    "X_w2v_news_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Word2vec model\n",
    "model_news = Word2Vec(X_w2v_news_train, min_count=1, vector_size=200)\n",
    "w2v_news = dict(zip(model_news.wv.index_to_key, model_news.wv.vectors)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_news_w = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_news = model_news_w.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_news = model_news_w.transform(X_w2v_news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n",
      "Confusion Matrix:\n",
      " [[300  19]\n",
      " [236  15]]\n",
      "AUC: 0.5268455956737315\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf = SVC(probability=True)\n",
    "news_clf.fit(X_train_vectors_w2v_news, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news = news_clf.predict(X_val_vectors_w2v_news)\n",
    "y_prob_news = news_clf.predict_proba(X_val_vectors_w2v_news)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news))\n",
    "print('Confusion Matrix:\\n',confusion_matrix(y_test_news, y_predict_news))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test_news, y_prob_news)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec Models\n",
    "\n",
    "When we do the intial training of the word2vec model (not when we are making final predictions), we are using our corpus to generate the space and the embeddigns for all of our tokens. We can also download a pretrained model, that already has the N-dimensional space defined (when it was trained on some different data), and use that to generate our embeddings. This is a common practice, and can be a good way to get started. Gensim has several models that have been trained on varying amounts of data, they are listed here: https://github.com/RaRe-Technologies/gensim-data along with several other datasets that we could use to train a model. \n",
    "\n",
    "The differences with using this pretrained model (or an existing corpus below) are:\n",
    "<ul>\n",
    "<li> Above, when training word2vec with our data, we used our corpus to generate the space in which the tokens are placed, then calculate those embeddings for each token. \n",
    "<li> With a pretrained model, we are using the space that was generated by the model that was trained on some other data, then placing our tokens in that space. \n",
    "</ul>\n",
    "\n",
    "So if we are using some text from wikipedia (like the second example), the space in which embeddings are made is defined by the text in wikipedia. So the \"closeness\" in meaning of words is based on what is in that corpus. We then take our tokens and calculate their embeddings in that space. The big advantage to this is someone else can train a model on lots of data, which hopefully generates a better understanding of the relationships between words, and we can then just score our words on those scales. This approach is common in large models, like text processing or image recognition, where the training load can be too large for \"regular folk\". We can also take these trained models and \"customize\" them to our data, we'll look at that with image recognition at the end of the semester. \n",
    "\n",
    "#### Use a Twitter Trained Model\n",
    "\n",
    "We can try using a pretrained model that was trained on Twitter data. This model has been pretrained, so it already knows how to represent words, we will then feed it all of our tokens, and it will generate the embeddings for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'FUCK' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15356\\1319878106.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel_twit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove-twitter-200\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel_twit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FUCK\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dlee2\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'FUCK' not present\""
     ]
    }
   ],
   "source": [
    "# Downlaod the model and do a little test\n",
    "\n",
    "import gensim.downloader as api\n",
    "model_twit = api.load(\"glove-twitter-200\")\n",
    "model_twit.most_similar(\"FUCK\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate Embeddings\n",
    "\n",
    "The model exists, so we will use it to transform our tokens into numerical representations. Then we can go use those to make classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.58      0.71       517\n",
      "           1       0.13      0.60      0.21        53\n",
      "\n",
      "    accuracy                           0.58       570\n",
      "   macro avg       0.53      0.59      0.46       570\n",
      "weighted avg       0.86      0.58      0.67       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_twit = dict(zip(model_twit.index_to_key, model_twit.vectors))\n",
    "model_twit_emb = MeanEmbeddingVectorizer(w2v_twit)\n",
    "\n",
    "X_train_twit = model_twit_emb.transform(X_w2v_news_train)\n",
    "X_test_twit = model_twit_emb.transform(X_w2v_news_test)\n",
    "\n",
    "# Make predictions\n",
    "twit_clf = SVC(probability=True)\n",
    "twit_clf.fit(X_train_twit, y_train_news)  #model\n",
    "\n",
    "# Predict y value for test dataset\n",
    "y_predict_twit = twit_clf.predict(X_test_twit)\n",
    "\n",
    "print(classification_report(y_predict_twit, y_test_news))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premade Corpus\n",
    "\n",
    "We can also train a model directly from a preexisting corpus, then generate our embeddings from that model. \n",
    "\n",
    "The \"text8\" corpus is a small corpus of text that is included with gensim. It is a small subset of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')\n",
    "model_corp = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 200)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_corp = dict(zip(model_corp.wv.index_to_key, model_corp.wv.vectors)) \n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "model_corp_emb = MeanEmbeddingVectorizer(w2v_news)\n",
    "X_train_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_train)\n",
    "X_val_vectors_w2v_corp = model_corp_emb.transform(X_w2v_news_test)\n",
    "X_train_vectors_w2v_corp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.94      0.70       319\n",
      "           1       0.44      0.06      0.11       251\n",
      "\n",
      "    accuracy                           0.55       570\n",
      "   macro avg       0.50      0.50      0.40       570\n",
      "weighted avg       0.51      0.55      0.44       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "news_clf_corp = SVC(probability=True)\n",
    "news_clf_corp.fit(X_train_vectors_w2v_corp, y_train_news)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict_news_corp = news_clf.predict(X_val_vectors_w2v_corp)\n",
    "y_prob_news_corp = news_clf.predict_proba(X_val_vectors_w2v_corp)[:,1]\n",
    "\n",
    "print(classification_report(y_test_news,y_predict_news_corp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Me!\n",
    "\n",
    "As we see with things like chatGPT and the assortment of voice assistants, NLP is currently exploding in both capability and prevelence. Those other models are based on these concepts, but there are a few key differences that help those tools be more powerful:\n",
    "<ul>\n",
    "<li> They are trained on much larger datasets. Very, very, very large datasets. In NLP specifically, this helps because it can help address the problem with us having so many words, many of which aren't used super often - i.e. the fact that there are a lot of words that don't occur together in the same sentence. If the training data is massive (e.g. \"the internet\"), we massively reduce the impact of this problem, as we see each word many times. \n",
    "<li> The use of neural networks, in particular recurrant neural networks (RNNs) that are able to deal with data as a sequence, and \"remember\" other parts of a sequence of words. This helps these models understand the context of a sentence, and the relationships between words.\n",
    "    <ul>\n",
    "    <li> Of note with neural networks, especially those using massive training data sets, is that the first layers of the model can perform equivalent data prep work that we've done here. So the model is more able to deal with data in its raw form, and doesn't need to be preprocessed as much separately, in advance. \n",
    "    </ul>\n",
    "<li> Manual intervention is used, humans provide examples of convesation, define labels, and evaluate the quality of the model's work. You may have heard news of Kenyans being paid low wages to label data for these models.\n",
    "<li> Other model types are used to help, such as reinforcement learning. Responses that are good are rewarded, and those that are bad are punished. This helps the model learn what is good and what is bad. This is particularly useful for generative models, such as chatGPT.\n",
    "</ul>\n",
    "\n",
    "As noted, this stuff is actively being developed right now, and the more advanced the tool, the more likely we are to see innovation or specific interventions to correct issues. The foundations we have looked at here are the building blocks of that work. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a67d3d74f81673499695f5753f7ab28afe8f7c82c0a4946d9a7d056c03b92cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
